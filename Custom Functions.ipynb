{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Custom Functions.ipynb","provenance":[],"collapsed_sections":["GyR7yxZNMmbR","5WDG7DcFKxsS","zbGKwTDqK5SB","57iK4o5NLKqa","GpmSB7bPLmtC","fvBMXcW0Lshf","w8eqPtax9mgM","zzfiWCzR-995","9N4fQ0a9WPPb"],"authorship_tag":"ABX9TyNxZyC+XdNSkocgDI2jhALP"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"wxCKg8ZZOf5W"},"source":["# pre-processing and cleaning"]},{"cell_type":"markdown","metadata":{"id":"GyR7yxZNMmbR"},"source":["## Imports"]},{"cell_type":"code","metadata":{"id":"G9dxJhpOOnhV"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.tree import plot_tree\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import cross_val_score\n","#from sklearn.metrics import confusion_matrix #bugged because confusion_matrix([1,1],[1,1]) = [[2]], not [[2,0],[0,0]]. Also confusion_matrix([0,0],[0,0]) = [[2]]\n","from sklearn.metrics import plot_confusion_matrix\n","from sklearn.model_selection import KFold\n","import random as rd\n","from sklearn.decomposition import PCA\n","from sklearn import preprocessing\n","import copy\n","from sklearn import tree as sktree\n","from sklearn.metrics import accuracy_score\n","from scipy.optimize import curve_fit\n","from scipy.stats import norm\n","import seaborn as sns\n","from sklearn.linear_model import LogisticRegression\n","#from sklearn.metrics import r2_score I make my own\n","from scipy.stats import rankdata\n","import warnings\n","from sklearn.model_selection import GridSearchCV\n","import seaborn as sns\n","from matplotlib.colors import ListedColormap\n","from sklearn import svm, datasets\n","from sklearn import linear_model\n","import random\n","from sklearn.utils import resample\n","from sklearn.ensemble import RandomForestClassifier\n","from tqdm.auto import tqdm\n","from sklearn.metrics import roc_auc_score\n","from matplotlib.lines import Line2D\n","import operator as op\n","from functools import reduce\n","\n","import torch\n","from torch import nn\n","from torch.autograd import Variable\n","from torch.optim import LBFGS\n","from torch.utils.data import Dataset, DataLoader\n","\n","def confusion_matrix(y_true,y_predicted):\n","  true_positives = np.sum([i*j for i,j in zip(y_true,y_predicted)])\n","  false_negatives = np.sum([int((i==1)&(j==0)) for i,j in zip(y_true,y_predicted)])\n","  true_negatives = np.sum([(1-i)*(1-j) for i,j in zip(y_true,y_predicted)])\n","  false_positives = np.sum([int((i==0)&(j==1)) for i,j in zip(y_true,y_predicted)])\n","  return [[true_positives,false_positives],[false_negatives,true_negatives]]\n","\n","def sensitivity_score(y_true,y_predicted):\n","  assert(len(y_true)==len(y_predicted))\n","  for i in y_true:\n","    assert((i == 1) | (i == 0))\n","  for i in y_predicted:\n","    assert((i == 1) | (i == 0))\n","  true_positives = np.sum([i*j for i,j in zip(y_true,y_predicted)])\n","  false_negatives = np.sum([int((i==1)&(j==0)) for i,j in zip(y_true,y_predicted)])\n","  if (true_positives+false_negatives) == 0:\n","    return np.nan\n","  return true_positives/(true_positives+false_negatives)\n","\n","def specificity_score(y_true,y_predicted):\n","  assert(len(y_true)==len(y_predicted))\n","  for i in y_true:\n","    assert((i == 1) | (i == 0))\n","  for i in y_predicted:\n","    assert((i == 1) | (i == 0))\n","  true_negatives = np.sum([(1-i)*(1-j) for i,j in zip(y_true,y_predicted)])\n","  false_positives = np.sum([int((i==0)&(j==1)) for i,j in zip(y_true,y_predicted)])\n","  if (true_negatives+false_positives) == 0:\n","    return np.nan\n","  return true_negatives/(true_negatives+false_positives)\n","\n","def r2_variance(y_true,y_predicted):\n","  assert(len(y_true)==len(y_predicted))\n","  for i in y_true:\n","    assert((i == 1) | (i == 0))\n","  return [(i-j)**2 for i,j in zip(y_true,y_predicted)]\n","\n","def r2_score(y_true,y_predicted):\n","  assert(len(y_true)==len(y_predicted))\n","  for i in y_true:\n","    assert((i == 1) | (i == 0))  \n","  a = -3.25\n","  return np.mean([np.log(1+np.exp(a*(1-2*np.abs(i-j)))) for i,j in zip(y_true,y_predicted)])\n","\n","def abs_differences(y_true,y_predicted,index = None):\n","  assert(len(y_true)==len(y_predicted))\n","  for i in y_true:\n","    assert((i == 1) | (i == 0))\n","  return pd.Series([np.abs(i-j) for i,j in zip(y_true,y_predicted)],index=index)\n","\n","plt.rcParams['figure.figsize'] = [16,8]\n","\n","def make_meshgrid(x, y, h=.02):\n","    \"\"\"Create a mesh of points to plot in\n","\n","    Parameters\n","    ----------\n","    x: data to base x-axis meshgrid on\n","    y: data to base y-axis meshgrid on\n","    h: stepsize for meshgrid, optional\n","\n","    Returns\n","    -------\n","    xx, yy : ndarray\n","    \"\"\"\n","    x_min, x_max = x.min(), x.max()\n","    y_min, y_max = y.min(), y.max()\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n","                         np.arange(y_min, y_max, h))\n","    return xx, yy\n","\n","def plot_contours(ax, clf, xx, yy, **params):\n","    \"\"\"Plot the decision boundaries for a classifier.\n","\n","    Parameters\n","    ----------\n","    ax: matplotlib axes object\n","    clf: a classifier\n","    xx: meshgrid ndarray\n","    yy: meshgrid ndarray\n","    params: dictionary of params to pass to contourf, optional\n","    \"\"\"\n","    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n","    Z = Z.reshape(xx.shape)\n","    out = ax.contourf(xx, yy, Z, **params)\n","    return out\n","\n","def binomial(size,rate):\n","  error = np.sqrt(rate*(1-rate)/size)\n","  return error\n","  \n","def out_of_data_z_score(size,rate,new_rate):\n","  return (new_rate-rate)/binomial(size,rate)\n","\n","def comb(n, r):\n","    r = min(r, n-r)\n","    numer = reduce(op.mul, range(n, n-r, -1), 1)\n","    denom = reduce(op.mul, range(1, r+1), 1)\n","    return numer // denom  # or / in Python 2comb(10,2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5WDG7DcFKxsS"},"source":["## Plots"]},{"cell_type":"code","metadata":{"id":"kHHtUjteM6OO"},"source":["#deprecated\n","def tau_plots(model_X,model_y,features_premade = [],show_features = [],random_features = True,features_each = []): #premade is always included, show_features are which part of premade you want to see (usually it won't show premade), features_each includes each feature you want to be a part of premade but you're not sure which and random_features adds other random features\n","  did_appear = {}\n","  for tau in tqdm(np.arange(0.5,7.0,0.1)):\n","    for i in range(10):\n","      random_features_list = []\n","      if random_features:\n","        random_features_list = [feature for feature in model_X.columns.drop(features_premade+features_each) if random.random() > 0.5]\n","      abc = random_features_list+features_premade\n","      if len(features_each) != 0:\n","        for each_feature in features_each:\n","          temp = hard_function(tau = tau, X = model_X[np.append(abc,each_feature)], y=model_y, estimator = estimator_models)\n","          if each_feature in temp:\n","            if each_feature not in did_appear.keys():\n","              did_appear[each_feature] = [tau]\n","            else:\n","              did_appear[each_feature].append(tau)\n","          else:\n","            if each_feature not in did_appear.keys():\n","              did_appear[each_feature] = [None]\n","            else:\n","              did_appear[each_feature].append(None)          \n","      else:\n","        temp = hard_function(tau = tau, X = model_X[abc], y=model_y, estimator = estimator_models)\n","      for feature in show_features+random_features_list:\n","        if feature in temp:\n","          if feature not in did_appear.keys():\n","            did_appear[feature] = [tau]\n","          else:\n","            did_appear[feature].append(tau)\n","        else:\n","          if feature not in did_appear.keys():\n","            did_appear[feature] = [None]\n","          else:\n","            did_appear[feature].append(None)        \n","  for i in did_appear.keys():\n","    fig = plt.figure(i)\n","    plt.plot(np.arange(len(did_appear[i])),did_appear[i])\n","    plt.title(i)\n","    plt.xlim([0,len(did_appear[i])])\n","    plt.ylim([0,7])\n","    plt.grid()\n","  return did_appear\n","\n","def confidence_plot(y,y_low=-0.2,y_high = 1.2, y_step = 0.07, edge = 0.5, flip = True,**models):\n","  warnings.filterwarnings('ignore')\n","  for j,model in models.items():\n","    temp = [np.average(y[(model > i) & (model < i + y_step)]) for i in np.arange(y_low,y_high,y_step)]\n","    if flip:\n","      for k in range(len(np.arange(y_low,y_high,y_step))):\n","        if round(y_low + (k+1/2)*y_step,1) <= edge:\n","          temp[k] = 1-temp[k]\n","    fig1 = plt.figure(figsize=(24,8))\n","\n","    plt.errorbar([np.str(round(i,2)) + \"-\" + np.str(round(i+y_step,2)) for i in np.arange(y_low,y_high,y_step)],temp,yerr = [1/np.sqrt(model[(model > i) & (model < i + y_step)].count()) for i in np.arange(y_low,y_high,y_step)],label=i)\n","    plt.ylim([0,1])\n","\n","  if flip:\n","    plt.title(f'Confidence in result versus y_predicted value, edge = {edge}')\n","  else:\n","    plt.title(f'Average of y versus y_predicted value, edge = {edge}')\n","  plt.legend()\n","  plt.grid()\n","  plt.show()\n","  warnings.resetwarnings()\n","\n","def population_y_predicted(y_low=-0.2,y_high = 1.2, y_step = 0.07,**models):\n","  warnings.filterwarnings('ignore')\n","  for i,model in models.items():\n","    print(i)\n","    temp = [model[(model > i) & (model < i + y_step)].count() for i in np.arange(y_low,y_high,y_step)]\n","    plt.figure(figsize=(24,8))\n","    plt.bar([np.str(round(i,2)) + \"-\" + np.str(round(i+y_step,2)) for i in np.arange(y_low,y_high,y_step)],temp,label=i)\n","    plt.legend()\n","    plt.grid()\n","    plt.show()\n","  warnings.resetwarnings()\n","\n","def delta_accuracy(model1,model2,step = 0.01,ymin=0.0,ymax=1.0):\n","  accuracy_deltas = []\n","  accuracy_deltas_err = []\n","  for i in np.arange(ymin,ymax + step,step):\n","    _,numbers = loading_scores(model1,y,pad=False,edge=i,b_compare=model2,show=False,estimator=estimator_models)\n","    accuracy_delta = numbers[\"Delta_Accuracy\"]\n","    accuracy_delta_err = numbers[\"Delta_Accuracy_Error\"]\n","    accuracy_deltas.append(accuracy_delta)\n","    accuracy_deltas_err.append(accuracy_delta_err)\n","  fig = plt.figure()\n","  x_range = [i for i in np.arange(ymin,ymax+step,step)]\n","  plt.errorbar(x_range,accuracy_deltas,yerr=accuracy_deltas_err)\n","  plt.title(f\"Accuracy difference vs Edge, first_model_accuracy - second_model_accuracy\") \n","  plt.plot(x_range,0*np.ones_like(x_range))\n","  plt.grid()\n","  plt.show()\n","  plt.close()\n","\n","def delta_sensitivity(model1,model2,step = 0.01,ymin=0.1,ymax=0.5):\n","  sensitivity_deltas = []\n","  sensitivity_deltas_err = []\n","  for i in np.arange(ymin,ymax + step,step):\n","    _,numbers = loading_scores(model1,y,pad=False,edge=i,b_compare=model2,show=False,estimator=estimator_models)\n","    sensitivity_delta = numbers['Delta_Sensitivity']\n","    sensitivity_delta_err = numbers['Delta_Sensitivity_Error']\n","    sensitivity_deltas.append(sensitivity_delta)\n","    sensitivity_deltas_err.append(sensitivity_delta_err)\n","  fig = plt.figure()\n","  x_range = [i for i in np.arange(ymin,ymax+step,step)]\n","  plt.errorbar(x_range,sensitivity_deltas,yerr=sensitivity_deltas_err)\n","  plt.title(f\"Sensitivity difference vs Edge, first_model_sensitivity - second_model_sensitivity\") \n","  plt.plot(x_range,0*np.ones_like(x_range))\n","  plt.grid()\n","  plt.show()\n","  plt.close()\n","  return sensitivity_deltas,sensitivity_deltas_err\n","\n","def delta_specificity(model1,model2,step = 0.01,ymin=0.1,ymax=0.5):\n","  specificity_deltas = []\n","  specificity_deltas_err = []\n","  for i in np.arange(ymin,ymax + step,step):\n","    _,numbers = loading_scores(model1,y,pad=False,edge=i,b_compare=model2,show=False,estimator=estimator_models)\n","    specificity_delta = numbers['Delta_Specificity']\n","    specificity_delta_err = numbers['Delta_Specificity_Error']\n","    specificity_deltas.append(specificity_delta)\n","    specificity_deltas_err.append(specificity_delta_err)\n","  fig = plt.figure()\n","  x_range = [i for i in np.arange(ymin,ymax+step,step)]\n","  plt.errorbar(x_range,specificity_deltas,yerr=specificity_deltas_err)\n","  plt.title(f\"Specificity difference vs Edge, first_model_specificity - second_model_specificity\") \n","  plt.plot(x_range,0*np.ones_like(x_range))\n","  plt.grid()\n","  plt.show()\n","  plt.close()\n","  return specificity_deltas,specificity_deltas_err\n","\n","def sensitivity_specificity_edge(model,step = 0.01,ymin=0.0,ymax=1.0,show=False,one_point_one_Sample=False):\n","  sensitivitys_specificitys = []\n","  if one_point_one_Sample:\n","    abc = sorted(model)\n","    _,numbers = loading_scores(model,y,pad=False,edge=abc[0]/2,show=False,estimator=estimator_models)\n","    sensitivity = numbers[\"Sensitivity\"]\n","    specificity = numbers[\"Specificity\"]\n","    sensitivitys_specificitys.append([sensitivity,specificity,abc[0]/2])\n","    for i in range(len(abc)-1):\n","      _,numbers = loading_scores(model,y,pad=False,edge=(abc[i]+abc[i+1])/2,show=False,estimator=estimator_models)\n","      sensitivity = numbers[\"Sensitivity\"]\n","      specificity = numbers[\"Specificity\"]\n","      sensitivitys_specificitys.append([sensitivity,specificity,abc[i]])\n","    _,numbers = loading_scores(model,y,pad=False,edge=(1.0+abc[-1])/2,show=False,estimator=estimator_models)\n","    sensitivity = numbers[\"Sensitivity\"]\n","    specificity = numbers[\"Specificity\"]\n","    sensitivitys_specificitys.append([sensitivity,specificity,(1.0+abc[-1])/2])\n","  else:\n","    for i in np.arange(ymin,ymax + step,step):\n","      _,numbers = loading_scores(model,y,pad=False,edge=i,show=False,estimator=estimator_models)\n","      sensitivity = numbers[\"Sensitivity\"]\n","      specificity = numbers[\"Specificity\"]\n","      sensitivitys_specificitys.append([sensitivity,specificity,i])\n","  return sensitivitys_specificitys\n","\n","def sensitivity_accuracy(step = 0.0025,ymin=0.1,ymax=0.7,sensitivity_range=[0.75,0.95],accuracy_range=[0.70,0.90],xerr=False,yerr=False,**models):\n","  fig = plt.figure()\n","  all_sensitivitys = {}\n","  all_sensitivitys_err = {}\n","  all_accuracies = {}\n","  all_accuracies_err = {}\n","  all_edges = {}\n","  out = {}\n","  for i,model in models.items():\n","    edges = []\n","    sensitivitys = []\n","    sensitivitys_err = []\n","    accuracies = []\n","    accuracies_err = []\n","    for edge in np.arange(ymin,ymax + step,step):\n","      _,numbers = loading_scores(model,y,pad=False,edge=edge,show=False,estimator=estimator_models)\n","      accuracy = numbers[\"Accuracy\"]\n","      accuracy_err = numbers[\"Accuracy_Error\"]\n","      sensitivity = numbers[\"Sensitivity\"]\n","      sensitivity_err = numbers[\"Sensitivity_Error\"]\n","      accuracies.append(accuracy)\n","      accuracies_err.append(accuracy_err)\n","      sensitivitys.append(sensitivity)\n","      sensitivitys_err.append(sensitivity_err)\n","      edges.append(edge)\n","    all_sensitivitys[i] = sensitivitys\n","    all_sensitivitys_err[i] = sensitivitys_err\n","    all_accuracies[i] = accuracies\n","    all_accuracies_err[i] = accuracies_err\n","    all_edges[i] = edges\n","    out[i] = [[a,b,c] for a,b,c in zip(sensitivitys,accuracies,edges)]\n","    if xerr:\n","      if yerr:\n","        #plt.errorbar(sensitivitys,accuracies,label = i)\n","        plt.errorbar(sensitivitys,accuracies,xerr = sensitivitys_err,yerr=accuracies_err,label = i)\n","        #plt.fill_between(np.array(sensitivitys) - np.array(accuracies_err), np.array(accuracies)-np.array(accuracies_err), np.array(accuracies)+np.array(accuracies_err),color='tab:cyan')\n","        #plt.fill_between(np.array(sensitivitys) - 0.5*np.array(accuracies_err), np.array(accuracies)-np.array(accuracies_err), np.array(accuracies)+np.array(accuracies_err),color='tab:cyan')\n","        #plt.fill_between(np.array(sensitivitys), np.array(accuracies)-np.array(accuracies_err), np.array(accuracies)+np.array(accuracies_err),color='tab:cyan')\n","        #plt.fill_between(np.array(sensitivitys) + 0.5*np.array(accuracies_err), np.array(accuracies)-np.array(accuracies_err), np.array(accuracies)+np.array(accuracies_err),color='tab:cyan')\n","        #plt.fill_between(np.array(sensitivitys) + np.array(accuracies_err), np.array(accuracies)-np.array(accuracies_err), np.array(accuracies)+np.array(accuracies_err),color='tab:cyan')\n","      else:\n","        plt.errorbar(sensitivitys,accuracies,xerr = sensitivitys_err,label = i)\n","    else:\n","      if yerr:\n","        plt.errorbar(sensitivitys,accuracies,yerr=accuracies_err,label = i)\n","      else:\n","        plt.errorbar(sensitivitys,accuracies,label = i)\n","    #plt.plot(sensitivitys,accuracies,label = i)\n","  plt.title(f\"Accuracy vs Sensitivity\")\n","  plt.xlim(sensitivity_range)\n","  plt.ylim(accuracy_range)\n","  plt.xlabel(\"Sensitivity\")\n","  plt.ylabel(\"Accuracy\")\n","  plt.legend()\n","  plt.grid()\n","  plt.show()\n","  plt.close()\n","  return out\n","\n","def sensitivity_delta_accuracy(model1,model2,step = 0.0025,ymin=0.0,ymax=1.0,sensitivity_range=[0.0,1.0],accuracy_delta_range=[-0.10,0.10],xerr=False,yerr=False):\n","  sensitivitys1 = sensitivity_specificity_edge(model1,step=step,ymin=ymin,ymax=ymax)\n","  sensitivitys2 = sensitivity_specificity_edge(model2,step=step,ymin=ymin,ymax=ymax)\n","  sensitivitys = []\n","  sensitivitys_err = []\n","  accuracy_deltas = []\n","  accuracy_deltas_err = []\n","  for i in np.arange(ymin,ymax + step,step):\n","    epsilon = 1.0\n","    for sensitivity1,_,edge1 in sensitivitys1:\n","      if np.abs(edge1 - i) < epsilon:\n","        closest_sensitivity1 = sensitivity1\n","        epsilon = np.abs(edge1 - i)\n","    epsilon = 1.0\n","    for sensitivity2,_,edge2 in sensitivitys2:\n","      if np.abs(sensitivity2 - closest_sensitivity1) < epsilon:\n","        closest_edge = edge2\n","        epsilon = np.abs(sensitivity2 - closest_sensitivity1)\n","    edge2 = closest_edge\n","    _,numbers = loading_scores(model1,y,pad=False,edge=i,b_compare=model2,show=False,edge2=edge2,estimator=estimator_models)\n","    sensitivity = numbers[\"Sensitivity\"]\n","    sensitivity_err = numbers[\"Sensitivity_Error\"]\n","    accuracy_delta = numbers[\"Delta_Accuracy\"]\n","    accuracy_delta_err = numbers[\"Delta_Accuracy_Error\"]\n","    accuracy_deltas.append(accuracy_delta)\n","    accuracy_deltas_err.append(accuracy_delta_err)\n","    sensitivitys.append(sensitivity)\n","    sensitivitys_err.append(sensitivity_err)\n","  fig = plt.figure()\n","  if xerr:\n","    if yerr:\n","      plt.errorbar(sensitivitys,accuracy_deltas,xerr = sensitivitys_err,yerr=accuracy_deltas_err)\n","    else:\n","      plt.errorbar(sensitivitys,accuracy_deltas,xerr = sensitivitys_err)\n","  else:\n","    if yerr:\n","      plt.errorbar(sensitivitys,accuracy_deltas,yerr=accuracy_deltas_err)\n","    else:\n","      plt.errorbar(sensitivitys,accuracy_deltas)\n","  plt.title(f\"Accuracy difference vs Sensitivity, first_model_accuracy - second_model_accuracy\") \n","  plt.plot(sensitivitys,0*np.ones_like(sensitivitys))\n","  plt.xlim(sensitivity_range)\n","  plt.ylim(accuracy_delta_range)\n","  plt.grid()\n","  plt.show()\n","  plt.close()\n","  return sensitivitys,sensitivitys_err,accuracy_deltas,accuracy_deltas_err\n","\n","#this one tries not to have 'correlations'. Delta here means edge - edge', delta means model1 - model2.\n","def Delta_sensitivity_delta_specificity(model1,model2,step = 0.0025,ymin=0.0,ymax=1.0,sensitivity_range=[0.0,1.0],specificity_delta_range=None,xerr=False,yerr=False):\n","  specificity_delta = 0.0\n","  sensitivitys1 = sensitivity_specificity_edge(model1,step=step,ymin=ymin,ymax=ymax)\n","  sensitivitys2 = sensitivity_specificity_edge(model2,step=step,ymin=ymin,ymax=ymax)\n","  sensitivitys = []\n","  sensitivitys_err = []\n","  specificity_deltas = []\n","  specificity_deltas_err = []\n","  for i in np.arange(ymin,ymax + step,step):\n","    epsilon = 1.0\n","    for sensitivity1,_,edge1 in sensitivitys1:\n","      if np.abs(edge1 - i) < epsilon:\n","        closest_sensitivity1 = sensitivity1\n","        epsilon = np.abs(edge1 - i)\n","    epsilon = 1.0\n","    for sensitivity2,_,edge2 in sensitivitys2:\n","      if np.abs(sensitivity2 - closest_sensitivity1) < epsilon:\n","        closest_edge = edge2\n","        epsilon = np.abs(sensitivity2 - closest_sensitivity1)\n","    edge2 = closest_edge\n","    _,numbers = loading_scores(model1,y,pad=False,edge=i,b_compare=model2,show=False,edge2=edge2,estimator=estimator_models)\n","    temp1 = specificity_delta\n","    sensitivity = numbers[\"Sensitivity\"]\n","    sensitivity_err = numbers[\"Sensitivity_Error\"]\n","    specificity_delta = numbers[\"Delta_Specificity\"]\n","    specificity_delta_err = numbers[\"Delta_Specificity_Error\"]\n","    specificity_deltas.append(specificity_delta-temp1)\n","    #accuracy_deltas_err.append(accuracy_delta_err)\n","    sensitivitys.append(sensitivity)\n","    #sensitivitys_err.append(sensitivity_err)\n","  fig = plt.figure()\n","  if xerr:\n","    if yerr:\n","      plt.errorbar(sensitivitys,specificity_deltas,xerr = sensitivitys_err,yerr=accuracy_deltas_err)\n","    else:\n","      plt.errorbar(sensitivitys,specificity_deltas,xerr = sensitivitys_err)\n","  else:\n","    if yerr:\n","      plt.errorbar(sensitivitys,specificity_deltas,yerr=accuracy_deltas_err)\n","    else:\n","      plt.errorbar(sensitivitys,specificity_deltas)\n","  plt.title(f\"Delta Specificity difference vs Sensitivity, first_model - second_model\") \n","  plt.plot(sensitivitys,0*np.ones_like(sensitivitys))\n","  plt.xlim(sensitivity_range)\n","  if specificity_delta_range:\n","    plt.ylim(specificity_delta_range)\n","  plt.grid()\n","  plt.show()\n","  plt.close()\n","  return sensitivitys,sensitivitys_err,specificity_deltas,specificity_deltas_err\n","\n","def delta_r2(model1,model2):\n","  _,numbers = loading_scores(model1,y,pad=False,b_compare=model2,show=False,estimator=estimator_models)\n","  r2_delta = numbers[8]\n","  r2_delta_err = numbers[9]\n","  return f\"R^2 of model 1 - R^2 of model 2 = {r2_delta}+-{r2_delta_err}\"\n","\n","def ROC(step = 0.0025,ymin=0.0,ymax=1.0,specificity_range=None,sensitivity_range=None,**models):\n","  fig = plt.figure(figsize=[8,8])\n","  all_sensitivitys = {}\n","  all_sensitivitys_err = {}\n","  all_specificitys = {}\n","  all_specificitys_err = {}\n","  all_edges = {}\n","  for i,model in models.items():\n","    edges = []\n","    sensitivitys = []\n","    sensitivitys_err = []\n","    specificitys = []\n","    specificitys_err = []\n","    for edge in np.arange(ymin,ymax + step,step):\n","      _,numbers = loading_scores(model,y,pad=False,edge=edge,show=False,estimator=estimator_models)\n","      specificity = numbers[\"Specificity\"]\n","      specificity_err = numbers[\"Specificity_Error\"]\n","      sensitivity = numbers[\"Sensitivity\"]\n","      sensitivity_err = numbers[\"Sensitivity_Error\"]\n","      specificitys.append(specificity)\n","      specificitys_err.append(specificity_err)\n","      sensitivitys.append(sensitivity)\n","      sensitivitys_err.append(sensitivity_err)\n","      edges.append(edge)\n","    all_sensitivitys[i] = sensitivitys\n","    all_sensitivitys_err[i] = sensitivitys_err\n","    all_specificitys[i] = specificitys\n","    all_specificitys_err[i] = specificitys_err\n","    all_edges[i] = edges\n","    plt.plot(sensitivitys,specificitys,label = i)\n","  plt.title(f\"Specificity vs Sensitivity\")\n","  if sensitivity_range:\n","    plt.xlim(sensitivity_range)\n","  if specificity_range:\n","    plt.ylim(specificity_range)\n","  plt.xlabel(\"Sensitivity\")\n","  plt.ylabel(\"Specificitys\")\n","  plt.legend()\n","  plt.grid()\n","  plt.show()\n","  plt.close()\n","  return all_sensitivitys,all_specificitys,all_edges\n","\n","def radial_delta_accuracy(model1,model2, ymin=0.0,ymax=1.0, step = 0.0025, smooth = 0, yerr=True,sensitivitys_specificitys1=None,sensitivitys_specificitys2=None,adjust_p_values = False,sensitivity_x = True):\n","  print('When deciding which model is better, we give Sensitivity and Specificity equal \"weight\".')\n","  if sensitivitys_specificitys1 is None:\n","    sensitivitys_specificitys1 = sensitivity_specificity_edge(model1,step=step,ymin=ymin,ymax=ymax,one_point_one_Sample=True)\n","  if sensitivitys_specificitys2 is None:\n","    sensitivitys_specificitys2 = sensitivity_specificity_edge(model2,step=step,ymin=ymin,ymax=ymax,one_point_one_Sample=True)\n","\n","  r_temp = (0,0)\n","  delta_rs = []\n","  edges1 = []\n","  edges2 = []\n","  rs = []\n","  rs_err = []\n","  angles = []\n","  xx = []\n","  for sensitivity1,specificity1,edge1 in sensitivitys_specificitys1:\n","    closest_sensitivity1 = sensitivity1\n","    closest_specificity1 = specificity1\n","    found_edge1 = edge1\n","    #not needed because of one_point_one_Sample\n","    # #make sure we don't have the same point twice\n","    # if len(xx) > 0:\n","    #   if (np.round(closest_sensitivity1,5) != np.round(temp2,5)) | (np.round(closest_specificity1,5) != np.round(temp,5)):\n","    #     if sensitivity_x:\n","    #       xx.append(closest_sensitivity1)\n","    #     else:\n","    #       xx.append(found_edge1)\n","    #   else:\n","    #     continue\n","    # else:\n","    if sensitivity_x:\n","      xx.append(closest_sensitivity1)\n","    else:\n","      xx.append(found_edge1)\n","    temp = closest_specificity1\n","    temp2 = closest_sensitivity1\n","    edges1.append(found_edge1)\n","    epsilon = 1.0\n","    for sensitivity2,specificity2,edge2 in sensitivitys_specificitys2:\n","      if np.sqrt((sensitivity2 - closest_sensitivity1)**2 + (specificity2 - closest_specificity1)**2) < epsilon: #if the sensitivity and specificity have 'equal error bars', then we just find the closest one\n","        closest_edge = edge2\n","        epsilon = np.sqrt((sensitivity2 - closest_sensitivity1)**2 + (specificity2 - closest_specificity1)**2)\n","    edge2 = closest_edge\n","    edges2.append(edge2)\n","    _,numbers = loading_scores(model1,y,pad=False,edge=edge1,b_compare=model2,show=False,edge2=edge2,estimator=estimator_models)\n","    delta_sensitivity = -numbers[\"Delta_Sensitivity\"] # don't ask me why this is -\n","    delta_sensitivity_err = numbers[\"Delta_Sensitivity_Error\"]\n","    delta_specificity = numbers[\"Delta_Specificity\"]\n","    delta_specificity_err = numbers[\"Delta_Specificity_Error\"]\n","    r = np.sqrt(delta_sensitivity**2 + delta_specificity**2)\n","    delta_r = (delta_sensitivity - r_temp[0],delta_specificity - r_temp[1])\n","    delta_r = np.sqrt(np.abs(np.sign(delta_r[0])*delta_r[0]**2 + np.sign(delta_r[1])*delta_r[1]**2)) # here when 'positive' Sensitivity becomes 'positive' Specificity, we don't count it.\n","    delta_rs.append(delta_r)\n","    r_temp = (delta_sensitivity,delta_specificity)\n","    if r != 0.0:\n","      df_1 = delta_sensitivity/r\n","      df_2 = delta_specificity/r\n","      r_err = np.sqrt(df_1**2*delta_sensitivity_err**2 + df_2**2*delta_specificity_err**2)\n","    else:\n","      r_err = np.sqrt(1/2*delta_sensitivity_err**2 + 1/2*delta_specificity_err**2) # here it depends on the direction that r=0 is achieved. We use 45 degrees.\n","    rs.append(r)\n","    rs_err.append(r_err)\n","    if (delta_specificity == 0.0) & (delta_sensitivity > 0.0):\n","      angle = 0.0\n","    elif (delta_specificity == 0.0) & (delta_sensitivity < 0.0):\n","      angle = 180.0\n","    else: \n","      if r != 0.0:\n","        angle = 180.0/np.pi*2.0*np.arctan(delta_specificity/(delta_sensitivity+r)) #this goes from -180 to 180\n","      else:\n","        angle = np.nan\n","    angles.append(angle)\n","\n","  if smooth > 0:\n","    print(f\"'smooth' {2*smooth + 1} points: We think about angles in two opposite buckets: model 1 better and model 2 better. Really these have to be added as Vectors.\")\n","    print(\"'smooth': Use only for points with the same direction.\")\n","    rs_err = [i**2 for i in rs_err]\n","    # i_max = len(rs) - 2*two_smooth - 1\n","    # j_max = 2*two_smooth - 1\n","    # i_max + j_max + 1 = len(rs) - 1\n","    for i in range(len(rs)-2*smooth):\n","      for j in range(2*smooth):\n","        rs[i] += rs[i+j+1]\n","        rs_err[i] += rs_err[i+j+1] # these need to be added like x**2\n","        xx[i] += xx[i+j+1]\n","    rs = rs[:-2*smooth]\n","    rs_err = rs_err[:-2*smooth]\n","    xx = xx[:-2*smooth]\n","    rs = [i/(2*smooth + 1) for i in rs]\n","    rs_err = [np.sqrt(i/(2*smooth + 1)) for i in rs_err]\n","    xx = [i/(2*smooth + 1) for i in xx]\n","    edges1 = edges1[smooth:-smooth]\n","    edges2 = edges2[smooth:-smooth]\n","    angles = angles[smooth:-smooth]\n","\n","  col = []\n","  for i in range(len(xx)):\n","    if (-180<=angles[i]<-45) or (135<angles[i]<=180): # we use Sensitivity == Specificity in our cost.\n","        col.append('blue')  \n","    elif -45<angles[i]<=135:\n","        col.append('orange') \n","        rs[i] = -rs[i]\n","        delta_rs[i] = -delta_rs[i]\n","    else:\n","        col.append('green')  # this is np.nan for r=0 \n","  \n","  fig2 = plt.figure()\n","  plt.scatter(xx,angles,c=col)\n","  plt.plot(xx,angles)\n","  plt.title(f\"Direction from first model to closest point on second model\") \n","  if sensitivity_x:\n","    plt.xlabel(\"Sensitivity1\")\n","  else:\n","    plt.xlabel(\"Edge1\")\n","  plt.ylabel(\"Angle(-180 to 180 degrees)\")  \n","  plt.grid()\n","  legend_elements = [Line2D([0], [0], marker='o', color='w', label='first_better',\n","                          markerfacecolor='blue', markersize=8),\n","                     Line2D([0], [0], marker='o', color='w', label='second_better',\n","                          markerfacecolor='orange', markersize=8),\n","                     Line2D([0], [0], marker='o', color='w', label='unsure',\n","                          markerfacecolor='green', markersize=8)]\n","  plt.legend(handles=legend_elements, loc=\"upper left\")  \n","  plt.show()\n","\n","  fig = plt.figure()\n","  plt.scatter(xx,rs,c=col)\n","  if yerr:\n","    plt.errorbar(xx,rs,yerr=rs_err)\n","  else:\n","    plt.errorbar(xx,rs)\n","  if sensitivity_x:\n","    plt.xlabel(\"Sensitivity1\")\n","    plt.title(f\"Radial Delta Accuracy vs Sensitivity1, first_model - second_model\") \n","  else:\n","    plt.xlabel(\"Edge1\")\n","    plt.title(f\"Radial Delta Accuracy vs Edge1, first_model - second_model\") \n","  plt.ylabel(\"Radial Delta Accuracy\")\n","  plt.grid()\n","  legend_elements = [Line2D([0], [0], marker='o', color='w', label='first_better',\n","                          markerfacecolor='blue', markersize=8),\n","                     Line2D([0], [0], marker='o', color='w', label='second_better',\n","                          markerfacecolor='orange', markersize=8),\n","                     Line2D([0], [0], marker='o', color='w', label='unsure',\n","                          markerfacecolor='green', markersize=8)]\n","  plt.legend(handles=legend_elements, loc=\"upper left\")  \n","  plt.show()\n","\n","  for i in range(len(rs_err)):\n","    if (np.isnan(rs_err[i])) & (rs[i]==0.0):\n","      rs_err[i] = 0.000001\n","  z_scores = [i/j for i,j in zip(rs,rs_err)]\n","  if adjust_p_values:\n","    z_scores = adjusted_p_values(z_scores)\n","  scatter = plt.scatter(xx,z_scores,c=col)\n","  plt.plot(xx,z_scores)\n","  legend_elements = [Line2D([0], [0], marker='o', color='w', label='first_better',\n","                          markerfacecolor='blue', markersize=8),\n","                     Line2D([0], [0], marker='o', color='w', label='second_better',\n","                          markerfacecolor='orange', markersize=8),\n","                     Line2D([0], [0], marker='o', color='w', label='unsure',\n","                          markerfacecolor='green', markersize=8)]\n","  plt.legend(handles=legend_elements, loc=\"upper left\")\n","  plt.title(f\"Z-scores\") \n","  if sensitivity_x:\n","    plt.xlabel(\"Sensitivity1\")\n","  else:\n","    plt.xlabel(\"Edge1\")\n","  plt.ylabel(\"Z-scores\")\n","  plt.grid()\n","  plt.show()\n","\n","  fig = plt.figure()\n","  # if sensitivity_x:\n","  #   if yerr:\n","  #     plt.scatter(xx,delta_rs,c=col)\n","  #     #plt.errorbar(xx,rs,yerr=rs_err)\n","  #   else:\n","  #     plt.scatter(xx,delta_rs,c=col)\n","  #     plt.errorbar(xx,delta_rs)\n","  #   plt.xlabel(\"Sensitivity1\")\n","  #   plt.title(f\"Delta Radial Delta Accuracy vs Sensitivity1, first_model - second_model\") \n","  # else:\n","  plt.scatter(range(len(delta_rs)),delta_rs,c=col)\n","  if sensitivity_x:\n","    plt.xlabel(\"Sensitivity1\")\n","  else:\n","    plt.xlabel(\"Edge1\")\n","  plt.xticks(np.arange(0, len(delta_rs), step=20), np.round(xx[::20],3))\n","  plt.title(f\"Delta Radial Delta Accuracy, each point is a person, first_model - second_model\") \n","  plt.ylabel(\"Delta Radial Delta Accuracy\")\n","  plt.grid()\n","  legend_elements = [Line2D([0], [0], marker='o', color='w', label='first_better',\n","                          markerfacecolor='blue', markersize=8),\n","                    Line2D([0], [0], marker='o', color='w', label='second_better',\n","                          markerfacecolor='orange', markersize=8),\n","                    Line2D([0], [0], marker='o', color='w', label='unsure',\n","                          markerfacecolor='green', markersize=8)]\n","  plt.legend(handles=legend_elements, loc=\"upper left\")  \n","  plt.show()\n","\n","  plt.close()\n","\n","  result = {}\n","  result['delta_rs'] = delta_rs\n","  result['edges1'] = edges1\n","  result['edges2'] = edges2\n","  result['rs'] = rs\n","  result['rs_err'] = rs_err\n","  result['angles'] = angles\n","  result['sensitivitys_specificitys1'] = sensitivitys_specificitys1\n","  result['sensitivitys_specificitys2'] = sensitivitys_specificitys2\n","\n","  return result\n","\n","def fdr(p_vals):\n","  ranked_p_values = rankdata(p_vals)\n","  fdr = p_vals * len(p_vals) / ranked_p_values\n","  for i in range(len(fdr)-1):\n","    if fdr[i]>fdr[i+1]:\n","      fdr[i]=fdr[i+1]\n","  fdr[fdr > 1] = 1\n","  return fdr\n","\n","def adjusted_p_values(z_scores):\n","  p_values = norm.sf(z_scores)*2 #twoside\n","  adjusted_p_values = fdr(p_values)\n","  adjusted_z_scores = norm.ppf(1-adjusted_p_values/2)\n","  return adjusted_z_scores\n","\n","def log_likelihood(first_model,y,y_low=0.0,y_high = 1.0, y_step = 0.04, flip = True):\n","  warnings.filterwarnings('ignore')\n","  \n","  delta_y = np.abs(y - first_model)\n","\n","  temp = [len([delta for delta in delta_y if ((delta > i) & (delta < i + y_step))]) for i in np.arange(y_low,y_high,y_step)]\n","\n","  temp = [i/len(y) for i in temp]\n","\n","  temp = np.log(temp)\n","\n","  fig1 = plt.figure(figsize=(24,8))\n","\n","  plt.errorbar([np.str(round(i,2)) + \"-\" + np.str(round(i+y_step,2)) for i in np.arange(y_low,y_high,y_step)],temp)\n","  \n","  regr = linear_model.LinearRegression()\n","\n","  regr.fit(np.reshape([ i+y_step/2 for i in np.arange(y_low,y_high,y_step)],(-1, 1)), temp)\n","\n","  line = regr.predict(np.reshape([ i+y_step/2 for i in np.arange(y_low,y_high,y_step)],(-1, 1)))\n","\n","  plt.plot([np.str(round(i,2)) + \"-\" + np.str(round(i+y_step,2)) for i in np.arange(y_low,y_high,y_step)],line)\n","\n","  plt.show()\n","  warnings.resetwarnings()\n","\n","  return regr.coef_[0],regr.intercept_\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zbGKwTDqK5SB"},"source":["## Estimators and Cross-correlation"]},{"cell_type":"code","metadata":{"id":"IohAPE3VK2XE"},"source":["def sigmoid(x):\n","  return 1/(1+np.exp(-x))\n","\n","def log_odds_ratio(y):\n","  return np.log(y/(1-y))\n","\n","def estimator_models(X,y,index,edge=0.5,b_compare=None,edge2=None):\n","  if edge2 is None:\n","    edge2 = edge\n","  b_predicted_2 = np.rint(X>edge)\n","\n","  accuracy = accuracy_score(y,b_predicted_2)\n","  accuracy_err = np.sqrt(accuracy*(1-accuracy)/len(y)) #thinking of this as a binomial problem\n","\n","  abs_differences_ = abs_differences(y,X,index)\n","  def likelihood_error(error,a):\n","    return 1/(1+np.exp(a*(1-2*error)))\n","  a = 3.3\n","  abs_differences_sum = np.sum([likelihood_error(i,a) for i in abs_differences_])\n","  abs_differences_err_sum = np.std([likelihood_error(i,a) for i in abs_differences_])*np.sqrt(len(abs_differences_))\n","  # this is really not the same likelihood function that you can use for y_abs_difference, but usually this is what people do\n","  y_abs_differences = [likelihood_error(np.abs(i-np.mean(y)),a) for i in y]\n","  y_abs_differences_sum = np.sum(y_abs_differences)\n","  y_abs_differences_err_sum = np.std(y_abs_differences)*np.sqrt(len(y_abs_differences))\n","  r2 = 1-abs_differences_sum/y_abs_differences_sum\n","  r2_err = np.sqrt(1.0/y_abs_differences_sum**2*abs_differences_err_sum**2+(abs_differences_sum/y_abs_differences_sum**2)**2*y_abs_differences_err_sum**2)\n","\n","  sensitivity = sensitivity_score(y,b_predicted_2)\n","  sensitivity_sample = np.count_nonzero(y == 1)\n","  sensitivity_err = np.sqrt(sensitivity*(1-sensitivity)/sensitivity_sample)\n","\n","  specificity = specificity_score(y,b_predicted_2)\n","  specificity_sample = np.count_nonzero(y == 0)\n","  specificity_err = np.sqrt(specificity*(1-specificity)/specificity_sample)\n","\n","  confusion = np.array(confusion_matrix(y,b_predicted_2))\n","\n","  auc = roc_auc_score(y,X)\n","\n","  result = {}\n","  result['accuracy'] = accuracy\n","  result['accuracy_err'] = accuracy_err\n","  result['r2'] = r2\n","  result['r2_err'] = r2_err\n","  result['sensitivity'] = sensitivity\n","  result['sensitivity_err'] = sensitivity_err\n","  result['specificity'] = specificity\n","  result['specificity_err'] = specificity\n","  result['confusion'] = confusion\n","  result['auc'] = auc\n","\n","  if b_compare is not None:\n","    b_compare_test = b_compare\n","    b_compare_test_2 = np.rint(b_compare > edge2)\n","    compare_accuracy = accuracy_score(y,b_compare_test_2)\n","    compare_r2 = r2_score(y,b_compare_test)\n","    sensitivity_0 = np.sum([np.rint((a==b) & (c == 1)) for a,b,c in zip(b_predicted_2,b_compare_test_2,y)])\n","    sensitivity_neg_1 = np.sum([np.rint((a<b) & (c == 1)) for a,b,c in zip(b_predicted_2,b_compare_test_2,y)])\n","    sensitivity_pos_1 = np.sum([np.rint((a>b) & (c == 1)) for a,b,c in zip(b_predicted_2,b_compare_test_2,y)])\n","    specificity_0 = np.sum([np.rint((a==b) & (c == 0)) for a,b,c in zip(b_predicted_2,b_compare_test_2,y)])\n","    specificity_neg_1 = np.sum([np.rint((a<b) & (c == 0)) for a,b,c in zip(b_predicted_2,b_compare_test_2,y)])\n","    specificity_pos_1 = np.sum([np.rint((a>b) & (c == 0)) for a,b,c in zip(b_predicted_2,b_compare_test_2,y)])\n","    delta_r2 = r2 - compare_r2\n","#    delta_r2_err = np.std() who needs this\n","    delta_accuracy = accuracy - compare_accuracy\n","    #this doesn't work because it's a - b which is not binomial\n","    #delta_accuracy_err = np.sqrt(accuracy*(1-accuracy)/len(y))\n","    delta_accuracy_err = np.std(b_predicted_2-b_compare_test_2)/np.sqrt(len(y))\n","    result['delta_accuracy'] = delta_accuracy\n","    result['delta_accuracy_err'] = delta_accuracy_err\n","    result['delta_r^2'] = delta_r2\n","#    result['delta_r^2_err'] = delta_r2_err\n","    result['delta_sensitivitys'] = [sensitivity_0,sensitivity_neg_1,sensitivity_pos_1]\n","    result['delta_specificitys'] = [specificity_0,specificity_neg_1,specificity_pos_1] \n","    result['specificity_sample'] = specificity_sample\n","    result['sensitivity_sample'] = sensitivity_sample\n","  return result\n","\n","def estimator_SVD(A_train,A_test,b_train,b_test,test_index,train_index,r = 0,q=0,p=0,edge=0.5,l2_reg=1.0): #q,p are not used\n","  if len(A_train.shape) == 1:\n","    b_predicted_test = A_test\n","    b_predicted_test_2 = [int(row > edge) for row in b_predicted_test]\n","    b_predicted_train = A_train\n","    b_predicted_train_2 = [int(row > edge) for row in b_predicted_train]\n","    x_fit = 1\n","    S = 1\n","  else:\n","    U,S,VT = np.linalg.svd(A_train,full_matrices=False)\n","    if r != 0:\n","      U = U[:,:r+1]\n","      S = S[:r+1]\n","      VT = VT[:r+1,:]\n","    S = np.diag(S)\n","    A_train_inv = VT.T @ np.linalg.inv(S) @ U.T\n","    x_fit = A_train_inv @ b_train\n","    b_predicted_test = A_test @ x_fit\n","    b_predicted_test_2 = [int(row > edge) for row in b_predicted_test]\n","    b_predicted_train = A_train @ x_fit\n","    b_predicted_train_2 = [int(row > edge) for row in b_predicted_train]\n","  test_accuracy = accuracy_score(b_test,b_predicted_test_2)\n","  train_accuracy = accuracy_score(b_train,b_predicted_train_2)\n","  #test_r2 = r2_score(b_test,b_predicted_test)\n","  test_abs_differences = abs_differences(b_test,b_predicted_test,test_index)\n","  train_abs_differences = abs_differences(b_train,b_predicted_train,train_index)\n","  test_sensitivity = sensitivity_score(b_test,b_predicted_test_2)\n","  train_sensitivity = sensitivity_score(b_train,b_predicted_train_2)\n","  test_specificity = specificity_score(b_test,b_predicted_test_2)\n","  train_specificity = specificity_score(b_train,b_predicted_train_2) \n","  if np.isnan(test_sensitivity):\n","    test_sensitivity = 0 # this is if we have 0 'sample size'\n","  if np.isnan(train_sensitivity):\n","    train_sensitivity = 0\n","  if np.isnan(test_specificity):\n","    test_specificity = 0 # this is if we have 0 'sample size'\n","  if np.isnan(train_specificity):\n","    train_specificity = 0\n","  if np.isnan(test_accuracy):\n","    test_accuracy = 0 # this is if we have 0 'sample size'\n","  confusion = confusion_matrix(b_test,b_predicted_test_2)\n","  result = {}\n","  result['test_accuracy'] = test_accuracy\n","  result['train_accuracy'] = train_accuracy\n","  result['x_fit'] = x_fit\n","  #result['test_r^2'] = test_r2\n","  result['test_abs_differences'] = test_abs_differences\n","  result['train_abs_differences'] = train_abs_differences\n","  result['test_sensitivity'] = test_sensitivity\n","  result['train_sensitivity'] = train_sensitivity\n","  result['b_predicted_test_2'] = b_predicted_test_2\n","  result['S'] = S\n","  result['confusion'] = confusion\n","  result['test_specificity'] = test_specificity\n","  result['train_specificity'] = train_specificity\n","  result['b_predicted_test'] = b_predicted_test\n","  result['b_predicted_train'] = b_predicted_train\n","  return result\n","\n","def cross_correlate_with_two_features(X,y,features1,features2,pad,estimator,method,nr_folds = 30, edge=0.5,c=None,gamma=None,show=False,svd_fit = True,svm_dim=2):\n","  result = {}\n","  result['test_accuracy'] = []\n","  result['train_accuracy'] = []\n","  result['x_fit'] = []\n","  result['test_abs_differences'] = pd.Series(dtype='float64',index=X.index)\n","  result['train_abs_differences'] = pd.DataFrame(dtype='float64',index=X.index)\n","  result['test_sensitivity'] = []\n","  result['train_sensitivity'] = []\n","  result['test_specificity'] = []\n","  result['train_specificity'] = []\n","  result['confusion'] = []\n","  result['test_sensitivity_sample'] = []\n","  result['test_specificity_sample'] = []\n","  result['train_sensitivity_sample'] = []\n","  result['train_specificity_sample'] = []\n","  result['b_train'] = []\n","  result['b_predicted_test'] = pd.Series(dtype='float64',index=X.index)\n","  result['which_model'] = []\n","  kf = KFold(n_splits=nr_folds,shuffle=True)\n","  gen = kf.split(X)\n","  gens = []\n","  for generator in gen:\n","    gens.append(generator)\n","  i = 0\n","  for generator in gens:\n","    i+=1\n","    A_train_1, A_test_1 = X[features1].iloc[generator[0]].to_numpy(), X[features1].iloc[generator[1]].to_numpy()\n","    A_train_2, A_test_2 = X[features2].iloc[generator[0]].to_numpy(), X[features2].iloc[generator[1]].to_numpy()\n","    test_index = X.iloc[generator[1]].index\n","    train_index = X.iloc[generator[0]].index\n","    b_train, b_test = np.rint(y.iloc[generator[0]].to_numpy()), np.rint(y.iloc[generator[1]].to_numpy())\n","    for i in b_train:\n","      assert((i == 1) | (i == 0))\n","    for i in b_test:\n","      assert((i == 1) | (i == 0))\n","    if pad:\n","      A_train_1 = np.pad(A_train_1,[(0,0),(0,1)],constant_values=1)\n","      A_test_1 = np.pad(A_test_1,[(0,0),(0,1)],constant_values=1)\n","      A_train_2 = np.pad(A_train_2,[(0,0),(0,1)],constant_values=1)\n","      A_test_2 = np.pad(A_test_2,[(0,0),(0,1)],constant_values=1)    \n","    out_1 = estimator(A_train_1,A_test_1,b_train,b_test,test_index,train_index,edge=edge)\n","    out_2 = estimator(A_train_2,A_test_2,b_train,b_test,test_index,train_index,edge=edge)\n","    b_train = pd.Series(b_train,index=train_index)\n","    b_test = pd.Series(b_test,index=test_index)\n","    b_predicted_test_1 = pd.Series(out_1['b_predicted_test'],index=test_index)\n","    b_predicted_train_1 = pd.Series(out_1['b_predicted_train'],index=train_index)\n","    b_predicted_test_2 = pd.Series(out_2['b_predicted_test'],index=test_index)\n","    b_predicted_train_2 = pd.Series(out_2['b_predicted_train'],index=train_index)\n","\n","    if method == 'svm':\n","      if svm_dim == 2:\n","        X_k_neighbors_train = np.column_stack((b_predicted_train_1,b_predicted_train_2)) # using both models for svm\n","        X_k_neighbors_test = np.column_stack((b_predicted_test_1,b_predicted_test_2))\n","      else:\n","        X_k_neighbors_train = b_predicted_train_1.to_numpy().reshape(-1,1) # using just model1 for svm\n","        X_k_neighbors_test = b_predicted_test_1.to_numpy().reshape(-1,1)\n","      b_best_train = pd.Series(dtype='float64',index=train_index)\n","      for i in train_index:\n","        if np.abs(b_predicted_train_1[i] - b_train[i]) < np.abs(b_predicted_train_2[i] - b_train[i]):\n","          b_best_train.loc[i] = 1.0\n","        else:\n","          b_best_train.loc[i] = 0.0\n","      if gamma is None:\n","        gamma='scale'\n","      if c is None:\n","        c = 1.0\n","      clf = svm.SVC(kernel='rbf', gamma=gamma, C=c)\n","      #clf = svm.SVC(kernel='poly', gamma=gamma, C=c,degree=2) # this is really slow\n","      clf.fit(X_k_neighbors_train, b_best_train)\n","      which_model_train = pd.Series(clf.predict(X_k_neighbors_train),index=train_index)\n","      which_model_test = pd.Series(clf.predict(X_k_neighbors_test),index=test_index)\n","      \n","      def helper(a,b,c):\n","        assert ((a==1.0) | (a == 0.0))\n","        if a == 1.0:\n","          return b\n","        else:\n","          return c\n","\n","      def divide(which_model,index):\n","        group1 = []\n","        group2 = []\n","        for i,j in zip(which_model,index):\n","          if i == 1.0:\n","            group1.append(j)\n","          else:\n","            group2.append(j)\n","        return group1,group2\n","\n","      if svd_fit:    \n","        group1_train,group2_train = divide(which_model_train,train_index)\n","        group1_test,group2_test = divide(which_model_test,test_index)\n","\n","        b_predicted_train_1_group_1 = b_predicted_train_1.loc[group1_train]\n","        b_predicted_train_2_group_1 = b_predicted_train_2.loc[group1_train]\n","        b_predicted_train_group_1 = np.column_stack((b_predicted_train_1_group_1.to_numpy(),b_predicted_train_2_group_1.to_numpy()))\n","        b_predicted_train_1_group_2 = b_predicted_train_1.loc[group2_train]\n","        b_predicted_train_2_group_2 = b_predicted_train_2.loc[group2_train]\n","        b_predicted_train_group_2 = np.column_stack((b_predicted_train_1_group_2.to_numpy(),b_predicted_train_2_group_2.to_numpy()))\n","        b_predicted_test_1_group_1 = b_predicted_test_1.loc[group1_test]\n","        b_predicted_test_2_group_1 = b_predicted_test_2.loc[group1_test]\n","        b_predicted_test_group_1 = np.column_stack((b_predicted_test_1_group_1.to_numpy(),b_predicted_test_2_group_1.to_numpy()))\n","        b_predicted_test_1_group_2 = b_predicted_test_1.loc[group2_test]\n","        b_predicted_test_2_group_2 = b_predicted_test_2.loc[group2_test]\n","        b_predicted_test_group_2 = np.column_stack((b_predicted_test_1_group_2.to_numpy(),b_predicted_test_2_group_2.to_numpy()))\n","        out_group_1 = estimator_SVD(b_predicted_train_group_1,b_predicted_test_group_1,b_train.loc[group1_train],b_test.loc[group1_test],group1_test,group1_train,edge=edge)\n","        out_group_2 = estimator_SVD(b_predicted_train_group_2,b_predicted_test_group_2,b_train.loc[group2_train],b_test.loc[group2_test],group2_test,group2_train,edge=edge)\n","        b_predicted_test_group_1 = pd.Series(out_group_1['b_predicted_test'],index=group1_test)\n","        b_predicted_train_group_1 = pd.Series(out_group_1['b_predicted_train'],index=group1_train)\n","        b_predicted_test_group_2 = pd.Series(out_group_2['b_predicted_test'],index=group2_test)\n","        b_predicted_train_group_2 = pd.Series(out_group_2['b_predicted_train'],index=group2_train)\n","        b_predict_train = pd.Series(index=train_index)\n","        b_predict_test = pd.Series(index=test_index)\n","        b_predict_train.loc[b_predicted_train_group_1.index] = b_predicted_train_group_1.values\n","        b_predict_train.loc[b_predicted_train_group_2.index] = b_predicted_train_group_2.values\n","        b_predict_test.loc[b_predicted_test_group_1.index] = b_predicted_test_group_1.values\n","        b_predict_test.loc[b_predicted_test_group_2.index] = b_predicted_test_group_2.values    \n","\n","      # this is for when we want to use model1 or model2, and not model1+model2 for each group\n","      else:\n","        b_predict_train = pd.Series([helper(a,b,c) for a,b,c in zip(which_model_train,b_predicted_train_1,b_predicted_train_2)],index=train_index)\n","        b_predict_test = pd.Series([helper(a,b,c) for a,b,c in zip(which_model_test,b_predicted_test_1,b_predicted_test_2)],index=test_index)\n","\n","      if show:\n","        if method == 'svm':\n","          X0, X1 = b_predicted_train_1, b_predicted_train_2\n","          xx, yy = make_meshgrid(b_predicted_train_1, b_predicted_train_2)\n","          fig, sub = plt.subplots(1, 1)\n","          plt.subplots_adjust(wspace=0.02, hspace=0.02)\n","          if svm_dim == 2:\n","            sub.contourf(xx, yy, clf.predict(np.c_[xx.ravel(),yy.ravel()]).reshape(xx.shape), cmap=plt.cm.coolwarm, alpha=0.8)\n","          else:\n","            sub.contourf(xx, yy, clf.predict(np.c_[xx.ravel()]).reshape(xx.shape), cmap=plt.cm.coolwarm, alpha=0.8)\n","          plt.scatter(X0, X1, c=b_best_train, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n","          plt.xlim([xx.min(), xx.max()])\n","          plt.ylim([yy.min(), yy.max()])\n","          plt.xlabel('log_model1')\n","          plt.ylabel('log_model2')\n","          plt.xticks(())\n","          plt.yticks(())\n","          plt.show()\n","\n","    elif method == 'lin':\n","      b_predicted_train = np.column_stack((b_predicted_train_1,b_predicted_train_2))\n","      b_predicted_test = np.column_stack((b_predicted_test_1,b_predicted_test_2))\n","      b_predicted_train = np.pad(b_predicted_train,[(0,0),(0,1)],constant_values=1)\n","      b_predicted_test = np.pad(b_predicted_test,[(0,0),(0,1)],constant_values=1)\n","      U,S,VT = np.linalg.svd(b_predicted_train,full_matrices=False)\n","      S = np.diag(S)\n","      b_predicted_train_inv = VT.T @ np.linalg.inv(S) @ U.T\n","      x_fit = b_predicted_train_inv @ b_train\n","      b_predict_test = pd.Series(b_predicted_test @ x_fit,index=test_index)\n","      b_predict_train = pd.Series(b_predicted_train @ x_fit,index=train_index)\n","      if show:\n","        print(f'Model1 loading score = {x_fit[0]}, model2 loading score = {x_fit[1]}, intercept {x_fit[2]}')\n","\n","\n","    elif method == 'log':\n","      b_predicted_train = np.column_stack((b_predicted_train_1,b_predicted_train_2))\n","      b_predicted_test = np.column_stack((b_predicted_test_1,b_predicted_test_2))\n","      model = LogisticRegression(max_iter = 1000,fit_intercept=True,solver='liblinear')\n","      model.fit(b_predicted_train,b_train)\n","      b_predict_train = pd.Series(model.predict_proba(b_predicted_train)[:, 1],index=train_index)\n","      b_predict_test = pd.Series(model.predict_proba(b_predicted_test)[:, 1],index=test_index)\n","      x_fit = np.append(model.coef_[0],model.intercept_[0])\n","      if show:\n","        print(f'Model1 loading score = {x_fit[0]}, model2 loading score = {x_fit[1]}, intercept {x_fit[2]}')\n","\n","    else:\n","      assert False\n","      \n","    b_predict_test_int = b_predict_test.apply(lambda i: np.float(i > edge)) # these are 0 or 1\n","    b_predict_train_int = b_predict_train.apply(lambda i: np.float(i > edge)) # these are 0 or 1\n","    test_accuracy = accuracy_score(b_test,b_predict_test_int)\n","    train_accuracy = accuracy_score(b_train,b_predict_train_int)\n","    test_sensitivity = sensitivity_score(b_test,b_predict_test_int)\n","    train_sensitivity = sensitivity_score(b_train,b_predict_train_int)\n","    test_specificity = specificity_score(b_test,b_predict_test_int)\n","    train_specificity = specificity_score(b_train,b_predict_train_int) \n","    if np.isnan(test_sensitivity):\n","      test_sensitivity = 0 # this is if we have 0 'sample size'\n","    if np.isnan(train_sensitivity):\n","      train_sensitivity = 0\n","    if np.isnan(test_specificity):\n","      test_specificity = 0 # this is if we have 0 'sample size'\n","    if np.isnan(train_specificity):\n","      train_specificity = 0\n","    test_abs_differences = abs_differences(b_test,b_predict_test,test_index)\n","    train_abs_differences = abs_differences(b_train,b_predict_train,train_index)\n","    confusion = confusion_matrix(b_test,b_predict_test_int)\n","    test_sensitivity_sample = np.count_nonzero(b_test == 1)\n","    test_specificity_sample = np.count_nonzero(b_test == 0)\n","    train_sensitivity_sample = np.count_nonzero(b_train == 1)\n","    train_specificity_sample = np.count_nonzero(b_train == 0)  \n","    result['test_accuracy'].append(test_accuracy)\n","    result['train_accuracy'].append(train_accuracy)\n","    result['test_abs_differences'].loc[test_abs_differences.index] = test_abs_differences.values\n","    result['train_abs_differences'].loc[train_abs_differences.index,i] = train_abs_differences.values\n","    result['test_sensitivity'].append(test_sensitivity)\n","    result['train_sensitivity'].append(train_sensitivity)\n","    result['test_specificity'].append(test_specificity)\n","    result['train_specificity'].append(train_specificity)\n","    result['test_sensitivity_sample'].append(test_sensitivity_sample)\n","    result['test_specificity_sample'].append(test_specificity_sample)\n","    result['train_sensitivity_sample'].append(train_sensitivity_sample)\n","    result['train_specificity_sample'].append(train_specificity_sample)\n","    result['confusion'].append(confusion)\n","    result['b_train'].append(b_train)\n","    result['b_predicted_test'].loc[b_predict_test.index] = b_predict_test.values\n","    result['x_fit'].append([out_1['x_fit'],out_2['x_fit']])\n","  return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dO68gHLc8IDN"},"source":["class NNet(nn.Module):\n","    def __init__(self, input_dim, hidden_layer_sizes, loss=nn.BCELoss(), sigmoid=False):\n","        super().__init__()\n","      \n","        self.input_dim = input_dim\n","        self.layer_sizes = hidden_layer_sizes\n","        self.iter = 0\n","        # The loss function could be MSE or BCELoss depending on the problem\n","        self.lossFct = loss\n","\n","        # We leave the optimizer empty for now to assign flexibly\n","        self.optim = None\n","\n","        hidden_layer_sizes = [input_dim] + hidden_layer_sizes\n","        last_layer = nn.Linear(hidden_layer_sizes[-1], 1)\n","        self.layers =\\\n","            [nn.Sequential(nn.Linear(input_, output_), nn.ReLU())\n","            for input_, output_ in \n","            zip(hidden_layer_sizes, hidden_layer_sizes[1:])] +\\\n","            [last_layer]\n","        \n","        # The output activation depends on the problem\n","        if sigmoid:\n","            self.layers = self.layers + [nn.Sigmoid()]\n","            \n","        self.layers = nn.Sequential(*self.layers)\n","\n","    def compute_l1_loss(self, w):\n","        return torch.abs(w).sum()\n","\n","    def forward(self, x):\n","        x = self.layers(x)\n","        return x\n","    \n","    def train(self, data_loader, epochs, validation_data=None):\n","\n","        for epoch in range(epochs):\n","            running_loss = self._train_iteration(data_loader)\n","            val_loss = None\n","            if validation_data is not None:\n","                y_hat = self(validation_data['X'])\n","                val_loss = self.lossFct(input=y_hat, target=validation_data['y']).detach().numpy()\n","            #     print('[%d] loss: %.3f | validation loss: %.3f' %\n","            #       (epoch + 1, running_loss, val_loss))\n","            # else:\n","            #     print('[%d] loss: %.3f' %\n","            #       (epoch + 1, running_loss))\n","\n","    def _train_iteration(self,data_loader):\n","        running_loss = 0.0\n","        for i, (X,y) in enumerate(data_loader):\n","            \n","            X = X.float()\n","            y = y.unsqueeze(1).float()\n","            \n","            X_ = Variable(X, requires_grad=True)\n","            y_ = Variable(y)\n","              \n","            ### Comment out the typical gradient calculation\n","#             pred = self(X)\n","#             loss = self.lossFct(pred, y)\n","            \n","#             self.optim.zero_grad()\n","#             loss.backward()\n","            \n","            ### Add the closure function to calculate the gradient.\n","            def closure():\n","                if torch.is_grad_enabled():\n","                    self.optim.zero_grad()\n","                output = self(X_)\n","                \n","                loss = self.lossFct(output, y_)\n","\n","                # Compute L1 loss component\n","                l1_weight = 0.0\n","                l1_parameters = []\n","                for parameter in self.parameters():\n","                    l1_parameters.append(parameter.view(-1))\n","                l1 = l1_weight * self.compute_l1_loss(torch.cat(l1_parameters))\n","                \n","                # Add L1 loss component\n","                loss += l1\n","\n","                # all_linear1_params = self.layers[0].weight[0]\n","                # l1_regularization = 0.05 * torch.norm(all_linear1_params, 1)\n","                # #l2_regularization = lambda2 * torch.norm(all_linear2_params, 2)\n","                # loss += l1_regularization\n","\n","                if loss.requires_grad:\n","                    loss.backward()\n","                return loss\n","            \n","            self.optim.step(closure)\n","            \n","            # calculate the loss again for monitoring\n","            output = self(X_)\n","            loss = closure()\n","            running_loss += loss.item()\n","              \n","        return running_loss\n","    \n","    # I like to include a sklearn like predict method for convenience\n","    def predict(self, X):\n","        X = torch.Tensor(X)\n","        return self(X).detach().numpy().squeeze()\n","\n","def my_loss(output, target):\n","  a = -3.25\n","  loss = torch.log(1+torch.exp(a*(1-2*torch.abs(output-target))))\n","  return torch.mean(loss)\n","\n","class ExperimentData(Dataset):\n","  def __init__(self, X, y):\n","      self.X = X\n","      self.y = y\n","      \n","  def __len__(self):\n","      return self.X.shape[0]\n","  \n","  def __getitem__(self, idx):\n","      return self.X[idx,:], self.y[idx]\n","\n","def logistic_estimator_with_pytorch(A_train,A_test,b_train,b_test,test_index,train_index,r = 0,q=0,p=0,edge=0.5,l2_reg=0.0):\n","  data = ExperimentData(A_train,b_train)\n","  INPUT_SIZE = A_train.shape[1]\n","  EPOCHS=1 # Few epochs to avoid overfitting\n","  pred_train = {}\n","  HIDDEN_LAYER_SIZE = []\n","  data_loader = DataLoader(data, batch_size=A_train.shape[0])\n","  net = NNet(INPUT_SIZE, HIDDEN_LAYER_SIZE, sigmoid=True, loss = my_loss)\n","  net.optim = LBFGS(net.parameters(), history_size=10, max_iter=10)#,lr=1.5)\n","  net.train(data_loader, EPOCHS)\n","  if len(test_index)>0:\n","    b_predicted_test = net.predict(A_test) # these are float\n","  else:\n","    b_predicted_test = []\n","  if A_test.shape[0] == 1:\n","    b_predicted_test = [b_predicted_test]\n","  b_predicted_train = net.predict(A_train)\n","  b_predicted_test_2 = np.array([np.float(i > edge) for i in b_predicted_test]) # these are 0 or 1\n","  b_predicted_train_2 = np.array([np.float(i > edge) for i in b_predicted_train]) # these are 0 or 1\n","  test_accuracy = accuracy_score(b_test,b_predicted_test_2)\n","  train_accuracy = accuracy_score(b_train,b_predicted_train_2)\n","  test_sensitivity = sensitivity_score(b_test,b_predicted_test_2)\n","  train_sensitivity = sensitivity_score(b_train,b_predicted_train_2)\n","  test_specificity = specificity_score(b_test,b_predicted_test_2)\n","  train_specificity = specificity_score(b_train,b_predicted_train_2) \n","  if np.isnan(test_sensitivity):\n","    test_sensitivity = 0 # this is if we have 0 'sample size'\n","  if np.isnan(train_sensitivity):\n","    train_sensitivity = 0\n","  if np.isnan(test_specificity):\n","    test_specificity = 0 # this is if we have 0 'sample size'\n","  if np.isnan(train_specificity):\n","    train_specificity = 0\n","  if np.isnan(test_accuracy):\n","    test_accuracy = 0 # this is if we have 0 'sample size'\n","  x_fit = np.append(net.layers[0].weight[0].detach().numpy(),net.layers[0].bias[0].detach().numpy()) #bias is always the last one\n","  test_abs_differences = abs_differences(b_test,b_predicted_test,test_index)\n","  train_abs_differences = abs_differences(b_train,b_predicted_train,train_index)\n","  #test_r2 = r2_score(b_test,b_predicted_test) this one only has 4 people in it and is no use\n","  confusion = confusion_matrix(b_test,b_predicted_test_2)\n","  result = {}\n","  result['test_accuracy'] = test_accuracy\n","  result['train_accuracy'] = train_accuracy\n","  result['x_fit'] = x_fit\n","  #result['test_r^2'] = test_r2\n","  result['test_abs_differences'] = test_abs_differences\n","  result['train_abs_differences'] = train_abs_differences\n","  result['test_sensitivity'] = test_sensitivity\n","  result['train_sensitivity'] = train_sensitivity\n","  result['b_predicted_test_2'] = b_predicted_test_2\n","  result['confusion'] = confusion\n","  result['test_specificity'] = test_specificity\n","  result['train_specificity'] = train_specificity\n","  result['b_predicted_test'] = b_predicted_test\n","  result['b_predicted_train'] = b_predicted_train\n","  return result\n","\n","# def logistic_estimator_with_keras(A_train,A_test,b_train,b_test,test_index,train_index,r = 0,q=0,p=0,edge=0.5,l2_reg=1.0):\n","#   # Our vectorized labels\n","#   b_train = np.asarray(b_train).astype('float32').reshape((-1,1))\n","#   b_test = np.asarray(b_test).astype('float32').reshape((-1,1))\n","\n","#   def custom_loss_function(y_true, y_pred):\n","#     a = -3.25\n","#     abc = tf.math.log(1+tf.math.exp(a*(1-2*tf.math.abs(y_true-y_pred))))\n","#     return tf.reduce_mean(abc, axis=-1)\n","#   model = keras.Sequential()\n","#   model.add(keras.layers.Dense(1, \n","#                   #kernel_initializer='glorot_uniform',\n","#                   input_dim=A_train.shape[1], \n","#                   #kernel_regularizer=L1L2(l1=0.0, l2=0.1),       \n","#                   #kernel_regularizer=keras.regularizers.l2(l2_reg),                \n","#                   activation='sigmoid'))\n","\n","#   model.compile(optimizer='sgd',\n","#                 loss='binary_crossentropy',\n","#                 metrics=['binary_accuracy'])\n","  \n","#   model.fit(A_train, b_train, epochs=50, shuffle=True, validation_data=(A_test, b_test), verbose = 0)\n","\n","#   if len(test_index)>0:\n","#     b_predicted_test = np.reshape(model.predict(A_test),(1, -1))[0] # these are float\n","#   else:\n","#     b_predicted_test = []\n","#   b_predicted_train = np.reshape(model.predict(A_train),(1, -1))[0]\n","#   b_predicted_test_2 = np.array([np.float(i > edge) for i in b_predicted_test]) # these are 0 or 1\n","#   b_predicted_train_2 = np.array([np.float(i > edge) for i in b_predicted_train]) # these are 0 or 1\n","#   test_accuracy = accuracy_score(b_test,b_predicted_test_2)\n","#   train_accuracy = accuracy_score(b_train,b_predicted_train_2)\n","#   test_sensitivity = sensitivity_score(b_test,b_predicted_test_2)\n","#   train_sensitivity = sensitivity_score(b_train,b_predicted_train_2)\n","#   test_specificity = specificity_score(b_test,b_predicted_test_2)\n","#   train_specificity = specificity_score(b_train,b_predicted_train_2) \n","#   if np.isnan(test_sensitivity):\n","#     test_sensitivity = 0 # this is if we have 0 'sample size'\n","#   if np.isnan(train_sensitivity):\n","#     train_sensitivity = 0\n","#   if np.isnan(test_specificity):\n","#     test_specificity = 0 # this is if we have 0 'sample size'\n","#   if np.isnan(train_specificity):\n","#     train_specificity = 0\n","#   if np.isnan(test_accuracy):\n","#     test_accuracy = 0 # this is if we have 0 'sample size'\n","#   x_fit = np.append(model.layers[0].get_weights()[0],model.layers[0].get_weights()[1])\n","#   test_abs_differences = abs_differences(b_test,b_predicted_test,test_index)\n","#   train_abs_differences = abs_differences(b_train,b_predicted_train,train_index)\n","#   #test_r2 = r2_score(b_test,b_predicted_test) this one only has 4 people in it and is no use\n","#   confusion = confusion_matrix(b_test,b_predicted_test_2)\n","#   result = {}\n","#   result['test_accuracy'] = test_accuracy\n","#   result['train_accuracy'] = train_accuracy\n","#   result['x_fit'] = x_fit\n","#   #result['test_r^2'] = test_r2\n","#   result['test_abs_differences'] = test_abs_differences\n","#   result['train_abs_differences'] = train_abs_differences\n","#   result['test_sensitivity'] = test_sensitivity\n","#   result['train_sensitivity'] = train_sensitivity\n","#   result['b_predicted_test_2'] = b_predicted_test_2\n","#   result['confusion'] = confusion\n","#   result['test_specificity'] = test_specificity\n","#   result['train_specificity'] = train_specificity\n","#   result['b_predicted_test'] = b_predicted_test\n","#   result['b_predicted_train'] = b_predicted_train\n","#   return result\n","\n","def logistic_estimator(A_train,A_test,b_train,b_test,test_index,train_index,r = 0,q=0,p=0,edge=0.5,l2_reg=1.0):\n","  model = LogisticRegression(max_iter = 1000,fit_intercept=True,solver='liblinear',C=l2_reg)\n","  model.fit(A_train,b_train)\n","  if len(test_index)>0:\n","    b_predicted_test = model.predict_proba(A_test)[:, 1] # these are float\n","  else:\n","    b_predicted_test = []\n","  b_predicted_train = model.predict_proba(A_train)[:, 1]\n","  b_predicted_test_2 = np.array([np.float(i > edge) for i in b_predicted_test]) # these are 0 or 1\n","  b_predicted_train_2 = np.array([np.float(i > edge) for i in b_predicted_train]) # these are 0 or 1\n","  test_accuracy = accuracy_score(b_test,b_predicted_test_2)\n","  train_accuracy = accuracy_score(b_train,b_predicted_train_2)\n","  test_sensitivity = sensitivity_score(b_test,b_predicted_test_2)\n","  train_sensitivity = sensitivity_score(b_train,b_predicted_train_2)\n","  test_specificity = specificity_score(b_test,b_predicted_test_2)\n","  train_specificity = specificity_score(b_train,b_predicted_train_2) \n","  if np.isnan(test_sensitivity):\n","    test_sensitivity = 0 # this is if we have 0 'sample size'\n","  if np.isnan(train_sensitivity):\n","    train_sensitivity = 0\n","  if np.isnan(test_specificity):\n","    test_specificity = 0 # this is if we have 0 'sample size'\n","  if np.isnan(train_specificity):\n","    train_specificity = 0\n","  if np.isnan(test_accuracy):\n","    test_accuracy = 0 # this is if we have 0 'sample size'\n","  x_fit = np.append(model.coef_[0],model.intercept_[0])\n","  test_abs_differences = abs_differences(b_test,b_predicted_test,test_index)\n","  train_abs_differences = abs_differences(b_train,b_predicted_train,train_index)\n","  #test_r2 = r2_score(b_test,b_predicted_test) this one only has 4 people in it and is no use\n","  confusion = confusion_matrix(b_test,b_predicted_test_2)\n","  result = {}\n","  result['test_accuracy'] = test_accuracy\n","  result['train_accuracy'] = train_accuracy\n","  result['x_fit'] = x_fit\n","  #result['test_r^2'] = test_r2\n","  result['test_abs_differences'] = test_abs_differences\n","  result['train_abs_differences'] = train_abs_differences\n","  result['test_sensitivity'] = test_sensitivity\n","  result['train_sensitivity'] = train_sensitivity\n","  result['b_predicted_test_2'] = b_predicted_test_2\n","  result['confusion'] = confusion\n","  result['test_specificity'] = test_specificity\n","  result['train_specificity'] = train_specificity\n","  result['b_predicted_test'] = b_predicted_test\n","  result['b_predicted_train'] = b_predicted_train\n","  return result\n","\n","def random_forest_estimator(A_train,A_test,b_train,b_test,test_index,train_index,r = 0,q=0,p=0,edge=0.5,l2_reg=1.0):\n","  model = RandomForestClassifier(max_depth=4)\n","  model.fit(A_train,b_train)\n","  if len(test_index)>0:\n","    b_predicted_test = model.predict_proba(A_test)[:, 1] # these are float\n","  else:\n","    b_predicted_test = []\n","  b_predicted_train = model.predict_proba(A_train)[:, 1]\n","  b_predicted_test_2 = np.array([np.float(i > edge) for i in b_predicted_test]) # these are 0 or 1\n","  b_predicted_train_2 = np.array([np.float(i > edge) for i in b_predicted_train]) # these are 0 or 1\n","  test_accuracy = accuracy_score(b_test,b_predicted_test_2)\n","  train_accuracy = accuracy_score(b_train,b_predicted_train_2)\n","  test_sensitivity = sensitivity_score(b_test,b_predicted_test_2)\n","  train_sensitivity = sensitivity_score(b_train,b_predicted_train_2)\n","  test_specificity = specificity_score(b_test,b_predicted_test_2)\n","  train_specificity = specificity_score(b_train,b_predicted_train_2) \n","  if np.isnan(test_sensitivity):\n","    test_sensitivity = 0 # this is if we have 0 'sample size'\n","  if np.isnan(train_sensitivity):\n","    train_sensitivity = 0\n","  if np.isnan(test_specificity):\n","    test_specificity = 0 # this is if we have 0 'sample size'\n","  if np.isnan(train_specificity):\n","    train_specificity = 0\n","  if np.isnan(test_accuracy):\n","    test_accuracy = 0 # this is if we have 0 'sample size'\n","  x_fit = 0#np.append(model.coef_[0],model.intercept_[0])\n","  test_abs_differences = abs_differences(b_test,b_predicted_test,test_index)\n","  train_abs_differences = abs_differences(b_train,b_predicted_train,train_index)\n","  #test_r2 = r2_score(b_test,b_predicted_test) this one only has 4 people in it and is no use\n","  confusion = confusion_matrix(b_test,b_predicted_test_2)\n","  result = {}\n","  result['test_accuracy'] = test_accuracy\n","  result['train_accuracy'] = train_accuracy\n","  result['x_fit'] = x_fit\n","  #result['test_r^2'] = test_r2\n","  result['test_abs_differences'] = test_abs_differences\n","  result['train_abs_differences'] = train_abs_differences\n","  result['test_sensitivity'] = test_sensitivity\n","  result['train_sensitivity'] = train_sensitivity\n","  result['b_predicted_test_2'] = b_predicted_test_2\n","  result['confusion'] = confusion\n","  result['test_specificity'] = test_specificity\n","  result['train_specificity'] = train_specificity\n","  result['b_predicted_test'] = b_predicted_test\n","  result['b_predicted_train'] = b_predicted_train\n","  return result\n","\n","def cross_correlate(X,y, pad, estimator, r = 0,nr_folds = 10, p = 0, q = 0,edge = 0.5,b_compare = None,edge2=None,l2_reg=1.0):\n","  # be careful the default estimator is 'baked in' so best not to have one. If you change it, you have to 'restart the runtime'.\n","  if estimator == logistic_estimator:\n","    pad = False # it pads automatically, you can't turn it off\n","  if edge2 is None:\n","    edge2 = edge\n","#non-parallel\n","  result = {}\n","  if b_compare is not None:\n","    result['delta_accuracy'] = []\n","    result['delta_r^2'] = []\n","    result['delta_sensitivitys'] = []   \n","    result['delta_specificitys'] = []  \n","  result['test_accuracy'] = []\n","  result['train_accuracy'] = []\n","  result['x_fit'] = []\n","  result['test_abs_differences'] = pd.Series(dtype='float64',index=X.index)\n","  result['train_abs_differences'] = pd.DataFrame(dtype='float64',index=X.index)\n","  result['test_sensitivity'] = []\n","  result['train_sensitivity'] = []\n","  result['test_sensitivity_sample'] = []\n","  result['test_specificity_sample'] = []\n","  result['train_sensitivity_sample'] = []\n","  result['train_specificity_sample'] = []\n","  result['confusion'] = []\n","  result['test_specificity'] = []\n","  result['train_specificity'] = []\n","  result['b_train'] = []\n","  result['b_predicted_test'] = pd.Series(dtype='float64',index=X.index)\n","  if estimator == estimator_SVD:\n","    result['S'] = []\n","  if nr_folds == 1:\n","    assert False\n","    # A_train, A_test, b_train, b_test = train_test_split(X,y)\n","    # if pad:\n","    #   if len(A_train.shape) == 1:\n","    #     A_train = pd.DataFrame(A_train)\n","    #     list_of_ones = pd.Series(np.ones_like(b_train),name='pad',index=A_train.index)\n","    #     A_train = A_train.join(list_of_ones)\n","    #     A_train = A_train.values\n","    #     A_test = pd.DataFrame(A_test)\n","    #     list_of_ones = pd.Series(np.ones_like(b_test),name='pad',index=A_test.index)\n","    #     A_test = A_test.join(list_of_ones)\n","    #     A_test = A_test.values\n","    #   else:\n","    #     A_train = np.pad(A_train,[(0,0),(0,1)],constant_values=1)\n","    #     A_test = np.pad(A_test,[(0,0),(0,1)],constant_values=1)\n","    # test_accuracy,train_accuracy,x_fit,test_r2,train_r2,train_sensitivity,test_sensitivity,confusion,b_predicted_test_2 = estimator(A_train,A_test,b_train,b_test,r=r,p=p,q=q,edge=edge,l2_reg=l2_reg)\n","    # sensitivity_sample = np.count_nonzero(b_test == 1.0)\n","    # result.append([test_accuracy,train_accuracy,x_fit,test_r2,train_r2,test_sensitivity,train_sensitivity,confusion,sensitivity_sample])\n","  else:\n","    kf = KFold(n_splits=nr_folds,shuffle=True)\n","    gen = kf.split(X)\n","    gens = []\n","    for generator in gen:\n","      gens.append(generator)\n","    i = 0\n","    for generator in gens:\n","      i+=1\n","      A_train, A_test = X.iloc[generator[0]].to_numpy(), X.iloc[generator[1]].to_numpy()\n","      test_index = X.iloc[generator[1]].index\n","      train_index = X.iloc[generator[0]].index\n","      if len(A_train.shape) == 1:\n","        A_train = A_train.reshape(-1,1)\n","        A_test = A_test.reshape(-1,1)\n","      b_train, b_test = np.rint(y.iloc[generator[0]].to_numpy()), np.rint(y.iloc[generator[1]].to_numpy())\n","      for i in b_train:\n","        assert((i == 1) | (i == 0))\n","      for i in b_test:\n","        assert((i == 1) | (i == 0))\n","      if pad:\n","        if len(A_train.shape) == 1:\n","          A_train = pd.DataFrame(A_train)\n","          list_of_ones = pd.Series(np.ones_like(b_train),name='pad',index=A_train.index)\n","          A_train = A_train.join(list_of_ones)\n","          A_train = A_train.values\n","          A_test = pd.DataFrame(A_test)\n","          list_of_ones = pd.Series(np.ones_like(b_test),name='pad',index=A_test.index)\n","          A_test = A_test.join(list_of_ones)\n","          A_test = A_test.values\n","        else:\n","          A_train = np.pad(A_train,[(0,0),(0,1)],constant_values=1)\n","          A_test = np.pad(A_test,[(0,0),(0,1)],constant_values=1)\n","      \n","      out = estimator(A_train,A_test,b_train,b_test,test_index,train_index,r=r,p=p,q=q,edge=edge,l2_reg=l2_reg)\n","\n","      b_predicted_test = pd.Series(out['b_predicted_test'],index=test_index)\n","      test_sensitivity_sample = np.count_nonzero(b_test == 1)\n","      test_specificity_sample = np.count_nonzero(b_test == 0)\n","      train_sensitivity_sample = np.count_nonzero(b_train == 1)\n","      train_specificity_sample = np.count_nonzero(b_train == 0)      \n","      if b_compare is not None:\n","        _, b_compare_test = b_compare.iloc[generator[0]], b_compare.iloc[generator[1]]\n","        b_compare_test_2 = [int(row > edge2) for row in b_compare_test]\n","        compare_accuracy = accuracy_score(b_test,b_compare_test_2)\n","        compare_r2 = r2_score(b_test,b_compare_test)\n","        sensitivity_0 = np.sum([np.rint((a==b) & (c == 1)) for a,b,c in zip(b_compare_test_2,out['b_predicted_test_2'],b_test)])\n","        sensitivity_neg_1 = np.sum([np.rint((a>b) & (c == 1)) for a,b,c in zip(b_compare_test_2,out['b_predicted_test_2'],b_test)])\n","        sensitivity_pos_1 = np.sum([np.rint((a<b) & (c == 1)) for a,b,c in zip(b_compare_test_2,out['b_predicted_test_2'],b_test)])\n","        specificity_0 = np.sum([np.rint((a==b) & (c == 0)) for a,b,c in zip(b_compare_test_2,out['b_predicted_test_2'],b_test)])\n","        specificity_neg_1 = np.sum([np.rint((a<b) & (c == 0)) for a,b,c in zip(b_compare_test_2,out['b_predicted_test_2'],b_test)])\n","        specificity_pos_1 = np.sum([np.rint((a>b) & (c == 0)) for a,b,c in zip(b_compare_test_2,out['b_predicted_test_2'],b_test)])\n","        delta_r2 = out['test_r2'] - compare_r2\n","        delta_accuracy = out['test_accuracy'] - compare_accuracy\n","        result['delta_accuracy'].append(delta_accuracy)\n","        result['delta_r^2'].append(delta_r2)\n","        result['delta_sensitivitys'].append([sensitivity_0,sensitivity_neg_1,sensitivity_pos_1]) \n","        result['delta_specificitys'].append([specificity_0,specificity_neg_1,specificity_pos_1]) \n","      result['test_accuracy'].append(out['test_accuracy'])\n","      result['train_accuracy'].append(out['train_accuracy'])\n","      result['x_fit'].append(out['x_fit'])\n","      result['test_abs_differences'].loc[out['test_abs_differences'].index] = out['test_abs_differences'].values\n","      result['train_abs_differences'].loc[out['train_abs_differences'].index,i] = out['train_abs_differences'].values\n","      result['test_sensitivity'].append(out['test_sensitivity'])\n","      result['train_sensitivity'].append(out['train_sensitivity'])\n","      result['test_specificity'].append(out['test_specificity'])\n","      result['train_specificity'].append(out['train_specificity'])\n","      result['test_sensitivity_sample'].append(test_sensitivity_sample)\n","      result['test_specificity_sample'].append(test_specificity_sample)\n","      result['train_sensitivity_sample'].append(train_sensitivity_sample)\n","      result['train_specificity_sample'].append(train_specificity_sample)\n","      result['confusion'].append(out['confusion'])\n","      result['b_train'].append(b_train)\n","      result['b_predicted_test'].loc[b_predicted_test.index] = b_predicted_test.values\n","      if estimator == estimator_SVD:\n","        result['S'].append(out['S'])\n","  return result\n","\n","def r_estimator(A_train,A_test,b_train,b_test,r,q,p,edge=0.5):\n","  ny = A_train.shape[1]\n","  P = np.random.randn(ny,r+p)\n","  Z = A_train @ P\n","  for k in range(q):\n","    Z = A_train @ (A_train.T @ Z)\n","  Q,R = np.linalg.qr(Z)\n","  Y = Q.T @ A_train\n","  UY,S,VT = np.linalg.svd(Y,full_matrices=False)\n","  U = Q @ UY\n","  S = np.diag(S)\n","  A_train_inv = VT.T @ np.linalg.inv(S) @ U.T\n","  x_fit = A_train_inv @ b_train\n","  b_predicted_test = A_test @ x_fit\n","  b_predicted_test_2 = [int(row > edge) for row in b_predicted_test]\n","  b_predicted_train = A_train @ x_fit\n","  b_predicted_train = [int(row > edge) for row in b_predicted_train]\n","  test_accuracy = accuracy_score(b_test,b_predicted_test_2)\n","  train_accuracy = accuracy_score(b_train,b_predicted_train_2)\n","  test_sensitivity = sensitivity_score(b_test,b_predicted_test_2)\n","  train_sensitivity = sensitivity_score(b_train,b_predicted_train_2)\n","  test_specificity = specificity_score(b_test,b_predicted_test_2)\n","  train_specificity = specificity_score(b_train,b_predicted_train_2) \n","  if np.isnan(test_sensitivity):\n","    test_sensitivity = 0 # this is if we have 0 'sample size'\n","  if np.isnan(train_sensitivity):\n","    train_sensitivity = 0\n","  if np.isnan(test_specificity):\n","    test_specificity = 0 # this is if we have 0 'sample size'\n","  if np.isnan(train_specificity):\n","    train_specificity = 0\n","  test_abs_differences = abs_differences(b_test,b_predicted_test)\n","  train_abs_differences = abs_differences(b_train,b_predicted_train)\n","  confusion = confusion_matrix(b_test,b_predicted_test_2)\n","  result = {}\n","  result['test_accuracy'] = test_accuracy\n","  result['train_accuracy'] = train_accuracy\n","  result['x_fit'] = x_fit\n","  #result['test_r^2'] = test_r2 this one only has 4 people in it and is no use\n","  result['test_abs_differences'] = test_abs_differences\n","  result['train_abs_differences'] = train_abs_differences\n","  result['test_sensitivity'] = test_sensitivity\n","  result['train_sensitivity'] = train_sensitivity\n","  result['b_predicted_test_2'] = b_predicted_test_2\n","  result['S'] = S\n","  result['confusion'] = confusion\n","  result['test_specificity'] = test_specificity\n","  result['train_specificity'] = train_specificity\n","  result['b_predicted_test'] = b_predicted_test\n","  return result\n","\n","def logistic_estimator_with_k_means(A_train,A_test,b_train,b_test,test_index,train_index,r = 0,q=0,p=0,edge=0.5,l2_reg=1.0):\n","  kmeans = KMeans(n_clusters=2, init='k-means++', max_iter=1000, n_init=10)\n","  kmeans.fit(A_train)\n","  A_train_1 = [A_train[point] for point in range(len(A_train)) if kmeans.labels_[point] == 0]\n","  b_train_1 = [b_train[point] for point in range(len(b_train)) if kmeans.labels_[point] == 0]\n","  A_train_2 = [A_train[point] for point in range(len(A_train)) if kmeans.labels_[point] == 1]\n","  b_train_2 = [b_train[point] for point in range(len(b_train)) if kmeans.labels_[point] == 1]\n","  model1 = LogisticRegression(max_iter = 1000,fit_intercept=True,solver='liblinear',C=l2_reg)\n","  model1.fit(A_train_1,b_train_1)\n","  model2 = LogisticRegression(max_iter = 1000,fit_intercept=True,solver='liblinear',C=l2_reg)\n","  model2.fit(A_train_1,b_train_1)\n","  abc = kmeans.predict(A_test)\n","  A_test_1 = [A_test[point] for point in range(len(A_test)) if abc[point] == 0]\n","  b_test_1 = [b_test[point] for point in range(len(b_test)) if abc[point] == 0]\n","  A_test_2 = [A_test[point] for point in range(len(A_test)) if abc[point] == 1]\n","  b_test_2 = [b_test[point] for point in range(len(b_test)) if abc[point] == 1]  \n","  if len(test_index)>0:\n","    b_predicted_test_1 = model1.predict_proba(A_test_1)[:, 1] # these are float\n","  else:\n","    b_predicted_test = []\n","  if len(test_index)>0:\n","    b_predicted_test_2 = model2.predict_proba(A_test_2)[:, 1] # these are float\n","  else:\n","    b_predicted_test = []\n","  b_predicted_test = np.append(b_predicted_test_1,b_predicted_test_2)\n","  b_test = np.append(b_test_1,b_test_2)\n","  b_predicted_train1 = model1.predict_proba(A_train_1)[:, 1]\n","  b_predicted_train2 = model2.predict_proba(A_train_2)[:, 1]\n","  b_predicted_train = np.append(b_predicted_train1,b_predicted_train2)\n","  b_train = np.append(b_train_1,b_train_2)\n","  b_predicted_test_2 = np.array([np.float(i > edge) for i in b_predicted_test]) # these are 0 or 1\n","  b_predicted_train_2 = np.array([np.float(i > edge) for i in b_predicted_train]) # these are 0 or 1\n","  test_accuracy = accuracy_score(b_test,b_predicted_test_2)\n","  train_accuracy = accuracy_score(b_train,b_predicted_train_2)\n","  test_sensitivity = sensitivity_score(b_test,b_predicted_test_2)\n","  train_sensitivity = sensitivity_score(b_train,b_predicted_train_2)\n","  test_specificity = specificity_score(b_test,b_predicted_test_2)\n","  train_specificity = specificity_score(b_train,b_predicted_train_2) \n","  if np.isnan(test_sensitivity):\n","    test_sensitivity = 0 # this is if we have 0 'sample size'\n","  if np.isnan(train_sensitivity):\n","    train_sensitivity = 0\n","  if np.isnan(test_specificity):\n","    test_specificity = 0 # this is if we have 0 'sample size'\n","  if np.isnan(train_specificity):\n","    train_specificity = 0\n","  if np.isnan(test_accuracy):\n","    test_accuracy = 0 # this is if we have 0 'sample size'\n","  x_fit = 0#np.append(model.coef_[0],model.intercept_[0])\n","  test_abs_differences = abs_differences(b_test,b_predicted_test,test_index)\n","  train_abs_differences = abs_differences(b_train,b_predicted_train,train_index)\n","  #test_r2 = r2_score(b_test,b_predicted_test) this one only has 4 people in it and is no use\n","  confusion = confusion_matrix(b_test,b_predicted_test_2)\n","  result = {}\n","  result['test_accuracy'] = test_accuracy\n","  result['train_accuracy'] = train_accuracy\n","  result['x_fit'] = x_fit\n","  #result['test_r^2'] = test_r2\n","  result['test_abs_differences'] = test_abs_differences\n","  result['train_abs_differences'] = train_abs_differences\n","  result['test_sensitivity'] = test_sensitivity\n","  result['train_sensitivity'] = train_sensitivity\n","  result['b_predicted_test_2'] = b_predicted_test_2\n","  result['confusion'] = confusion\n","  result['test_specificity'] = test_specificity\n","  result['train_specificity'] = train_specificity\n","  result['b_predicted_test'] = b_predicted_test\n","  result['b_predicted_train'] = b_predicted_train\n","  return result"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"57iK4o5NLKqa"},"source":["## Putting everything together and metrics"]},{"cell_type":"code","metadata":{"id":"Yg41nmyRLID7"},"source":["def loading_scores(X,y,pad,estimator,features1 = None, features2 = None,method = None, r=0,p=0,q=0,nr_folds=None,edge=0.5,b_compare = None, show = True, n_rep = 1,edge2=None,c=None,gamma=None,svd_fit=False,svm_dim=2,l2_reg=1.0,scores=True):\n","  temp = []\n","  temp2 = []\n","  temp3 = []\n","  temp4 = []\n","  temp5 = []\n","  temp6 = []\n","  temp7 = []\n","  temp8 = []\n","  temp9 = []\n","  temp10 = []\n","  temp11 = []\n","  temp12 = []\n","  temp13 = []\n","  temp14 =[]\n","  temp15 = []\n","  temp20 =[]\n","  temp21 = []\n","  temp24 = []\n","  temp25 = []\n","  temp26 = []\n","  temp27 = []\n","  temp28 = []\n","  temp29 = []\n","  temp32 = []\n","  if b_compare is not None:\n","    temp16 = []\n","    temp17 = []\n","    temp18 = []\n","    temp19 = []\n","    temp22 = []\n","    temp23 = []\n","    temp30 = []\n","    temp31 = []\n","  if (features1 is not None) and (features2 is None):\n","    X_copy = copy.deepcopy(X[features1])\n","  else:\n","    X_copy = copy.deepcopy(X)\n","    if (features1 is None) and estimator != estimator_models:\n","      features1 = X_copy.columns.to_list()\n","  if nr_folds is None:\n","    nr_folds = len(y)\n","  scaled_data = preprocessing.scale(X_copy)\n","  if estimator == estimator_models:\n","    X_copy = pd.Series(data=scaled_data, index = X_copy.index)\n","    assert(not pad)\n","    result = estimator_models(X,y,X.index,edge=edge,b_compare=b_compare,edge2=edge2)\n","    accuracy = result['accuracy']\n","    accuracy_err = result['accuracy_err']\n","    r2 = result['r2']\n","    r2_err = result['r2_err']\n","    sensitivity = result['sensitivity']\n","    sensitivity_err = result['sensitivity_err']\n","    specificity = result['specificity']\n","    specificity_err = result['specificity_err']\n","    confusion = result['confusion']\n","    auc = result['auc']\n","    #individual_std = result['individual_std'] #if we say 0.85, what is the std of that (maybe np.sqrt(0.85*0.15))\n","    if b_compare is not None:\n","      delta_accuracy = result['delta_accuracy']\n","      delta_accuracy_err = result['delta_accuracy_err']\n","      delta_r2 = result['delta_r^2']\n","      [delta_sensitivity_0,delta_sensitivity_neg_1,delta_sensitivity_pos_1] = result['delta_sensitivitys']\n","      [delta_specificity_0,delta_specificity_neg_1,delta_specificity_pos_1] = result['delta_specificitys']\n","      delta_sensitivity = (delta_sensitivity_pos_1 - delta_sensitivity_neg_1)/result['sensitivity_sample']\n","      delta_specificity = (delta_specificity_pos_1 - delta_specificity_neg_1)/result['specificity_sample']\n","      delta_sensitivity_err = np.sqrt(((-1-delta_sensitivity)**2*delta_sensitivity_neg_1+(delta_sensitivity)**2*delta_sensitivity_0+(1-delta_sensitivity)**2*delta_sensitivity_pos_1))/result['sensitivity_sample']\n","      delta_specificity_err = np.sqrt(((-1-delta_specificity)**2*delta_specificity_neg_1+(delta_specificity)**2*delta_specificity_0+(1-delta_specificity)**2*delta_specificity_pos_1))/result['specificity_sample']\n","\n","    if show:\n","      print(f'Accuracy (test)= {np.round(accuracy,4)}+-{np.round(accuracy_err,4)}')\n","      print(f'R^2 (test)= {np.round(r2,4)}+-{np.round(r2_err,4)}')\n","      print(f'Sensitivity (test) = {np.round(sensitivity,4)}+-{np.round(sensitivity_err,4)}')\n","      print(f'Specificity (test) = {np.round(specificity,4)}+-{np.round(specificity_err,4)}')\n","      print(f'AUC = {np.round(auc,4)}')\n","\n","      group_names = [\"True Pos\",\"False Pos\",\"False Neg\",\"True Neg\"]\n","      group_counts = [\"{0:0.0f}\".format(value) for value in\n","                      confusion.flatten()]\n","      group_percentages = [\"{0:.2%}\".format(value) for value in\n","                          confusion.flatten()/np.sum(confusion)]\n","      group_errors = [\"{0:.2%}\".format(value) for value in\n","                          confusion.flatten()/np.sum(confusion)]\n","      labels = [f\"{v1}\\n{v2}+-{v3}\" for v1, v2, v3 in\n","                zip(group_names,group_percentages,group_errors)]\n","      labels = np.asarray(labels).reshape(2,2)\n","      sns.heatmap(confusion, annot=labels, fmt=\"\", cmap='Blues')\n","    numbers = {'Accuracy':accuracy,\n","             'Accuracy_Error':accuracy_err,\n","             'Sensitivity':sensitivity,\n","             'Sensitivity_Error':sensitivity_err,\n","             'Specificity':specificity,\n","             'Specificity_Error':specificity_err,\n","             'R^2':r2,\n","             'R^2_Error':r2_err,\n","             'AUC':auc\n","             }\n","    if b_compare is not None:\n","      temp = {'Delta_Accuracy': delta_accuracy,\n","              'Delta_Accuracy_Error': delta_accuracy_err,\n","              'Delta_R^2': delta_r2,\n","              'Delta_Sensitivity': delta_sensitivity,\n","              'Delta_Sensitivity_Error': delta_sensitivity_err,\n","              'Delta_Specificity': delta_specificity,\n","              'Delta_Specificity_Error': delta_specificity_err}\n","      numbers.update(temp)\n","    return _,numbers\n","  else:\n","    X_copy = pd.DataFrame(data=scaled_data, columns=X_copy.columns, index = X_copy.index)\n","    for i in range(n_rep):\n","      if features2 is None:\n","        result = cross_correlate(X_copy,y,pad,nr_folds=nr_folds,estimator=estimator,r=r,p=p,q=q,edge=edge,b_compare=b_compare,edge2=edge2,l2_reg=l2_reg)\n","      elif features1 is None:\n","        assert False\n","      else:\n","        result = cross_correlate_with_two_features(X=X_copy,y=y,features1=features1,features2=features2,pad=pad,estimator=estimator,method=method,nr_folds=nr_folds,edge=edge,c=c,gamma=gamma,svd_fit=svd_fit,svm_dim=svm_dim)\n","      predicted_test = result['b_predicted_test']\n","      test_mean = np.mean(result['test_accuracy'])\n","      test_err_mean = np.std(result['test_accuracy'])/np.sqrt(nr_folds)\n","      train_mean = np.mean(result['train_accuracy'])\n","      train_err_mean = np.std(result['train_accuracy'])/np.sqrt(nr_folds)\n","\n","      def likelihood_error(error,a):\n","        return np.log(1+np.exp(a*(1-2*error)))\n","      def saturated_error(a):\n","        return likelihood_error(0,a)\n","      a = -3.25\n","\n","      test_abs_differences_sum = np.sum([likelihood_error(i,a) for i in result['test_abs_differences']])\n","      test_abs_differences_err_sum = np.std([likelihood_error(i,a) for i in result['test_abs_differences']])*np.sqrt(len(result['test_abs_differences']))\n","      train_abs_differences_each = np.mean(result['train_abs_differences'],axis=1)\n","      train_abs_differences_each_err = np.std(result['train_abs_differences'],axis=1)/np.sqrt(nr_folds)\n","      train_abs_differences_sum = np.sum([likelihood_error(i,a) for i in train_abs_differences_each])\n","\n","      def df(error,a):\n","        return -likelihood_error(error,a)**2*np.exp(a*(1-2*error))*a*(-2.0)\n","\n","      train_abs_differences_err_sum = np.sqrt(np.sum([np.abs(i*df(j,a)) for i,j in zip(train_abs_differences_each_err,train_abs_differences_each)]))\n","      # this is really not the same likelihood function that you can use for y_abs_difference, but usually this is what people do. In other words, if the error for a line is Gaussian, the error for just y_pred = y_mean may not be Gaussian but you still do the 'Sum of Squares' for R^2.\n","      y_abs_differences = [likelihood_error(np.abs(i-np.mean(y)),a) for i in y]\n","      y_abs_differences_sum = np.sum(y_abs_differences)\n","      y_abs_differences_err_sum = np.std(y_abs_differences)*np.sqrt(len(y_abs_differences))\n","      saturated_model = [saturated_error(a) for i in y]\n","      test_r2 = (test_abs_differences_sum-y_abs_differences_sum)/(saturated_model-y_abs_differences_sum)\n","      test_r2_err = np.sqrt(1.0/(saturated_model-y_abs_differences_sum)**2*test_abs_differences_err_sum**2+((test_abs_differences_sum-saturated_model)/(saturated_model-y_abs_differences_sum)**2)**2*y_abs_differences_err_sum**2)\n","      train_r2 = (train_abs_differences_sum-y_abs_differences_sum)/(saturated_model-y_abs_differences_sum)\n","      train_r2_err = np.sqrt(1.0/(saturated_model-y_abs_differences_sum)**2*train_abs_differences_err_sum**2+((train_abs_differences_sum-saturated_model)/(saturated_model-y_abs_differences_sum)**2)**2*y_abs_differences_err_sum**2)\n","      true_positives_test = np.sum([i*j for i,j in zip(result['test_sensitivity'],result['test_sensitivity_sample'])]) # here we need to go back to 1's and 0's because for sensitivity, each fold has different 'sample size'\n","      false_negatives_test = np.sum([(1-i)*j for i,j in zip(result['test_sensitivity'],result['test_sensitivity_sample'])])\n","      test_sensitivity = true_positives_test/(false_negatives_test+true_positives_test) \n","      test_sensitivity_err = np.sqrt(test_sensitivity*(1-test_sensitivity)/np.sum(result['test_sensitivity_sample']))\n","      true_positives_train = np.sum([i*j for i,j in zip(result['train_sensitivity'],result['train_sensitivity_sample'])]) # here we need to go back to 1's and 0's because for sensitivity, each fold has different 'sample size'\n","      false_negatives_train = np.sum([(1-i)*j for i,j in zip(result['train_sensitivity'],result['train_sensitivity_sample'])])\n","      train_sensitivity = true_positives_train/(false_negatives_train+true_positives_train) \n","      train_sensitivity_err = np.sqrt(train_sensitivity*(1-train_sensitivity)/np.sum(result['train_sensitivity_sample']))\n","      true_negatives_test = np.sum([i*j for i,j in zip(result['test_specificity'],result['test_specificity_sample'])]) # here we need to go back to 1's and 0's because for sensitivity, each fold has different 'sample size'\n","      false_positives_test = np.sum([(1-i)*j for i,j in zip(result['test_specificity'],result['test_specificity_sample'])])\n","      test_specificity = true_negatives_test/(false_positives_test+true_negatives_test) \n","      test_specificity_err = np.sqrt(test_specificity*(1-test_specificity)/np.sum(result['test_specificity_sample']))\n","      true_negatives_train = np.sum([i*j for i,j in zip(result['train_specificity'],result['train_specificity_sample'])]) # here we need to go back to 1's and 0's because for sensitivity, each fold has different 'sample size'\n","      false_positives_train = np.sum([(1-i)*j for i,j in zip(result['train_specificity'],result['train_specificity_sample'])])\n","      train_specificity = true_negatives_train/(false_positives_train+true_negatives_train) \n","      train_specificity_err = np.sqrt(train_specificity*(1-train_specificity)/np.sum(result['train_specificity_sample']))\n","      confusion = np.mean(result['confusion'],axis=0)\n","      confusion_err = np.std(result['confusion'],axis=0)/np.sqrt(nr_folds)\n","      if b_compare is not None:\n","        delta_accuracy = np.mean(result['delta_accuracy'])\n","        delta_accuracy_err = np.std(result['delta_accuracy'])/np.sqrt(nr_folds)\n","        delta_r2 = np.mean(result['delta_r^2'])\n","        delta_r2_err = np.std(result['delta_r^2'])/np.sqrt(nr_folds)\n","        delta_sensitivity_0 = np.sum([i[0] for i in result['delta_sensitivitys']])\n","        delta_sensitivity_neg1 = np.sum([i[1] for i in result['delta_sensitivitys']])\n","        delta_sensitivity_pos1 = np.sum([i[2] for i in result['delta_sensitivitys']])\n","        delta_sensitivity = (delta_sensitivity_pos1 - delta_sensitivity_neg1)/(false_negatives_test+true_positives_test)\n","        delta_sensitivity_err = np.sqrt(((-1-delta_sensitivity)**2*delta_sensitivity_neg1+(delta_sensitivity)**2*delta_sensitivity_0+(1-delta_sensitivity)**2*delta_sensitivity_pos1))/(false_negatives_test+true_positives_test)\n","        delta_specificity_0 = np.sum([i[0] for i in result['delta_specificitys']])\n","        delta_specificity_neg1 = np.sum([i[1] for i in result['delta_specificitys']])\n","        delta_specificity_pos1 = np.sum([i[2] for i in result['delta_specificitys']])\n","        delta_specificity = (delta_specificity_pos1 - delta_specificity_neg1)/(false_positives_test+true_negatives_test)\n","        delta_specificity_err = np.sqrt(((-1-delta_specificity)**2*delta_specificity_neg1+(delta_specificity)**2*delta_specificity_0+(1-delta_specificity)**2*delta_specificity_pos1))/(false_positives_test+true_negatives_test)\n","      if (features2 is None) & (scores == True):\n","        if (len(X_copy.shape) == 1) and (estimator!=logistic_estimator) and (not pad):\n","          importance = [1.0]\n","          importance_stdev = [0.0]\n","          average = 0.0\n","        else:\n","          importance = np.mean(result['x_fit'],axis=0)\n","          importance_stdev = np.std(result['x_fit'],axis=0)[:-1]\n","          average = importance[-1]\n","          importance = importance[:-1]\n","        temp9.append(average)\n","        loading = [i*np.std(X[j]) for i,j in zip(importance,features1)]\n","        loading_stdev = [i*np.std(X[j]) for i,j in zip(importance_stdev,features1)]\n","        average_loading = average - np.sum([i*np.mean(X[j]) for i,j in zip(loading,features1)])\n","      else:\n","        loading,loading_stdev,importance,importance_stdev,average = [1.0],[0.0],[1.0],[0.0],0\n","      temp.append(test_mean)\n","      temp2.append(test_err_mean)\n","      temp3.append(train_mean)\n","      temp4.append(train_err_mean)\n","      temp5.append(loading)\n","      temp6.append(loading_stdev)\n","      temp7.append(importance)\n","      temp8.append(importance_stdev)\n","      temp10.append(test_r2)\n","      temp11.append(test_r2_err)\n","      temp12.append(train_r2)\n","      temp13.append(train_r2_err)\n","      temp14.append(confusion)\n","      temp15.append(confusion_err)\n","      temp20.append(test_sensitivity)\n","      temp21.append(test_sensitivity_err)\n","      temp24.append(train_sensitivity)\n","      temp25.append(train_sensitivity_err)\n","      temp26.append(test_specificity)\n","      temp27.append(test_specificity_err)\n","      temp28.append(train_specificity)\n","      temp29.append(train_specificity_err)\n","      temp32.append(predicted_test)    \n","      if b_compare is not None:\n","        temp16.append(delta_accuracy)\n","        temp17.append(delta_accuracy_err)\n","        temp18.append(delta_r2)\n","        temp19.append(delta_r2_err)\n","        temp22.append(delta_sensitivity)\n","        temp23.append(delta_sensitivity_err)\n","        temp30.append(delta_specificity)\n","        temp31.append(delta_specificity_err)\n","    predictions = pd.concat(temp32, axis=1, sort=False).mean(axis=1)\n","    auc = roc_auc_score(y,predictions)\n","    mean_test = (np.mean(temp))\n","    mean_err_test = (np.mean(temp2))\n","    mean_train = (np.mean(temp3))\n","    mean_err_train = (np.mean(temp4))\n","    r2_test = (np.mean(temp10))\n","    r2_err_test = (np.mean(temp11))\n","    r2_train = (np.mean(temp12))\n","    r2_err_train = (np.mean(temp13))\n","    cf_matrix = (np.mean(temp14,axis=0))\n","    cf_matrix_err = (np.mean(temp15,axis=0))\n","    sensitivity_test = (np.mean(temp20))\n","    sensitivity_err_test = (np.mean(temp21))\n","    sensitivity_train = (np.mean(temp24))\n","    sensitivity_err_train = (np.mean(temp25))\n","    specificity_test = (np.mean(temp26))\n","    specificity_err_test = (np.mean(temp27))\n","    specificity_train = (np.mean(temp28))\n","    specificity_err_train = (np.mean(temp29))\n","    if b_compare is not None:\n","      mean_delta_accuracy = (np.mean(temp16))\n","      mean_err_delta_accuracy = (np.mean(temp17))\n","      mean_delta_r2 = (np.mean(temp18))\n","      mean_err_delta_r2 = (np.mean(temp19))\n","      mean_delta_sensitivity = (np.mean(temp22))\n","      mean_err_delta_sensitivity = (np.mean(temp23))\n","      mean_delta_specificity = (np.mean(temp30))\n","      mean_err_delta_specificity = (np.mean(temp31))\n","    if (len(X_copy.shape) == 1) or features2 is not None:\n","      loading_scores = [np.mean([loading[0] for loading in temp5])]\n","      loading_stdev = [np.mean([loading_stdev[0] for loading_stdev in temp6])]\n","      importance_scores = [np.mean([importance[0] for importance in temp7])]\n","      importance_stdev = [np.mean([importance_stdev[0] for importance_stdev in temp8])]\n","    else:\n","      loading_scores = [np.mean([loading[i] for loading in temp5]) for i in range(len(X_copy.columns))]\n","      loading_stdev = [np.mean([loading_stdev[i] for loading_stdev in temp6]) for i in range(len(X_copy.columns))]\n","      importance_scores = [np.mean([importance[i] for importance in temp7]) for i in range(len(X_copy.columns))]\n","      importance_stdev = [np.mean([importance_stdev[i] for importance_stdev in temp8]) for i in range(len(X_copy.columns))]\n","    average = (np.mean(temp9))\n","\n","    if show:\n","      print(f'Accuracy (test)= {np.round(mean_test,4)}+-{np.round(mean_err_test,4)}')\n","      print(f'Acuracy (train)= {np.round(mean_train,4)}+-{np.round(mean_err_train,4)}')\n","      print(f'R^2 (test)= {np.round(r2_test,4)}+-{np.round(r2_err_test,4)}')\n","      print(f'R^2 (train)= {np.round(r2_train,4)}+-{np.round(r2_err_train,4)}')\n","      print(f'Sensitivity (test) = {np.round(sensitivity_test,4)}+-{np.round(sensitivity_err_test,4)}')\n","      print(f'Sensitivity (train) = {np.round(sensitivity_train,4)}+-{np.round(sensitivity_err_train,4)}')\n","      print(f'Specificity (test) = {np.round(specificity_test,4)}+-{np.round(specificity_err_test,4)}')\n","      print(f'Specificity (train) = {np.round(specificity_train,4)}+-{np.round(specificity_err_train,4)}')    \n","      print(f'AUC = {np.round(auc,4)}')\n","\n","      if b_compare is not None:\n","        print('###Delta_accuracy and Delta_R^2###')\n","        print(\"Delta = Original model minus b_compare model\")\n","        print(f'Delta Accuracy (test)= {np.round(mean_delta_accuracy,4)}+-{np.round(mean_err_delta_accuracy,4)}')\n","        print(f'Delta R^2 (test) = {np.round(mean_delta_r2,4)}+-{np.round(mean_err_delta_r2,4)}')\n","        print(f'Delta Sensitivity (test) = {np.round(mean_delta_sensitivity,4)}+-{np.round(mean_err_delta_sensitivity,4)}')\n","        print(f'Delta Specificity (test) = {np.round(mean_delta_specificity,4)}+-{np.round(mean_err_delta_specificity,4)}')\n","      if (len(X_copy.shape) != 1) and (features2 is None):\n","        plt.rcParams['figure.figsize'] = [16,8]\n","\n","        x_tick = X_copy.columns.to_list()\n","        fig = plt.figure()\n","        plt.bar(x_tick,importance_scores,yerr=importance_stdev)\n","        plt.ylabel(\"Significance\")\n","        plt.show()\n","\n","      group_names = [\"True Pos\",\"False Pos\",\"False Neg\",\"True Neg\"]\n","      group_counts = [\"{0:0.0f}\".format(value) for value in\n","                      cf_matrix.flatten()]\n","      group_percentages = [\"{0:.2%}\".format(value) for value in\n","                          cf_matrix.flatten()/np.sum(cf_matrix)]\n","      group_errors = [\"{0:.2%}\".format(value) for value in\n","                          cf_matrix_err.flatten()/np.sum(cf_matrix)]\n","      labels = [f\"{v1}\\n{v2}+-{v3}\" for v1, v2, v3 in\n","                zip(group_names,group_percentages,group_errors)]\n","      labels = np.asarray(labels).reshape(2,2)\n","      sns.heatmap(cf_matrix, annot=labels, fmt=\"\", cmap='Blues')\n","    numbers = {'Accuracy':mean_test,\n","              'Accuracy_Error':mean_err_test,\n","              'Sensitivity':sensitivity_test,\n","              'Sensitivity_Error':sensitivity_err_test,\n","              'Specificity':specificity_test,\n","              'Specificity_Error':specificity_err_test,\n","              'R^2':r2_test,\n","              'R^2_Error':r2_err_test,\n","              'Importance_Scores':importance_scores,\n","              'Average':average,\n","              'AUC': auc\n","              }\n","    if b_compare is not None:\n","      temp = {'Delta_Accuracy':mean_delta_accuracy,\n","              'Delta_Accuracy_Error':mean_err_delta_accuracy,\n","              'Delta_R^2':mean_delta_r2,\n","              'Delta_R^2_Error':mean_err_delta_r2,\n","              'Delta_Sensitivity':mean_delta_sensitivity,\n","              'Delta_Sensitivity_Error':mean_err_delta_sensitivity,\n","              'Delta_Specificity':mean_delta_specificity,\n","              'Delta_Specificity_Error':mean_err_delta_specificity}\n","      numbers.update(temp)\n","    return predictions,numbers\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GpmSB7bPLmtC"},"source":["## Feature selection"]},{"cell_type":"code","metadata":{"id":"-EvQ8ZNULhv_"},"source":["ca_list = ['ca_0.0','ca_1.0','ca_2.0','ca_3.0']\n","thal_list = ['thal_3.0','thal_6.0','thal_7.0']\n","slope_list = ['slope_1.0','slope_2.0','slope_3.0']\n","cp_list = ['cp_1.0','cp_2.0','cp_3.0','cp_4.0']\n","restecg_list = ['restecg_0.0','restecg_1.0','restecg_2.0']\n","\n","#helper for feature_selection\n","def complete_cats(cat,dropped,complete,orthogonal_to,importance,importance_stdev,average,x_tick,df,X,debug):\n","  if complete == True:\n","    n = []\n","    pos=[]\n","    n_dropped = len(df.loc[df[cat] == dropped])\n","    for i in globals()[cat+'_list']:\n","      if i != cat + \"_\" + dropped: \n","        n.append(len(df.loc[df[cat] == float(i.split(\"_\")[1])]))\n","        pos.append(X.columns.get_loc(i))\n","    x = np.sum([n[i]*importance[pos[i]] for i in range(len(n))])/(np.sum(n)+n_dropped)\n","    dropped_error = np.sqrt(np.sum([n[i]*importance_stdev[pos[i]]**2 for i in range(len(n))])/len(n)*(len(n)+1)/(np.sum(n)+n_dropped))\n","    importance = np.append(importance,-x)\n","    importance_stdev = np.append(importance_stdev,dropped_error)\n","    for i in range(len(n)):\n","      importance[pos[i]] += -x\n","      importance_stdev[pos[i]] = np.sqrt((n[i]*importance_stdev[pos[i]]**2 + n_dropped*dropped_error**2)/(1/2*(n_dropped+n[i])))\n","    x_tick.append(cat+\"_\" + dropped)\n","    if debug:\n","      plt.figure(figsize=(22,11))\n","      plt.bar(x_tick,importance,yerr=importance_stdev)\n","      plt.ylabel(\"Significance\")\n","      plt.show()\n","    average = average + x\n","  elif len(orthogonal_to) > 0:\n","    n = []\n","    pos=[]\n","    orthogonal = []\n","    for i in globals()[cat+'_list']:\n","      if i in X.columns.to_list(): \n","        n.append(len(df.loc[df[cat] == float(i.split(\"_\")[1])]))\n","        pos.append(X.columns.get_loc(i))\n","      elif i in orthogonal_to:\n","        orthogonal.append(i)\n","    assert (len(orthogonal) == len(globals()[cat+'_list']) - 1) and (len(n) == 1) == False\n","    if ((len(n) + len(orthogonal) == len(globals()[cat+'_list'])) and (len(n)>1)):\n","      x = np.sum([n[i]*importance[pos[i]] for i in range(len(n))])/np.sum(n)\n","      for i in range(len(n)):\n","        importance[pos[i]] += -x\n","      if debug:\n","        plt.figure(figsize=(22,11))\n","        plt.bar(x_tick,importance,yerr=importance_stdev)\n","        plt.ylabel(\"Significance\")\n","        plt.show()\n","      average = average + x # this may need to be changed but is not used\n","  return importance, importance_stdev,average,x_tick\n","\n","#feature-selection\n","not_categorical = ['age','chol','oldpeak','trestbps','thalach']\n","def automated_leave_k_out(X, y, target_features = False,method = 'forward_selection',metric = 'auc',usage_map=False,intermediate_n = False,gridsearch=False,size_test = 5,size_gridsearch_test=5,gridsearch_metric = 'auc', no_cv=False,keep_going = False,gridsearch_mode = False,or_features=False,decider_misses = 5,decider_type = 'history',decider_min_jumps = 5):\n","  assert (metric == 'auc') or (metric == 'accuracy')\n","  assert (gridsearch_metric == 'auc') or (gridsearch_metric == 'accuracy') or (gridsearch_metric == 'binomial_fraying')\n","  assert (method == 'forward_selection') or (method == 'backward_selection') or (method == 'step_forward_selection')\n","  assert (not (target_features and gridsearch))\n","\n","  def find_next(chosen,feature,metric,generator,X,y):\n","    features = np.append(chosen,feature)\n","    assert len(features)>len(chosen)\n","    # For logistic regression, we don't need to take out one of every ohe feature.\n","    A_train, A_test = X[features].iloc[generator[0]].to_numpy(), X[features].iloc[generator[1]].to_numpy()\n","    b_train, b_test = np.rint(y.iloc[generator[0]].to_numpy()), np.rint(y.iloc[generator[1]].to_numpy())\n","    for i in b_train:\n","      assert((i == 1) | (i == 0))\n","    for i in b_test:\n","      assert((i == 1) | (i == 0))\n","    model = LogisticRegression(max_iter = 1000,fit_intercept=True,solver='liblinear',C=1000)\n","    model.fit(A_train,b_train)\n","    intercept = model.intercept_[0]\n","    x_fit = []\n","    coef_ = []\n","    for j in range(len(features)):\n","      coef_.append(model.coef_[0][j])\n","      x_fit.append(features[j])\n","    x_fit = [fit for _,fit in sorted(zip([abs(i) for i in coef_],x_fit),reverse=True)]\n","    if metric == 'accuracy':\n","      result = accuracy_score(b_train,np.rint(model.predict(A_train)))\n","    elif metric == 'auc':\n","      result = roc_auc_score(b_train,model.predict_proba(A_train)[:,1])\n","    test_accuracy = accuracy_score(b_test,np.rint(model.predict(A_test)))\n","    test_size = len(b_test)\n","    return result,x_fit[:len(chosen)+1],test_accuracy*test_size,model.predict_proba(A_test)[:, 1]\n","\n","  def remove_next(chosen,metric,generator,X,y):\n","    features = chosen\n","    # For logistic regression, we don't need to take out one of every ohe feature.\n","    A_train, A_test = X[features].iloc[generator[0]].to_numpy(), X[features].iloc[generator[1]].to_numpy()\n","    b_train, b_test = np.rint(y.iloc[generator[0]].to_numpy()), np.rint(y.iloc[generator[1]].to_numpy())\n","    for i in b_train:\n","      assert((i == 1) | (i == 0))\n","    for i in b_test:\n","      assert((i == 1) | (i == 0))\n","    model = LogisticRegression(max_iter = 1000,fit_intercept=True,solver='liblinear',C=1000)\n","    model.fit(A_train,b_train)\n","    x_fit = []\n","    coef_ = []\n","    for j in range(len(features)):\n","      coef_.append(model.coef_[0][j])\n","      x_fit.append(features[j])\n","    x_fit = [fit for _,fit in sorted(zip([abs(i) for i in coef_],x_fit),reverse=True)]\n","    chosen = x_fit[:len(chosen)-1]\n","    features = chosen\n","    # For logistic regression, we don't need to take out one of every ohe feature.\n","    A_train, A_test = X[features].iloc[generator[0]].to_numpy(), X[features].iloc[generator[1]].to_numpy()\n","    b_train, b_test = np.rint(y.iloc[generator[0]].to_numpy()), np.rint(y.iloc[generator[1]].to_numpy())\n","    model.fit(A_train,b_train)\n","    intercept = model.intercept_[0]\n","    x_fit = []\n","    coef_ = []\n","    for j in range(len(features)):\n","      coef_.append(model.coef_[0][j])\n","      x_fit.append(features[j])\n","    if metric == 'accuracy':\n","      result = accuracy_score(b_train,np.rint(model.predict(A_train)))\n","    elif metric == 'auc':\n","      result = roc_auc_score(b_train,model.predict_proba(A_train)[:,1])\n","    test_accuracy = accuracy_score(b_test,np.rint(model.predict(A_test)))\n","    test_size = len(b_test)\n","    return result,x_fit,test_accuracy*test_size,model.predict_proba(A_test)[:, 1]\n","\n","  #X_copy = copy.deepcopy(pd.DataFrame(preprocessing.scale(X),columns = X.columns,index = X.index))\n","  X_copy = copy.deepcopy(X)\n","  how_many = []\n","  chosen_all = []\n","  out = pd.Series(index = y.index,dtype='float64')\n","  test_rights = []\n","  if intermediate_n:\n","    chosen_all_intermediate = {}\n","    test_rights_intermediate = {}\n","    out_intermediate = pd.DataFrame(index = y.index,dtype='float64')\n","  kf = KFold(n_splits=int(np.rint(len(y)/size_test)),shuffle=True)\n","  gen = kf.split(X_copy)\n","  gens = []\n","  map_columns = X_copy.columns.to_list()\n","  if decider_type == 'history':\n","    histories = []\n","  elif decider_type == 'jump':\n","    feature_position_histories = []\n","\n","  for generator in gen:\n","    gens.append(generator)\n","    if decider_type == 'history':\n","      histories.append([])\n","    \n","  generator_index = -1\n","  for generator in tqdm(gens):\n","    generator_index += 1\n","    if gridsearch and or_features:\n","      target_features,or_opportunities = automated_leave_k_out(X_copy.iloc[generator[0]], y.iloc[generator[0]], target_features = False,method = method,metric = metric,usage_map=False,intermediate_n = True,gridsearch=False,size_test = size_gridsearch_test,no_cv=no_cv,keep_going = keep_going,gridsearch_metric = gridsearch_metric,gridsearch_mode = True,or_features=True,decider_type=decider_type)\n","      local_X_copy = copy.deepcopy(X_copy)\n","      flatten_or_opportunities = set([i for abc in or_opportunities.values() for i in abc])\n","      print(flatten_or_opportunities)\n","      for opportunity in flatten_or_opportunities:\n","        iter_opportunity = iter(opportunity)\n","        opportunity_1 = next(iter_opportunity)\n","        opportunity_2 = next(iter_opportunity)\n","        local_X_copy[opportunity_1 + ' or ' + opportunity_2] = local_X_copy[opportunity_1] | local_X_copy[opportunity_2]\n","        if opportunity_1 + ' or ' + opportunity_2 not in map_columns:\n","          map_columns.append(opportunity_1 + ' or ' + opportunity_2)\n","      local_X_copy = pd.DataFrame(preprocessing.scale(local_X_copy),columns = local_X_copy.columns,index = local_X_copy.index)\n","      print(local_X_copy.columns)\n","    elif gridsearch and not or_features:\n","      target_features = automated_leave_k_out(X_copy.iloc[generator[0]], y.iloc[generator[0]], target_features = False,method = method,metric = metric,usage_map=False,intermediate_n = True,gridsearch=False,size_test = size_gridsearch_test,no_cv=no_cv,keep_going = keep_going,gridsearch_metric = gridsearch_metric,gridsearch_mode = True,or_features=False,decider_type=decider_type)\n","      local_X_copy = copy.deepcopy(pd.DataFrame(preprocessing.scale(X_copy),columns = X_copy.columns,index = X_copy.index))\n","    else:\n","      local_X_copy = copy.deepcopy(pd.DataFrame(preprocessing.scale(X_copy),columns = X_copy.columns,index = X_copy.index))\n","    #assert False #think about this a bit more\n","    if method == 'step_forward_selection':\n","      selection_counter = 0\n","      chosen = []\n","    elif method == 'forward_selection':\n","      chosen = []\n","    elif method == 'backward_selection':\n","      chosen = local_X_copy.columns.to_list()\n","    best_result = 0.0\n","    feature_positions = []\n","    while True:\n","      if method == 'step_forward_selection':\n","        selection_counter += 1\n","      if (method == 'forward_selection') or ((method =='step_forward_selection') and (selection_counter %3 != 0)):\n","        best_feature = ()\n","        results = []\n","        features_ = []\n","        for feature in [i for i in local_X_copy.columns if i not in chosen]:\n","          result,x_fit,test_right,model = find_next(chosen,feature,metric,generator,local_X_copy,y)\n","          results.append(result)\n","          features_.append(feature)\n","          if len(best_feature) == 0:\n","            best_feature = (feature,result,test_right,model)\n","          elif result > best_feature[1]:\n","            best_feature = (feature,result,test_right,model)\n","          if result > best_result:\n","            new_chosen = x_fit\n","            best_result = result\n","            best_test_right = test_right\n","            right_out = model\n","        if selection_counter %3 == 1:\n","          feature_positions.append([feature for _,feature in sorted(zip(results,features_),reverse=True)])\n","        if (target_features>0 and not keep_going) or (len(new_chosen) > len(chosen)) or (keep_going and (len(chosen) < len(local_X_copy.columns))): \n","          if len(new_chosen) <= len(chosen): # first we try it the old way but if we keep_going, we still do the best we can.\n","            chosen = np.append(chosen,best_feature[0])\n","            best_test_right = best_feature[2]\n","            right_out = best_feature[3]\n","            new_chosen = chosen\n","          else:\n","            chosen = new_chosen\n","          if intermediate_n and ((not method == 'step_forward_selection') or len(chosen) == len(local_X_copy.columns)):\n","            if len(chosen) in test_rights_intermediate.keys():\n","              test_rights_intermediate[len(chosen)].append(best_test_right)\n","            else:\n","              test_rights_intermediate[len(chosen)] = [best_test_right]\n","            if len(chosen) in chosen_all_intermediate.keys():\n","              chosen_all_intermediate[len(chosen)].append(frozenset(chosen))\n","            else:\n","              chosen_all_intermediate[len(chosen)] = [frozenset(chosen)]\n","            if len(chosen) in out_intermediate.columns:\n","              out_intermediate.iloc[generator[1], out_intermediate.columns.get_loc(len(chosen))] = right_out\n","            else:\n","              out_intermediate[len(chosen)] = 0\n","              out_intermediate.iloc[generator[1], out_intermediate.columns.get_loc(len(chosen))] = right_out\n","          if (len(chosen) == target_features) and ((not method == 'step_forward_selection') or len(chosen) == len(local_X_copy.columns)):\n","            print(target_features)\n","            how_many.append(target_features)\n","            out.iloc[generator[1]] = right_out\n","            chosen_all.append(frozenset(chosen)) \n","            test_rights.append(best_test_right)\n","            if not keep_going and (method == 'forward_selection'):\n","              break\n","        else:\n","          if not target_features:\n","            how_many.append(len(chosen))\n","            out.iloc[generator[1]] = right_out\n","            chosen_all.append(frozenset(chosen)) \n","            test_rights.append(best_test_right)\n","          if (method != 'step_forward_selection') or (len(chosen) == len(local_X_copy.columns)):\n","            break\n","      if (method == 'backward_selection') or ((method =='step_forward_selection') and (selection_counter %3 == 0)):\n","        result,x_fit,test_right,model = remove_next(chosen,metric,generator,local_X_copy,y)\n","        new_chosen = x_fit\n","        best_result = result\n","        best_test_right = test_right\n","        right_out = model\n","        if not gridsearch_mode and or_features:\n","          print(new_chosen)\n","        if decider_type == 'history':\n","          histories[generator_index].append(new_chosen)\n","        if not len(new_chosen) == 0: \n","          chosen = new_chosen\n","          if intermediate_n:\n","            if len(chosen) in test_rights_intermediate.keys():\n","              test_rights_intermediate[len(chosen)].append(best_test_right)\n","            else:\n","              test_rights_intermediate[len(chosen)] = [best_test_right]\n","            if len(chosen) in chosen_all_intermediate.keys():\n","              chosen_all_intermediate[len(chosen)].append(frozenset(chosen))\n","            else:\n","              chosen_all_intermediate[len(chosen)] = [frozenset(chosen)]\n","            if len(chosen) in out_intermediate.columns:\n","              out_intermediate.iloc[generator[1], out_intermediate.columns.get_loc(len(chosen))] = right_out\n","            else:\n","              out_intermediate[len(chosen)] = 0\n","              out_intermediate.iloc[generator[1], out_intermediate.columns.get_loc(len(chosen))] = right_out\n","          if len(chosen) == target_features:\n","            print(target_features)\n","            how_many.append(target_features)\n","            out.iloc[generator[1]] = right_out\n","            chosen_all.append(frozenset(chosen)) \n","            test_rights.append(best_test_right)\n","            if not keep_going:\n","              break\n","          if len(chosen) == 0:\n","            break\n","          if ((method =='step_forward_selection') and (len(chosen) == len(local_X_copy.columns) - 1)):\n","            break\n","        else:\n","          if not target_features:\n","            how_many.append(len(chosen))\n","            out.iloc[generator[1]] = right_out\n","            chosen_all.append(frozenset(chosen)) \n","            test_rights.append(best_test_right)\n","          break\n","    if or_features:\n","      feature_position_histories.append(feature_positions)\n","    if no_cv and not gridsearch_mode:\n","      break\n","\n","  if gridsearch_mode:\n","    metrics = {}\n","    for i in np.arange(1,len(chosen_all_intermediate)+1,1):\n","      if np.sum(out_intermediate[i].isna()) > 0: # this is because if one of my Folds didn't go to that n_features, I don't want to look at it\n","        break # this should happen only once\n","      if gridsearch_metric == 'auc':\n","        metrics[i] = roc_auc_score(y,out_intermediate[i])\n","      elif gridsearch_metric == 'accuracy':\n","        metrics[i] = np.sum(test_rights_intermediate[i])/len(y)\n","      elif gridsearch_metric == 'binomial_fraying':\n","        fraying_set = set(chosen_all_intermediate[i])\n","        fraying_list = []\n","        for features in fraying_set:\n","          fraying_list.append(chosen_all_intermediate[i].count(features))\n","        fraying = np.sum([j**2/len(gens)**2 for j in fraying_list])\n","        if (i <= 6) or (i>=20):\n","          metrics[i] = 0.0\n","        else:\n","          metrics[i] = fraying\n","    n_best = max(metrics, key=metrics.get)\n","    if or_features:\n","      n_best_feature_set = set(chosen_all_intermediate[n_best])\n","      fraying_dicts = {}\n","      frayings = {}\n","      or_opportunities = {}\n","      and_opportunities = {}\n","      for i in np.arange(1,n_best,1):\n","        fraying_set = set(chosen_all_intermediate[i])\n","        fraying_dict = {}\n","        for features in fraying_set:\n","          fraying_dict[features] = chosen_all_intermediate[i].count(features)\n","        fraying = np.sum([j**2/len(gens)**2 for j in fraying_dict.values()])\n","        new_or_opportunities = []\n","        new_and_opportunities = []\n","        if decider_type == 'history':\n","          if len(frayings) > 0:\n","            if frayings[i-1] > fraying:\n","              abc = list(fraying_set)\n","              pair_gen = ((abc[x],abc[y]) for x in range(len(abc)) for y in range(len(abc)) if x<y)\n","              for pair in pair_gen:\n","                both = pair[0].intersection(pair[1])\n","                first_feature = next(iter(pair[0]-pair[1])) # next(iter()) just gets 'a' from frozenset({'a'})\n","                second_feature = next(iter(pair[1]-pair[0]))\n","                if both in fraying_dicts[i-1].keys() and first_feature not in not_categorical and second_feature not in not_categorical: # that means that len(xor(_,_)) = 2\n","                  print(first_feature,second_feature)\n","          #         # make sure they aren't put together in the end\n","          #         #n_best_targets = [n_best_features for n_best_features in set(chosen_all_intermediate[n_best]) if first_feature in n_best_features and second_feature in n_best_features]\n","          #         #decider = np.sum([chosen_all_intermediate[n_best].count(j) for j in n_best_targets]) >= fraying_dict[pair[0]] + fraying_dict[pair[1]] #here we don't think about the cases where a different branch develops both pair[0] and pair[1] later\n","          #         #decider = False # if we don't care if they come back together again afterwards because we didn't do it perfectly (see 311)\n","          #         #we could use history here to do decider:\n","                  decider = 0\n","                  for history in histories:\n","                    if first_feature in history[n_best-1] and second_feature in history[n_best-1] or first_feature not in history[n_best-1] and second_feature not in history[n_best-1]:\n","                      pass\n","                    else:\n","                      decider += 1\n","                  print(decider)\n","                  if decider >= decider_misses:\n","                    new_or_opportunities.append(frozenset([first_feature,second_feature]))\n","        #or we can see if a Feature position jumped more than decider_min_jumps backwards\n","        elif decider_type == 'jump':\n","          for feature_positions in feature_position_histories:\n","            for j in range(len(feature_positions)-1):\n","              if set(feature_positions[j+1]).issubset(set(feature_positions[j])): #otherwise two were taken and one was returned and we don't know what made the jump happen\n","                for feature_index in range(len(feature_positions[j+1])):\n","                  if feature_positions[j].index(feature_positions[j+1][feature_index]) + decider_min_jumps <= feature_index: #before position+decider_min_jumps <= now position #everything will jump one position because of the different length\n","                    print([feature_positions[j+1][feature_index],next(iter(set(feature_positions[j])-set(feature_positions[j+1])))])\n","                    new_or_opportunities.append(frozenset([feature_positions[j+1][feature_index],next(iter(set(feature_positions[j])-set(feature_positions[j+1])))]))\n","                  if feature_positions[j].index(feature_positions[j+1][feature_index]) - decider_min_jumps >= feature_index: #before position-decider_min_jumps >= now position #everything will jump one position because of the different length\n","                    new_and_opportunities.append(frozenset([feature_positions[j+1][feature_index],next(iter(set(feature_positions[j])-set(feature_positions[j+1])))]))         \n","        or_opportunities[i] = new_or_opportunities\n","        and_opportunities[i] = new_and_opportunities\n","        fraying_dicts[i] = fraying_dict\n","        frayings[i] = fraying\n","      print(or_opportunities)\n","      print(and_opportunities)\n","      return n_best,or_opportunities\n","    else:\n","      return n_best\n","  if no_cv:\n","    if intermediate_n:\n","      for i in np.arange(1,len(chosen_all_intermediate)+1,1):\n","        test_accuracy = np.sum(test_rights_intermediate[i])/len(y.iloc[gens[0][1]])\n","        try:\n","          auc = roc_auc_score(y.iloc[gens[0][1]],out_intermediate[i].iloc[gens[0][1]])\n","        except:\n","          auc = np.nan #here if test_size is too small and it's all 1 or all 0, the AUC won't work\n","        print(f'Length {i}')\n","        print(f'Accuracy = {test_accuracy}')\n","        print(f'AUC = {auc}')\n","    if usage_map:\n","      map = pd.Series(index = map_columns,dtype = 'float64')\n","      for feature in map_columns:\n","        map.loc[feature] = np.sum([1 for features in chosen_all if feature in features])/len(chosen_all)\n","      map.sort_values(ascending=False,inplace=True)    \n","      with pd.option_context('display.max_rows', None):\n","        print(map)\n","  else:\n","    print(decider_type)\n","    if intermediate_n:\n","      for i in np.arange(1,len(chosen_all_intermediate)+1,1):\n","        if np.sum(out_intermediate[i].isna()) > 0:\n","          break\n","        auc = roc_auc_score(y,out_intermediate[i])\n","        fraying_set = set(chosen_all_intermediate[i])\n","        fraying_list = []\n","        for features in fraying_set:\n","          fraying_list.append(chosen_all_intermediate[i].count(features))\n","        fraying = np.sum([i**2/len(gens)**2 for i in fraying_list])\n","        test_accuracy = np.sum(test_rights_intermediate[i])/len(y)\n","        print(f'Length {i}')\n","        if fraying == 1.0:\n","          print(fraying_set)\n","        print(f'Accuracy = {test_accuracy}')\n","        print(f'AUC = {auc}')\n","        print(f'\"Fraying\":{fraying}, {fraying_list}')\n","    aucs = []\n","    abc = pd.DataFrame(out,columns=['out'])\n","    abc['y'] = y\n","    for i in range(100):\n","      ddd = resample(abc)\n","      aucs.append(roc_auc_score(ddd['y'],ddd['out']))\n","    auc = np.mean(aucs)\n","    auc_error = np.std(aucs)\n","    test_accuracy = np.sum(test_rights)/len(y)\n","    fraying_set = set(chosen_all)\n","    fraying_list = []\n","    for features in fraying_set:\n","      fraying_list.append(chosen_all.count(features))\n","    fraying = np.sum([i**2/len(gens)**2 for i in fraying_list])\n","    if intermediate_n:\n","      print('Altogether:')\n","    if (np.sum(fraying_list) == i) and (fraying == 1.0):\n","      print(fraying_set)\n","    print(f'Accuracy = {test_accuracy}')\n","    print(f'AUC = {auc} +- {auc_error}')\n","    print(f'\"Fraying\":{fraying}, {fraying_list}')\n","    if usage_map:\n","      map = pd.Series(index = map_columns,dtype = 'float64')\n","      for feature in map_columns:\n","        map.loc[feature] = np.sum([1 for features in chosen_all if feature in features])/len(chosen_all)\n","      map.sort_values(ascending=False,inplace=True)    \n","      with pd.option_context('display.max_rows', None):\n","        print(map)\n","    plt.hist(how_many,bins=np.arange(np.min(how_many),np.max(how_many)+1,1)-0.5)\n","    plt.title('Chosen # Features for each Fold')\n","    plt.show()\n","    plt.close()\n","  return out\n","\n","#we don't use these anymore\n","def hard_function(tau, X, y, orthogonal_to = [], show = False, debug = False, estimator = logistic_estimator_with_pytorch, pad = True):\n","  \n","  X_copy = copy.deepcopy(X)\n","  rows = X_copy.columns.to_list()\n","  ca_complete = all(x in rows for x in ca_list)\n","  thal_complete = all(x in rows for x in thal_list)\n","  cp_complete = all(x in rows for x in cp_list)\n","  slope_complete = all(x in rows for x in slope_list)\n","  restecg_complete = all(x in rows for x in restecg_list)\n","\n","  if ca_complete == True:\n","    X_copy.drop([\"ca_0.0\"],axis = 1, inplace=True)\n","  if thal_complete == True:\n","    X_copy.drop([\"thal_3.0\"],axis = 1, inplace= True)\n","  if slope_complete == True:\n","    X_copy.drop([\"slope_1.0\"],axis = 1, inplace= True)\n","  if cp_complete == True:\n","    X_copy.drop([\"cp_4.0\"],axis = 1, inplace= True)\n","  if restecg_complete == True:\n","    X_copy.drop([\"restecg_0.0\"],axis = 1, inplace= True)\n","   \n","  X_copy['y'] = y\n","\n","  x_fit = []\n","  for i in range(30):\n","    abc = resample(X_copy)\n","    abc_y = abc['y'].to_numpy()\n","    abc_x = abc.drop(['y'],axis=1)\n","    scaled_data = preprocessing.scale(abc_x)\n","    data = ExperimentData(scaled_data,abc_y)\n","    INPUT_SIZE = scaled_data.shape[1]\n","    EPOCHS=1 # Few epochs to avoid overfitting\n","    pred_train = {}\n","    HIDDEN_LAYER_SIZE = []\n","    data_loader = DataLoader(data, batch_size=scaled_data.shape[0])\n","    net = NNet(INPUT_SIZE, HIDDEN_LAYER_SIZE, sigmoid=True, loss = my_loss)\n","    net.optim = LBFGS(net.parameters(), history_size=10, max_iter=10,lr=2.0)\n","    net.train(data_loader, EPOCHS)\n","    x_fit.append(np.append(net.layers[0].weight[0].detach().numpy(),net.layers[0].bias[0].detach().numpy()))\n","\n","  # result = cross_correlate(scaled_data,y,estimator=estimator,pad=pad)\n","  \n","  # train_mean = np.mean(result['train_accuracy'])\n","  # train_err_mean = np.std(result['train_accuracy'])\n","  # test_mean = np.mean(result['test_accuracy'])\n","  # test_err_mean = np.std(result['test_accuracy'])\n","\n","  # if debug:\n","  #   print(train_mean) # test\n","  #   print(train_err_mean)\n","\n","  #   print(test_mean) # train\n","  #   print(train_err_mean)\n","\n","  X_copy.drop(['y'],axis=1,inplace=True)\n","\n","  importance = np.mean(x_fit,axis=0)\n","  importance_stdev = np.std(x_fit,axis=0)[:-1]\n","  average = importance[-1]\n","  importance = importance[:-1]\n","\n","  plt.rcParams['figure.figsize'] = [16,8]\n","  x_tick = X_copy.columns.to_list()\n","\n","  if debug:\n","\n","    fig = plt.figure()\n","    plt.bar(x_tick,importance,yerr=importance_stdev)\n","    plt.ylabel(\"Significance\")\n","    plt.show()\n","\n","  importance, importance_stdev,average,x_tick = complete_cats(\"ca\",\"0.0\",ca_complete,orthogonal_to,importance, importance_stdev,average,x_tick,df,X_copy,debug)\n","  importance, importance_stdev,average,x_tick = complete_cats(\"cp\",\"4.0\",cp_complete,orthogonal_to,importance, importance_stdev,average,x_tick,df,X_copy,debug)\n","  importance, importance_stdev,average,x_tick = complete_cats(\"thal\",\"3.0\",thal_complete,orthogonal_to,importance, importance_stdev,average,x_tick,df,X_copy,debug)\n","  importance, importance_stdev,average,x_tick = complete_cats(\"slope\",\"1.0\",slope_complete,orthogonal_to,importance, importance_stdev,average,x_tick,df,X_copy,debug)\n","  importance, importance_stdev,average,x_tick = complete_cats(\"restecg\",\"0.0\",restecg_complete,orthogonal_to,importance, importance_stdev,average,x_tick,df,X_copy,debug)\n","\n","  std_count = np.array([importance[i]/importance_stdev[i] for i in range(len(importance))])\n","  x_tick = np.array(x_tick)\n","  sorted_std = sorted(std_count, key=abs,reverse=True)\n","  sorted_ticks = [tick for _,tick in sorted(zip(abs(std_count),x_tick),reverse=True)]\n","  if debug:\n","    plt.figure(figsize=(28,14))\n","    plt.bar(sorted_ticks,sorted_std)\n","    plt.ylabel(\"sorted std's\")\n","    plt.show()\n","  out = np.array(sorted_ticks)[np.abs(sorted_std)>tau]\n","  if len(np.array(sorted_ticks)[np.abs(sorted_std)<tau]) == 0 or len(out) == 0:\n","    if show:\n","      plt.figure(figsize=(28,14))\n","      plt.bar(sorted_ticks,sorted_std)\n","      plt.ylabel(\"sorted std's\")\n","      plt.show()\n","    return out;\n","  else: \n","    if debug: print(out)\n","    return hard_function(tau, X[out], y, orthogonal_to=orthogonal_to, show = show, debug = debug)\n","\n","#not used\n","def other_hard_function(X, y, orthogonal_to = [], show = False, debug = False, estimator = logistic_estimator_with_pytorch, pad = True,random_features = True, features_premade = []):\n","  \n","  X_copy = copy.deepcopy(X)\n","  # rows = X_copy.columns.to_list()\n","  # ca_complete = all(x in rows for x in ca_list)\n","  # thal_complete = all(x in rows for x in thal_list)\n","  # cp_complete = all(x in rows for x in cp_list)\n","  # slope_complete = all(x in rows for x in slope_list)\n","  # restecg_complete = all(x in rows for x in restecg_list)\n","\n","  # if ca_complete == True:\n","  #   X_copy.drop([\"ca_0.0\"],axis = 1, inplace=True)\n","  # if thal_complete == True:\n","  #   X_copy.drop([\"thal_3.0\"],axis = 1, inplace= True)\n","  # if slope_complete == True:\n","  #   X_copy.drop([\"slope_1.0\"],axis = 1, inplace= True)\n","  # if cp_complete == True:\n","  #   X_copy.drop([\"cp_4.0\"],axis = 1, inplace= True)\n","  # if restecg_complete == True:\n","  #   X_copy.drop([\"restecg_0.0\"],axis = 1, inplace= True)\n","\n","  X_copy['y'] = y\n","\n","  hij = []\n","  did_appear = {}\n","  for feature in X.columns:\n","    did_appear[feature] = []\n","\n","  for i in tqdm(range(2000)):\n","    random_features_list = []\n","    if random_features:\n","      random_features_list = [feature for feature in X.columns.drop(features_premade) if random.random() > 0.05]\n","    bcd = random_features_list+features_premade + ['y']\n","    abc = resample(X_copy[bcd])\n","    abc_y = abc['y'].to_numpy()\n","    abc_x = abc.drop(['y'],axis=1)\n","    scaled_data = preprocessing.scale(abc_x)\n","    \n","    model = LogisticRegression(max_iter = 1000,fit_intercept=True,solver='liblinear',C=1000)\n","    model.fit(abc_x,abc_y)\n","    x_fit = np.append(model.coef_[0],model.intercept_[0])\n","\n","    # data = ExperimentData(scaled_data,abc_y)\n","    # INPUT_SIZE = scaled_data.shape[1]\n","    # EPOCHS=1 # Few epochs to avoid overfitting\n","    # pred_train = {}\n","    # HIDDEN_LAYER_SIZE = []\n","    # data_loader = DataLoader(data, batch_size=scaled_data.shape[0])\n","    # net = NNet(INPUT_SIZE, HIDDEN_LAYER_SIZE, sigmoid=True, loss = my_loss)\n","    # net.optim = LBFGS(net.parameters(), history_size=10, max_iter=10,lr=2.0)\n","    # net.train(data_loader, EPOCHS)\n","    # x_fit = np.append(net.layers[0].weight[0].detach().numpy(),net.layers[0].bias[0].detach().numpy())\n","    # hij.append(x_fit)\n","    features = random_features_list+features_premade\n","    for i in range(len(features)):\n","      did_appear[features[i]].append(x_fit[i])\n","\n","  X_copy.drop(['y'],axis=1,inplace=True)\n","\n","  importance = {}\n","  importance_stdev = {}\n","  for feature in X.columns:\n","    importance[feature] = np.mean(did_appear[feature])\n","    importance_stdev[feature] = np.std(did_appear[feature])\n","\n","  plt.rcParams['figure.figsize'] = [16,8]\n","  x_tick = X_copy.columns.to_list()\n","\n","  fig = plt.figure()\n","  plt.bar(x_tick,importance.values(),yerr=importance_stdev.values())\n","  plt.ylabel(\"Significance\")\n","  plt.show()\n","\n","  if debug:\n","    for feature in X.columns:\n","      plt.hist(did_appear[feature],label = feature)\n","      plt.legend()\n","      plt.show()\n","      plt.close()\n","\n","  # importance, importance_stdev,average,x_tick = complete_cats(\"ca\",\"0.0\",ca_complete,orthogonal_to,importance, importance_stdev,average,x_tick,df,X_copy,debug)\n","  # importance, importance_stdev,average,x_tick = complete_cats(\"cp\",\"4.0\",cp_complete,orthogonal_to,importance, importance_stdev,average,x_tick,df,X_copy,debug)\n","  # importance, importance_stdev,average,x_tick = complete_cats(\"thal\",\"3.0\",thal_complete,orthogonal_to,importance, importance_stdev,average,x_tick,df,X_copy,debug)\n","  # importance, importance_stdev,average,x_tick = complete_cats(\"slope\",\"1.0\",slope_complete,orthogonal_to,importance, importance_stdev,average,x_tick,df,X_copy,debug)\n","  # importance, importance_stdev,average,x_tick = complete_cats(\"restecg\",\"0.0\",restecg_complete,orthogonal_to,importance, importance_stdev,average,x_tick,df,X_copy,debug)\n","\n","  std_count = np.array([importance[i]/importance_stdev[i] for i in X.columns])\n","  x_tick = np.array(x_tick)\n","  sorted_std = sorted(std_count, key=abs,reverse=True)\n","  sorted_ticks = [tick for _,tick in sorted(zip(abs(std_count),x_tick),reverse=True)]\n","\n","  plt.figure(figsize=(150,75))\n","  plt.bar(sorted_ticks,sorted_std)\n","  plt.ylabel(\"sorted std's\")\n","  plt.show()\n","  \n","  return did_appear"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fvBMXcW0Lshf"},"source":["## Data 'wrangling'"]},{"cell_type":"code","metadata":{"id":"vccEcXy3OoTg"},"source":["df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data',header=None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G_wmJjL3hjBp"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8f5VM0aCOvLO","executionInfo":{"status":"ok","timestamp":1657080019691,"user_tz":420,"elapsed":265,"user":{"displayName":"漢訥澀","userId":"11127674219014497621"}},"outputId":"d1ccde3b-e04c-4860-b690-c8231c49a2b0"},"source":["df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     0    1    2      3      4    5    6      7    8    9    10   11   12  13\n","0  63.0  1.0  1.0  145.0  233.0  1.0  2.0  150.0  0.0  2.3  3.0  0.0  6.0   0\n","1  67.0  1.0  4.0  160.0  286.0  0.0  2.0  108.0  1.0  1.5  2.0  3.0  3.0   2\n","2  67.0  1.0  4.0  120.0  229.0  0.0  2.0  129.0  1.0  2.6  2.0  2.0  7.0   1\n","3  37.0  1.0  3.0  130.0  250.0  0.0  0.0  187.0  0.0  3.5  3.0  0.0  3.0   0\n","4  41.0  0.0  2.0  130.0  204.0  0.0  2.0  172.0  0.0  1.4  1.0  0.0  3.0   0"],"text/html":["\n","  <div id=\"df-7a6ebe31-ab60-4fd5-8c72-15b388f4911e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>63.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>145.0</td>\n","      <td>233.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>150.0</td>\n","      <td>0.0</td>\n","      <td>2.3</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>6.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>67.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>160.0</td>\n","      <td>286.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>108.0</td>\n","      <td>1.0</td>\n","      <td>1.5</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>67.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>120.0</td>\n","      <td>229.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>129.0</td>\n","      <td>1.0</td>\n","      <td>2.6</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>7.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>37.0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>130.0</td>\n","      <td>250.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>187.0</td>\n","      <td>0.0</td>\n","      <td>3.5</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>3.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>41.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>130.0</td>\n","      <td>204.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>172.0</td>\n","      <td>0.0</td>\n","      <td>1.4</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>3.0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7a6ebe31-ab60-4fd5-8c72-15b388f4911e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-7a6ebe31-ab60-4fd5-8c72-15b388f4911e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-7a6ebe31-ab60-4fd5-8c72-15b388f4911e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"xqwfhy23PEq9"},"source":["columns = [\"age\",\"sex\",\"cp\",\"trestbps\",\"chol\",\"fbs\",\"restecg\",\"thalach\",\"exang\",\"oldpeak\",\"slope\",\"ca\",\"thal\",\"num\"]       "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dTJr05HVQdq5"},"source":["* sex: sex (1 = male; 0 = female)\n","\n","\n","\n","* cp: \n","\n","        -- Value 1: typical angina\n","        -- Value 2: atypical angina\n","        -- Value 3: non-anginal pain\n","        -- Value 4: asymptomatic\n","\n","* trestbps: resting blood pressure (in mm Hg on admission to the hospital)\n","\n","* chol: serum cholestoral in mg/dl\n","\n","* fbs: (fasting blood sugar > 120 mg/dl)  (1 = true; 0 = false)\n","\n","* restecg: resting electrocardiographic results\n","\n","        -- Value 0: normal\n","        -- Value 1: having ST-T wave abnormality (T wave inversions and/or ST \n","                    elevation or depression of > 0.05 mV)\n","        -- Value 2: showing probable or definite left ventricular hypertrophy\n","                    by Estes' criteria\n","\n","* thalach: maximum heart rate achieved\n","\n","* exang: exercise induced angina (1 = yes; 0 = no)\n","\n","* oldpeak = ST depression induced by exercise relative to rest\n","\n","* slope: the slope of the peak exercise ST segment\n","        -- Value 1: upsloping\n","        -- Value 2: flat\n","        -- Value 3: downsloping\n","* ca: number of major vessels (0-3) colored by flourosopy\n","\n","* thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n","\n","* num: diagnosis of heart disease (angiographic disease status)\n","        -- Value 0: < 50% diameter narrowing\n","        -- Value 1: > 50% diameter narrowing\n","        (in any major vessel: attributes 59 through 68 are vessels)\n"]},{"cell_type":"code","metadata":{"id":"k8sGM3RPQtDN"},"source":["df.columns = columns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nlrI-8p2TJnd","executionInfo":{"status":"ok","timestamp":1657080019692,"user_tz":420,"elapsed":256,"user":{"displayName":"漢訥澀","userId":"11127674219014497621"}},"outputId":"5a00be86-1993-4f85-fd67-8b6b1b89cc47"},"source":["df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["    age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n","0  63.0  1.0  1.0     145.0  233.0  1.0      2.0    150.0    0.0      2.3   \n","1  67.0  1.0  4.0     160.0  286.0  0.0      2.0    108.0    1.0      1.5   \n","2  67.0  1.0  4.0     120.0  229.0  0.0      2.0    129.0    1.0      2.6   \n","3  37.0  1.0  3.0     130.0  250.0  0.0      0.0    187.0    0.0      3.5   \n","4  41.0  0.0  2.0     130.0  204.0  0.0      2.0    172.0    0.0      1.4   \n","\n","   slope   ca thal  num  \n","0    3.0  0.0  6.0    0  \n","1    2.0  3.0  3.0    2  \n","2    2.0  2.0  7.0    1  \n","3    3.0  0.0  3.0    0  \n","4    1.0  0.0  3.0    0  "],"text/html":["\n","  <div id=\"df-ecf5dc25-56a3-414a-8e3f-db1271e6ca0b\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>sex</th>\n","      <th>cp</th>\n","      <th>trestbps</th>\n","      <th>chol</th>\n","      <th>fbs</th>\n","      <th>restecg</th>\n","      <th>thalach</th>\n","      <th>exang</th>\n","      <th>oldpeak</th>\n","      <th>slope</th>\n","      <th>ca</th>\n","      <th>thal</th>\n","      <th>num</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>63.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>145.0</td>\n","      <td>233.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>150.0</td>\n","      <td>0.0</td>\n","      <td>2.3</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>6.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>67.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>160.0</td>\n","      <td>286.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>108.0</td>\n","      <td>1.0</td>\n","      <td>1.5</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>67.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>120.0</td>\n","      <td>229.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>129.0</td>\n","      <td>1.0</td>\n","      <td>2.6</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>7.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>37.0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>130.0</td>\n","      <td>250.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>187.0</td>\n","      <td>0.0</td>\n","      <td>3.5</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>3.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>41.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>130.0</td>\n","      <td>204.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>172.0</td>\n","      <td>0.0</td>\n","      <td>1.4</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>3.0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ecf5dc25-56a3-414a-8e3f-db1271e6ca0b')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-ecf5dc25-56a3-414a-8e3f-db1271e6ca0b button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-ecf5dc25-56a3-414a-8e3f-db1271e6ca0b');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":10}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gh_Xldq6Xrk3","executionInfo":{"status":"ok","timestamp":1657080019692,"user_tz":420,"elapsed":254,"user":{"displayName":"漢訥澀","userId":"11127674219014497621"}},"outputId":"43f60bbd-1893-4553-8252-5fec95e5165d"},"source":["df[\"ca\"].value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.0    176\n","1.0     65\n","2.0     38\n","3.0     20\n","?        4\n","Name: ca, dtype: int64"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GpH5X0947WKy","executionInfo":{"status":"ok","timestamp":1657080019693,"user_tz":420,"elapsed":253,"user":{"displayName":"漢訥澀","userId":"11127674219014497621"}},"outputId":"d4901ec3-c30b-412e-9257-c589d66d568c"},"source":["df.info()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 303 entries, 0 to 302\n","Data columns (total 14 columns):\n"," #   Column    Non-Null Count  Dtype  \n","---  ------    --------------  -----  \n"," 0   age       303 non-null    float64\n"," 1   sex       303 non-null    float64\n"," 2   cp        303 non-null    float64\n"," 3   trestbps  303 non-null    float64\n"," 4   chol      303 non-null    float64\n"," 5   fbs       303 non-null    float64\n"," 6   restecg   303 non-null    float64\n"," 7   thalach   303 non-null    float64\n"," 8   exang     303 non-null    float64\n"," 9   oldpeak   303 non-null    float64\n"," 10  slope     303 non-null    float64\n"," 11  ca        303 non-null    object \n"," 12  thal      303 non-null    object \n"," 13  num       303 non-null    int64  \n","dtypes: float64(11), int64(1), object(2)\n","memory usage: 33.3+ KB\n"]}]},{"cell_type":"markdown","metadata":{"id":"v4Sv0_Rc8CK7"},"source":["Let's look at ca and thal."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PqAAa2xnT5NS","executionInfo":{"status":"ok","timestamp":1657080019693,"user_tz":420,"elapsed":252,"user":{"displayName":"漢訥澀","userId":"11127674219014497621"}},"outputId":"9d99fb58-e038-46cd-edb8-6bd012dd7692"},"source":["df[\"ca\"].value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.0    176\n","1.0     65\n","2.0     38\n","3.0     20\n","?        4\n","Name: ca, dtype: int64"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"YvJOL0Sw79G1"},"source":["The ? represent missing data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ASheoYEQVBqz","executionInfo":{"status":"ok","timestamp":1657080019693,"user_tz":420,"elapsed":250,"user":{"displayName":"漢訥澀","userId":"11127674219014497621"}},"outputId":"c0661347-8f6f-4b82-a75a-61f2a00bd595"},"source":["df[\"thal\"].value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3.0    166\n","7.0    117\n","6.0     18\n","?        2\n","Name: thal, dtype: int64"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fn5hGCMlVJbs","executionInfo":{"status":"ok","timestamp":1657080019694,"user_tz":420,"elapsed":250,"user":{"displayName":"漢訥澀","userId":"11127674219014497621"}},"outputId":"9ce5af28-8f32-469c-800a-1decb68d7fce"},"source":["df.loc[(df[\"ca\"]==\"?\") | (df[\"thal\"]==\"?\")]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n","87   53.0  0.0  3.0     128.0  216.0  0.0      2.0    115.0    0.0      0.0   \n","166  52.0  1.0  3.0     138.0  223.0  0.0      0.0    169.0    0.0      0.0   \n","192  43.0  1.0  4.0     132.0  247.0  1.0      2.0    143.0    1.0      0.1   \n","266  52.0  1.0  4.0     128.0  204.0  1.0      0.0    156.0    1.0      1.0   \n","287  58.0  1.0  2.0     125.0  220.0  0.0      0.0    144.0    0.0      0.4   \n","302  38.0  1.0  3.0     138.0  175.0  0.0      0.0    173.0    0.0      0.0   \n","\n","     slope   ca thal  num  \n","87     1.0  0.0    ?    0  \n","166    1.0    ?  3.0    0  \n","192    2.0    ?  7.0    1  \n","266    2.0  0.0    ?    2  \n","287    2.0    ?  7.0    0  \n","302    1.0    ?  3.0    0  "],"text/html":["\n","  <div id=\"df-dd827bcc-1b79-46ee-a619-5f1a89eaff2b\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>sex</th>\n","      <th>cp</th>\n","      <th>trestbps</th>\n","      <th>chol</th>\n","      <th>fbs</th>\n","      <th>restecg</th>\n","      <th>thalach</th>\n","      <th>exang</th>\n","      <th>oldpeak</th>\n","      <th>slope</th>\n","      <th>ca</th>\n","      <th>thal</th>\n","      <th>num</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>87</th>\n","      <td>53.0</td>\n","      <td>0.0</td>\n","      <td>3.0</td>\n","      <td>128.0</td>\n","      <td>216.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>115.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>?</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>166</th>\n","      <td>52.0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>138.0</td>\n","      <td>223.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>169.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>?</td>\n","      <td>3.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>192</th>\n","      <td>43.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>132.0</td>\n","      <td>247.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>143.0</td>\n","      <td>1.0</td>\n","      <td>0.1</td>\n","      <td>2.0</td>\n","      <td>?</td>\n","      <td>7.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>266</th>\n","      <td>52.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>128.0</td>\n","      <td>204.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>156.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>?</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>287</th>\n","      <td>58.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>125.0</td>\n","      <td>220.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>144.0</td>\n","      <td>0.0</td>\n","      <td>0.4</td>\n","      <td>2.0</td>\n","      <td>?</td>\n","      <td>7.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>302</th>\n","      <td>38.0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>138.0</td>\n","      <td>175.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>173.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>?</td>\n","      <td>3.0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dd827bcc-1b79-46ee-a619-5f1a89eaff2b')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-dd827bcc-1b79-46ee-a619-5f1a89eaff2b button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-dd827bcc-1b79-46ee-a619-5f1a89eaff2b');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NEtGcCq2VTm1","executionInfo":{"status":"ok","timestamp":1657080019695,"user_tz":420,"elapsed":249,"user":{"displayName":"漢訥澀","userId":"11127674219014497621"}},"outputId":"92a956c7-33f9-4cc0-a212-89caf7cd11bb"},"source":["df.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(303, 14)"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"nlVjBpth0r0M"},"source":["Imputing missing values (this is done once below). We get the same result using 'num' or not using 'num' so we can do this once and don't need to differentiate train and test.\n"]},{"cell_type":"code","metadata":{"id":"T9yMgOi99TLV"},"source":["# #Because we have Accuracy = 85%, we only use data with more than 85% chance of being correct.\n","df.loc[87,'thal'] = 3.0 # chance of being correct is 99% - this becomes 98% when using num\n","# df.loc[166,'ca'] = 0.0 # chance is 75%\n","# df.loc[192,'ca'] = 0.0 # chance is 70#\n","# df.loc[266,'thal'] = 7.0 # chance of being correct is 71% - this becomes 88.5% when also using num but we can't always use num\n","# df.loc[287,'ca'] = 0.0 # chance is 69%\n","df.loc[302,'ca'] = 0.0 # chance is 95% - this becomes 94% when using num"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OF8Yw311V2ep"},"source":["df.drop(index=166,inplace=True)\n","df.drop(index=192,inplace=True)\n","df.drop(index=266,inplace=True)\n","df.drop(index=287,inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zJhoOt2X9bbt","executionInfo":{"status":"ok","timestamp":1657080019696,"user_tz":420,"elapsed":228,"user":{"displayName":"漢訥澀","userId":"11127674219014497621"}},"outputId":"d92acab9-9e52-4400-842c-941a08cf98d2"},"source":["df.loc[(df[\"ca\"]==\"?\") | (df[\"thal\"]==\"?\")]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Empty DataFrame\n","Columns: [age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpeak, slope, ca, thal, num]\n","Index: []"],"text/html":["\n","  <div id=\"df-aebfd3c0-23b2-452e-9bcb-c7e97069d96a\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>sex</th>\n","      <th>cp</th>\n","      <th>trestbps</th>\n","      <th>chol</th>\n","      <th>fbs</th>\n","      <th>restecg</th>\n","      <th>thalach</th>\n","      <th>exang</th>\n","      <th>oldpeak</th>\n","      <th>slope</th>\n","      <th>ca</th>\n","      <th>thal</th>\n","      <th>num</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aebfd3c0-23b2-452e-9bcb-c7e97069d96a')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-aebfd3c0-23b2-452e-9bcb-c7e97069d96a button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-aebfd3c0-23b2-452e-9bcb-c7e97069d96a');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"eryyIwkM9d0T"},"source":["We don't have any more missing data and can start building the model. df_old is the original data frame."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v3qTSA7kKQzv","executionInfo":{"status":"ok","timestamp":1657080019696,"user_tz":420,"elapsed":227,"user":{"displayName":"漢訥澀","userId":"11127674219014497621"}},"outputId":"b3d724de-cce5-41ba-9fdf-7f4b27a33df0"},"source":["df['ca'] = df['ca'].astype(float, errors = 'raise')\n","df['thal'] = df['thal'].astype(float, errors = 'raise')\n","\n","df.info()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 299 entries, 0 to 302\n","Data columns (total 14 columns):\n"," #   Column    Non-Null Count  Dtype  \n","---  ------    --------------  -----  \n"," 0   age       299 non-null    float64\n"," 1   sex       299 non-null    float64\n"," 2   cp        299 non-null    float64\n"," 3   trestbps  299 non-null    float64\n"," 4   chol      299 non-null    float64\n"," 5   fbs       299 non-null    float64\n"," 6   restecg   299 non-null    float64\n"," 7   thalach   299 non-null    float64\n"," 8   exang     299 non-null    float64\n"," 9   oldpeak   299 non-null    float64\n"," 10  slope     299 non-null    float64\n"," 11  ca        299 non-null    float64\n"," 12  thal      299 non-null    float64\n"," 13  num       299 non-null    int64  \n","dtypes: float64(13), int64(1)\n","memory usage: 35.0 KB\n"]}]},{"cell_type":"markdown","metadata":{"id":"w8eqPtax9mgM"},"source":["### **Split data into dependent and independent variables.**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wgS7BxTl-yNh","executionInfo":{"status":"ok","timestamp":1657080019696,"user_tz":420,"elapsed":223,"user":{"displayName":"漢訥澀","userId":"11127674219014497621"}},"outputId":"3def8be8-58b4-4b2b-fe51-d34c90b4ca37"},"source":["y = df['num'].copy()\n","X = df.drop('num',axis=1).copy()\n","display(X)\n","display(y)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["      age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n","0    63.0  1.0  1.0     145.0  233.0  1.0      2.0    150.0    0.0      2.3   \n","1    67.0  1.0  4.0     160.0  286.0  0.0      2.0    108.0    1.0      1.5   \n","2    67.0  1.0  4.0     120.0  229.0  0.0      2.0    129.0    1.0      2.6   \n","3    37.0  1.0  3.0     130.0  250.0  0.0      0.0    187.0    0.0      3.5   \n","4    41.0  0.0  2.0     130.0  204.0  0.0      2.0    172.0    0.0      1.4   \n","..    ...  ...  ...       ...    ...  ...      ...      ...    ...      ...   \n","298  45.0  1.0  1.0     110.0  264.0  0.0      0.0    132.0    0.0      1.2   \n","299  68.0  1.0  4.0     144.0  193.0  1.0      0.0    141.0    0.0      3.4   \n","300  57.0  1.0  4.0     130.0  131.0  0.0      0.0    115.0    1.0      1.2   \n","301  57.0  0.0  2.0     130.0  236.0  0.0      2.0    174.0    0.0      0.0   \n","302  38.0  1.0  3.0     138.0  175.0  0.0      0.0    173.0    0.0      0.0   \n","\n","     slope   ca  thal  \n","0      3.0  0.0   6.0  \n","1      2.0  3.0   3.0  \n","2      2.0  2.0   7.0  \n","3      3.0  0.0   3.0  \n","4      1.0  0.0   3.0  \n","..     ...  ...   ...  \n","298    2.0  0.0   7.0  \n","299    2.0  2.0   7.0  \n","300    2.0  1.0   7.0  \n","301    2.0  1.0   3.0  \n","302    1.0  0.0   3.0  \n","\n","[299 rows x 13 columns]"],"text/html":["\n","  <div id=\"df-eefcf0c3-b2b8-472d-af4c-2087cece36b0\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>sex</th>\n","      <th>cp</th>\n","      <th>trestbps</th>\n","      <th>chol</th>\n","      <th>fbs</th>\n","      <th>restecg</th>\n","      <th>thalach</th>\n","      <th>exang</th>\n","      <th>oldpeak</th>\n","      <th>slope</th>\n","      <th>ca</th>\n","      <th>thal</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>63.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>145.0</td>\n","      <td>233.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>150.0</td>\n","      <td>0.0</td>\n","      <td>2.3</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>6.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>67.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>160.0</td>\n","      <td>286.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>108.0</td>\n","      <td>1.0</td>\n","      <td>1.5</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>67.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>120.0</td>\n","      <td>229.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>129.0</td>\n","      <td>1.0</td>\n","      <td>2.6</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>7.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>37.0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>130.0</td>\n","      <td>250.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>187.0</td>\n","      <td>0.0</td>\n","      <td>3.5</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>41.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>130.0</td>\n","      <td>204.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>172.0</td>\n","      <td>0.0</td>\n","      <td>1.4</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>298</th>\n","      <td>45.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>110.0</td>\n","      <td>264.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>132.0</td>\n","      <td>0.0</td>\n","      <td>1.2</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>7.0</td>\n","    </tr>\n","    <tr>\n","      <th>299</th>\n","      <td>68.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>144.0</td>\n","      <td>193.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>141.0</td>\n","      <td>0.0</td>\n","      <td>3.4</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>7.0</td>\n","    </tr>\n","    <tr>\n","      <th>300</th>\n","      <td>57.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>130.0</td>\n","      <td>131.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>115.0</td>\n","      <td>1.0</td>\n","      <td>1.2</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>7.0</td>\n","    </tr>\n","    <tr>\n","      <th>301</th>\n","      <td>57.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>130.0</td>\n","      <td>236.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>174.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>302</th>\n","      <td>38.0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>138.0</td>\n","      <td>175.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>173.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>3.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>299 rows × 13 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eefcf0c3-b2b8-472d-af4c-2087cece36b0')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-eefcf0c3-b2b8-472d-af4c-2087cece36b0 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-eefcf0c3-b2b8-472d-af4c-2087cece36b0');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["0      0\n","1      2\n","2      1\n","3      0\n","4      0\n","      ..\n","298    1\n","299    2\n","300    3\n","301    1\n","302    0\n","Name: num, Length: 299, dtype: int64"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C1vJdjC-Dr6Q","executionInfo":{"status":"ok","timestamp":1657080019697,"user_tz":420,"elapsed":214,"user":{"displayName":"漢訥澀","userId":"11127674219014497621"}},"outputId":"e159b041-6d9a-4312-ff80-0c7011521942"},"source":["y.value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    162\n","1     54\n","2     35\n","3     35\n","4     13\n","Name: num, dtype: int64"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"y6uKbGBzDxAw"},"source":["Why is the outcome between 0 and 4? It should just be 0 or 1. Even the website doesn't know."]},{"cell_type":"markdown","metadata":{"id":"LoCPW8-_Tc_j"},"source":["Let's look at the data:"]},{"cell_type":"markdown","metadata":{"id":"zzfiWCzR-995"},"source":["### *one-hot encoding*"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FuLIF8tmDX4-","executionInfo":{"status":"ok","timestamp":1657080019697,"user_tz":420,"elapsed":212,"user":{"displayName":"漢訥澀","userId":"11127674219014497621"}},"outputId":"bc053afc-cf48-4432-d633-50fdf956b8b2"},"source":["for i in X.columns:\n","  display(X[i].value_counts())"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["58.0    18\n","57.0    17\n","54.0    16\n","59.0    14\n","60.0    12\n","51.0    12\n","56.0    11\n","62.0    11\n","44.0    11\n","52.0    11\n","64.0    10\n","41.0    10\n","67.0     9\n","63.0     9\n","42.0     8\n","45.0     8\n","53.0     8\n","55.0     8\n","61.0     8\n","65.0     8\n","50.0     7\n","66.0     7\n","43.0     7\n","48.0     7\n","46.0     7\n","47.0     5\n","49.0     5\n","70.0     4\n","68.0     4\n","35.0     4\n","39.0     4\n","69.0     3\n","71.0     3\n","40.0     3\n","34.0     2\n","37.0     2\n","38.0     2\n","29.0     1\n","77.0     1\n","74.0     1\n","76.0     1\n","Name: age, dtype: int64"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["1.0    202\n","0.0     97\n","Name: sex, dtype: int64"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["4.0    142\n","3.0     85\n","2.0     49\n","1.0     23\n","Name: cp, dtype: int64"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["120.0    37\n","130.0    36\n","140.0    32\n","110.0    19\n","150.0    17\n","138.0    11\n","160.0    11\n","128.0    11\n","125.0    10\n","112.0     9\n","132.0     7\n","118.0     7\n","124.0     6\n","108.0     6\n","135.0     6\n","152.0     5\n","134.0     5\n","145.0     5\n","100.0     4\n","170.0     4\n","122.0     4\n","126.0     3\n","136.0     3\n","115.0     3\n","180.0     3\n","142.0     3\n","105.0     3\n","102.0     2\n","146.0     2\n","144.0     2\n","148.0     2\n","178.0     2\n","94.0      2\n","165.0     1\n","123.0     1\n","114.0     1\n","154.0     1\n","156.0     1\n","106.0     1\n","155.0     1\n","172.0     1\n","200.0     1\n","101.0     1\n","129.0     1\n","192.0     1\n","158.0     1\n","104.0     1\n","174.0     1\n","117.0     1\n","164.0     1\n","Name: trestbps, dtype: int64"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["197.0    6\n","234.0    6\n","269.0    5\n","204.0    5\n","212.0    5\n","        ..\n","247.0    1\n","340.0    1\n","160.0    1\n","394.0    1\n","131.0    1\n","Name: chol, Length: 152, dtype: int64"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["0.0    256\n","1.0     43\n","Name: fbs, dtype: int64"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["0.0    148\n","2.0    147\n","1.0      4\n","Name: restecg, dtype: int64"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["162.0    11\n","163.0     9\n","160.0     9\n","152.0     8\n","150.0     7\n","         ..\n","177.0     1\n","127.0     1\n","97.0      1\n","190.0     1\n","90.0      1\n","Name: thalach, Length: 91, dtype: int64"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["0.0    202\n","1.0     97\n","Name: exang, dtype: int64"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["0.0    98\n","1.2    17\n","0.6    14\n","1.4    13\n","0.8    13\n","1.0    13\n","0.2    12\n","1.6    11\n","1.8    10\n","2.0     9\n","0.4     8\n","0.1     6\n","2.8     6\n","2.6     6\n","1.9     5\n","0.5     5\n","3.0     5\n","1.5     5\n","3.6     4\n","2.2     4\n","3.4     3\n","0.9     3\n","2.4     3\n","0.3     3\n","4.0     3\n","1.1     2\n","4.2     2\n","2.3     2\n","2.5     2\n","3.2     2\n","5.6     1\n","2.9     1\n","6.2     1\n","2.1     1\n","1.3     1\n","3.1     1\n","3.8     1\n","0.7     1\n","3.5     1\n","4.4     1\n","Name: oldpeak, dtype: int64"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["1.0    141\n","2.0    137\n","3.0     21\n","Name: slope, dtype: int64"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["0.0    176\n","1.0     65\n","2.0     38\n","3.0     20\n","Name: ca, dtype: int64"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["3.0    166\n","7.0    115\n","6.0     18\n","Name: thal, dtype: int64"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"asLWvVFiEdn1"},"source":["- [ ] age is a float\n","- [ ] sex is a category (0,1)\n","- [ ] cp is a category (1,2,3,4)\n","- [ ] trestbps is a float\n","- [ ] chol is a float\n","- [ ] fbs is a category (0,1)\n","- [ ] restecg is a category (0,1,2) !! 1 only appears 4 times\n","- [ ] thalach is a float\n","- [ ] exang is a category (0,1)\n","- [ ] oldpeak is a float\n","- [ ] slope is a category (1,2,3)\n","- [ ] ca is a category (0,1,2,3)\n","- [ ] thal is a category (3,6,7)\n","\n","\n","I need to one-hot encode:\n","\n","cp,restecg,slope,ca,thal\n","\n","my other categories are:\n","\n","sex,fbs,exang\n","\n","my other floats are:\n","\n","age, trestbps,chol,thalach,oldpeak\n"]},{"cell_type":"markdown","metadata":{"id":"QLV386R8GamO"},"source":["Let's do cp:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t7_huvj_Gqio","executionInfo":{"status":"ok","timestamp":1657080020053,"user_tz":420,"elapsed":563,"user":{"displayName":"漢訥澀","userId":"11127674219014497621"}},"outputId":"3fbb4a16-ee0a-46ad-8ef8-7dda562255ab"},"source":["X[\"cp\"].value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4.0    142\n","3.0     85\n","2.0     49\n","1.0     23\n","Name: cp, dtype: int64"]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"PmCE77SZHusK"},"source":["We need to set cp1 to 1 if cp = 1 or 0 if it's not:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6rz0WSn3GuFJ","executionInfo":{"status":"ok","timestamp":1657080020057,"user_tz":420,"elapsed":76,"user":{"displayName":"漢訥澀","userId":"11127674219014497621"}},"outputId":"9819841b-ec5f-4dbd-93b8-c5b71ba9cb47"},"source":["X_old = X.copy()\n","for row in X.index:\n","  X.loc[row,\"cp1\"] = X.loc[row,\"cp\"] == 1.0\n","  X.loc[row,\"cp2\"] = X.loc[row,\"cp\"] == 2.0\n","  X.loc[row,\"cp3\"] = X.loc[row,\"cp\"] == 3.0\n","  X.loc[row,\"cp4\"] = X.loc[row,\"cp\"] == 4.0\n","X.drop(\"cp\",axis=1,inplace=True)\n","X"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      age  sex  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n","0    63.0  1.0     145.0  233.0  1.0      2.0    150.0    0.0      2.3    3.0   \n","1    67.0  1.0     160.0  286.0  0.0      2.0    108.0    1.0      1.5    2.0   \n","2    67.0  1.0     120.0  229.0  0.0      2.0    129.0    1.0      2.6    2.0   \n","3    37.0  1.0     130.0  250.0  0.0      0.0    187.0    0.0      3.5    3.0   \n","4    41.0  0.0     130.0  204.0  0.0      2.0    172.0    0.0      1.4    1.0   \n","..    ...  ...       ...    ...  ...      ...      ...    ...      ...    ...   \n","298  45.0  1.0     110.0  264.0  0.0      0.0    132.0    0.0      1.2    2.0   \n","299  68.0  1.0     144.0  193.0  1.0      0.0    141.0    0.0      3.4    2.0   \n","300  57.0  1.0     130.0  131.0  0.0      0.0    115.0    1.0      1.2    2.0   \n","301  57.0  0.0     130.0  236.0  0.0      2.0    174.0    0.0      0.0    2.0   \n","302  38.0  1.0     138.0  175.0  0.0      0.0    173.0    0.0      0.0    1.0   \n","\n","      ca  thal    cp1    cp2    cp3    cp4  \n","0    0.0   6.0   True  False  False  False  \n","1    3.0   3.0  False  False  False   True  \n","2    2.0   7.0  False  False  False   True  \n","3    0.0   3.0  False  False   True  False  \n","4    0.0   3.0  False   True  False  False  \n","..   ...   ...    ...    ...    ...    ...  \n","298  0.0   7.0   True  False  False  False  \n","299  2.0   7.0  False  False  False   True  \n","300  1.0   7.0  False  False  False   True  \n","301  1.0   3.0  False   True  False  False  \n","302  0.0   3.0  False  False   True  False  \n","\n","[299 rows x 16 columns]"],"text/html":["\n","  <div id=\"df-5f9dedab-7b1b-474e-9cb7-75bfcd7df950\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>sex</th>\n","      <th>trestbps</th>\n","      <th>chol</th>\n","      <th>fbs</th>\n","      <th>restecg</th>\n","      <th>thalach</th>\n","      <th>exang</th>\n","      <th>oldpeak</th>\n","      <th>slope</th>\n","      <th>ca</th>\n","      <th>thal</th>\n","      <th>cp1</th>\n","      <th>cp2</th>\n","      <th>cp3</th>\n","      <th>cp4</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>63.0</td>\n","      <td>1.0</td>\n","      <td>145.0</td>\n","      <td>233.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>150.0</td>\n","      <td>0.0</td>\n","      <td>2.3</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>6.0</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>67.0</td>\n","      <td>1.0</td>\n","      <td>160.0</td>\n","      <td>286.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>108.0</td>\n","      <td>1.0</td>\n","      <td>1.5</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>67.0</td>\n","      <td>1.0</td>\n","      <td>120.0</td>\n","      <td>229.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>129.0</td>\n","      <td>1.0</td>\n","      <td>2.6</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>7.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>37.0</td>\n","      <td>1.0</td>\n","      <td>130.0</td>\n","      <td>250.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>187.0</td>\n","      <td>0.0</td>\n","      <td>3.5</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>3.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>41.0</td>\n","      <td>0.0</td>\n","      <td>130.0</td>\n","      <td>204.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>172.0</td>\n","      <td>0.0</td>\n","      <td>1.4</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>3.0</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>298</th>\n","      <td>45.0</td>\n","      <td>1.0</td>\n","      <td>110.0</td>\n","      <td>264.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>132.0</td>\n","      <td>0.0</td>\n","      <td>1.2</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>7.0</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>299</th>\n","      <td>68.0</td>\n","      <td>1.0</td>\n","      <td>144.0</td>\n","      <td>193.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>141.0</td>\n","      <td>0.0</td>\n","      <td>3.4</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>7.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>300</th>\n","      <td>57.0</td>\n","      <td>1.0</td>\n","      <td>130.0</td>\n","      <td>131.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>115.0</td>\n","      <td>1.0</td>\n","      <td>1.2</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>7.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>301</th>\n","      <td>57.0</td>\n","      <td>0.0</td>\n","      <td>130.0</td>\n","      <td>236.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>174.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>302</th>\n","      <td>38.0</td>\n","      <td>1.0</td>\n","      <td>138.0</td>\n","      <td>175.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>173.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>3.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>299 rows × 16 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5f9dedab-7b1b-474e-9cb7-75bfcd7df950')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-5f9dedab-7b1b-474e-9cb7-75bfcd7df950 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-5f9dedab-7b1b-474e-9cb7-75bfcd7df950');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","metadata":{"id":"_dOo0QHbNaKv"},"source":["That looks good, let's check if any are all False (that means, the cp was something other than what we looked for above)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cqOTuJtIK_EL","executionInfo":{"status":"ok","timestamp":1657080020058,"user_tz":420,"elapsed":74,"user":{"displayName":"漢訥澀","userId":"11127674219014497621"}},"outputId":"41009e62-f996-42d5-f132-3c18bcca08d7"},"source":["maskcp = ((X[\"cp1\"] == False) & (X[\"cp2\"] == False) & (X[\"cp3\"] == False) & (X[\"cp4\"] == False))\n","X.loc[maskcp]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Empty DataFrame\n","Columns: [age, sex, trestbps, chol, fbs, restecg, thalach, exang, oldpeak, slope, ca, thal, cp1, cp2, cp3, cp4]\n","Index: []"],"text/html":["\n","  <div id=\"df-20e9275f-44b2-42f2-bd0d-f9708bebcf62\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>sex</th>\n","      <th>trestbps</th>\n","      <th>chol</th>\n","      <th>fbs</th>\n","      <th>restecg</th>\n","      <th>thalach</th>\n","      <th>exang</th>\n","      <th>oldpeak</th>\n","      <th>slope</th>\n","      <th>ca</th>\n","      <th>thal</th>\n","      <th>cp1</th>\n","      <th>cp2</th>\n","      <th>cp3</th>\n","      <th>cp4</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-20e9275f-44b2-42f2-bd0d-f9708bebcf62')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-20e9275f-44b2-42f2-bd0d-f9708bebcf62 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-20e9275f-44b2-42f2-bd0d-f9708bebcf62');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"kC7NkrCWJ0Cl"},"source":["That means we got them all. Finally, let's make sure that each new column has some true values:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"feV0wzIwNtTg","executionInfo":{"status":"ok","timestamp":1657080020427,"user_tz":420,"elapsed":441,"user":{"displayName":"漢訥澀","userId":"11127674219014497621"}},"outputId":"e276deb9-8dfb-41ab-d26c-8e1c3c850a28"},"source":["display(X[\"cp1\"].value_counts())\n","display(X[\"cp2\"].value_counts())\n","display(X[\"cp3\"].value_counts())\n","display(X[\"cp4\"].value_counts())"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["False    276\n","True      23\n","Name: cp1, dtype: int64"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["False    250\n","True      49\n","Name: cp2, dtype: int64"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["False    214\n","True      85\n","Name: cp3, dtype: int64"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["False    157\n","True     142\n","Name: cp4, dtype: int64"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"-x_SZXOlH2Fj"},"source":["Now we're done with cp, let's do the other ones.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CstxpJIgI19j","executionInfo":{"status":"ok","timestamp":1657080021304,"user_tz":420,"elapsed":882,"user":{"displayName":"漢訥澀","userId":"11127674219014497621"}},"outputId":"d6e8d1bd-c058-4140-9c12-0f25a2623163"},"source":["for row in X.index:\n","  X.loc[row,\"restecg0\"] = X.loc[row,\"restecg\"] == 0.0\n","  X.loc[row,\"restecg1\"] = X.loc[row,\"restecg\"] == 1.0\n","  X.loc[row,\"restecg2\"] = X.loc[row,\"restecg\"] == 2.0\n","X.drop(\"restecg\",axis=1,inplace=True)\n","\n","for row in X.index:\n","  X.loc[row,\"slope1\"] = X.loc[row,\"slope\"] == 1.0\n","  X.loc[row,\"slope2\"] = X.loc[row,\"slope\"] == 2.0\n","  X.loc[row,\"slope3\"] = X.loc[row,\"slope\"] == 3.0\n","X.drop(\"slope\",axis=1,inplace=True)\n","\n","for row in X.index:\n","  X.loc[row,\"ca0\"] = X.loc[row,\"ca\"] == 0.0\n","  X.loc[row,\"ca1\"] = X.loc[row,\"ca\"] == 1.0\n","  X.loc[row,\"ca2\"] = X.loc[row,\"ca\"] == 2.0\n","  X.loc[row,\"ca3\"] = X.loc[row,\"ca\"] == 3.0\n","X.drop(\"ca\",axis=1,inplace=True)\n","\n","for row in X.index:\n","  X.loc[row,\"thal3\"] = X.loc[row,\"thal\"] == 3.0\n","  X.loc[row,\"thal6\"] = X.loc[row,\"thal\"] == 6.0\n","  X.loc[row,\"thal7\"] = X.loc[row,\"thal\"] == 7.0\n","X.drop(\"thal\",axis=1,inplace=True)\n","X"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      age  sex  trestbps   chol  fbs  thalach  exang  oldpeak    cp1    cp2  \\\n","0    63.0  1.0     145.0  233.0  1.0    150.0    0.0      2.3   True  False   \n","1    67.0  1.0     160.0  286.0  0.0    108.0    1.0      1.5  False  False   \n","2    67.0  1.0     120.0  229.0  0.0    129.0    1.0      2.6  False  False   \n","3    37.0  1.0     130.0  250.0  0.0    187.0    0.0      3.5  False  False   \n","4    41.0  0.0     130.0  204.0  0.0    172.0    0.0      1.4  False   True   \n","..    ...  ...       ...    ...  ...      ...    ...      ...    ...    ...   \n","298  45.0  1.0     110.0  264.0  0.0    132.0    0.0      1.2   True  False   \n","299  68.0  1.0     144.0  193.0  1.0    141.0    0.0      3.4  False  False   \n","300  57.0  1.0     130.0  131.0  0.0    115.0    1.0      1.2  False  False   \n","301  57.0  0.0     130.0  236.0  0.0    174.0    0.0      0.0  False   True   \n","302  38.0  1.0     138.0  175.0  0.0    173.0    0.0      0.0  False  False   \n","\n","     ... slope1 slope2 slope3    ca0    ca1    ca2    ca3  thal3  thal6  thal7  \n","0    ...  False  False   True   True  False  False  False  False   True  False  \n","1    ...  False   True  False  False  False  False   True   True  False  False  \n","2    ...  False   True  False  False  False   True  False  False  False   True  \n","3    ...  False  False   True   True  False  False  False   True  False  False  \n","4    ...   True  False  False   True  False  False  False   True  False  False  \n","..   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...  \n","298  ...  False   True  False   True  False  False  False  False  False   True  \n","299  ...  False   True  False  False  False   True  False  False  False   True  \n","300  ...  False   True  False  False   True  False  False  False  False   True  \n","301  ...  False   True  False  False   True  False  False   True  False  False  \n","302  ...   True  False  False   True  False  False  False   True  False  False  \n","\n","[299 rows x 25 columns]"],"text/html":["\n","  <div id=\"df-c5ea096a-c90e-4abc-b292-4a7670880bb1\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>sex</th>\n","      <th>trestbps</th>\n","      <th>chol</th>\n","      <th>fbs</th>\n","      <th>thalach</th>\n","      <th>exang</th>\n","      <th>oldpeak</th>\n","      <th>cp1</th>\n","      <th>cp2</th>\n","      <th>...</th>\n","      <th>slope1</th>\n","      <th>slope2</th>\n","      <th>slope3</th>\n","      <th>ca0</th>\n","      <th>ca1</th>\n","      <th>ca2</th>\n","      <th>ca3</th>\n","      <th>thal3</th>\n","      <th>thal6</th>\n","      <th>thal7</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>63.0</td>\n","      <td>1.0</td>\n","      <td>145.0</td>\n","      <td>233.0</td>\n","      <td>1.0</td>\n","      <td>150.0</td>\n","      <td>0.0</td>\n","      <td>2.3</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>67.0</td>\n","      <td>1.0</td>\n","      <td>160.0</td>\n","      <td>286.0</td>\n","      <td>0.0</td>\n","      <td>108.0</td>\n","      <td>1.0</td>\n","      <td>1.5</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>67.0</td>\n","      <td>1.0</td>\n","      <td>120.0</td>\n","      <td>229.0</td>\n","      <td>0.0</td>\n","      <td>129.0</td>\n","      <td>1.0</td>\n","      <td>2.6</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>37.0</td>\n","      <td>1.0</td>\n","      <td>130.0</td>\n","      <td>250.0</td>\n","      <td>0.0</td>\n","      <td>187.0</td>\n","      <td>0.0</td>\n","      <td>3.5</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>41.0</td>\n","      <td>0.0</td>\n","      <td>130.0</td>\n","      <td>204.0</td>\n","      <td>0.0</td>\n","      <td>172.0</td>\n","      <td>0.0</td>\n","      <td>1.4</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>...</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>298</th>\n","      <td>45.0</td>\n","      <td>1.0</td>\n","      <td>110.0</td>\n","      <td>264.0</td>\n","      <td>0.0</td>\n","      <td>132.0</td>\n","      <td>0.0</td>\n","      <td>1.2</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>299</th>\n","      <td>68.0</td>\n","      <td>1.0</td>\n","      <td>144.0</td>\n","      <td>193.0</td>\n","      <td>1.0</td>\n","      <td>141.0</td>\n","      <td>0.0</td>\n","      <td>3.4</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>300</th>\n","      <td>57.0</td>\n","      <td>1.0</td>\n","      <td>130.0</td>\n","      <td>131.0</td>\n","      <td>0.0</td>\n","      <td>115.0</td>\n","      <td>1.0</td>\n","      <td>1.2</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>301</th>\n","      <td>57.0</td>\n","      <td>0.0</td>\n","      <td>130.0</td>\n","      <td>236.0</td>\n","      <td>0.0</td>\n","      <td>174.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>302</th>\n","      <td>38.0</td>\n","      <td>1.0</td>\n","      <td>138.0</td>\n","      <td>175.0</td>\n","      <td>0.0</td>\n","      <td>173.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>299 rows × 25 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c5ea096a-c90e-4abc-b292-4a7670880bb1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c5ea096a-c90e-4abc-b292-4a7670880bb1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c5ea096a-c90e-4abc-b292-4a7670880bb1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"3xF3olhaOeyR"},"source":["Let's check again that we got everything:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gsA8GQ-POLXc","executionInfo":{"status":"ok","timestamp":1657080021311,"user_tz":420,"elapsed":55,"user":{"displayName":"漢訥澀","userId":"11127674219014497621"}},"outputId":"242703d5-951b-403a-d87c-86a6bf70abc4"},"source":["maskrestecg = ((X[\"restecg0\"] == False) & (X[\"restecg1\"] == False) & (X[\"restecg2\"] == False))\n","display(X.loc[maskrestecg])\n","maskslope = ((X[\"slope1\"] == False) & (X[\"slope2\"] == False) & (X[\"slope3\"] == False))\n","display(X.loc[maskslope])\n","maskca = ((X[\"ca0\"] == False) & (X[\"ca1\"] == False) & (X[\"ca2\"] == False) & (X[\"ca3\"] == False))\n","display(X.loc[maskca])\n","maskthal = ((X[\"thal3\"] == False) & (X[\"thal6\"] == False) & (X[\"thal7\"] == False))\n","display(X.loc[maskthal])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Empty DataFrame\n","Columns: [age, sex, trestbps, chol, fbs, thalach, exang, oldpeak, cp1, cp2, cp3, cp4, restecg0, restecg1, restecg2, slope1, slope2, slope3, ca0, ca1, ca2, ca3, thal3, thal6, thal7]\n","Index: []\n","\n","[0 rows x 25 columns]"],"text/html":["\n","  <div id=\"df-291559cb-fa56-4afa-8d5b-81cb17582ea8\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>sex</th>\n","      <th>trestbps</th>\n","      <th>chol</th>\n","      <th>fbs</th>\n","      <th>thalach</th>\n","      <th>exang</th>\n","      <th>oldpeak</th>\n","      <th>cp1</th>\n","      <th>cp2</th>\n","      <th>...</th>\n","      <th>slope1</th>\n","      <th>slope2</th>\n","      <th>slope3</th>\n","      <th>ca0</th>\n","      <th>ca1</th>\n","      <th>ca2</th>\n","      <th>ca3</th>\n","      <th>thal3</th>\n","      <th>thal6</th>\n","      <th>thal7</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","<p>0 rows × 25 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-291559cb-fa56-4afa-8d5b-81cb17582ea8')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-291559cb-fa56-4afa-8d5b-81cb17582ea8 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-291559cb-fa56-4afa-8d5b-81cb17582ea8');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Empty DataFrame\n","Columns: [age, sex, trestbps, chol, fbs, thalach, exang, oldpeak, cp1, cp2, cp3, cp4, restecg0, restecg1, restecg2, slope1, slope2, slope3, ca0, ca1, ca2, ca3, thal3, thal6, thal7]\n","Index: []\n","\n","[0 rows x 25 columns]"],"text/html":["\n","  <div id=\"df-373e56c1-753f-4dda-abdb-e3b31d96efa0\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>sex</th>\n","      <th>trestbps</th>\n","      <th>chol</th>\n","      <th>fbs</th>\n","      <th>thalach</th>\n","      <th>exang</th>\n","      <th>oldpeak</th>\n","      <th>cp1</th>\n","      <th>cp2</th>\n","      <th>...</th>\n","      <th>slope1</th>\n","      <th>slope2</th>\n","      <th>slope3</th>\n","      <th>ca0</th>\n","      <th>ca1</th>\n","      <th>ca2</th>\n","      <th>ca3</th>\n","      <th>thal3</th>\n","      <th>thal6</th>\n","      <th>thal7</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","<p>0 rows × 25 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-373e56c1-753f-4dda-abdb-e3b31d96efa0')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-373e56c1-753f-4dda-abdb-e3b31d96efa0 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-373e56c1-753f-4dda-abdb-e3b31d96efa0');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Empty DataFrame\n","Columns: [age, sex, trestbps, chol, fbs, thalach, exang, oldpeak, cp1, cp2, cp3, cp4, restecg0, restecg1, restecg2, slope1, slope2, slope3, ca0, ca1, ca2, ca3, thal3, thal6, thal7]\n","Index: []\n","\n","[0 rows x 25 columns]"],"text/html":["\n","  <div id=\"df-4f9beb81-c5c6-4620-8b2c-0a0f95ebfd79\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>sex</th>\n","      <th>trestbps</th>\n","      <th>chol</th>\n","      <th>fbs</th>\n","      <th>thalach</th>\n","      <th>exang</th>\n","      <th>oldpeak</th>\n","      <th>cp1</th>\n","      <th>cp2</th>\n","      <th>...</th>\n","      <th>slope1</th>\n","      <th>slope2</th>\n","      <th>slope3</th>\n","      <th>ca0</th>\n","      <th>ca1</th>\n","      <th>ca2</th>\n","      <th>ca3</th>\n","      <th>thal3</th>\n","      <th>thal6</th>\n","      <th>thal7</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","<p>0 rows × 25 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4f9beb81-c5c6-4620-8b2c-0a0f95ebfd79')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-4f9beb81-c5c6-4620-8b2c-0a0f95ebfd79 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-4f9beb81-c5c6-4620-8b2c-0a0f95ebfd79');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Empty DataFrame\n","Columns: [age, sex, trestbps, chol, fbs, thalach, exang, oldpeak, cp1, cp2, cp3, cp4, restecg0, restecg1, restecg2, slope1, slope2, slope3, ca0, ca1, ca2, ca3, thal3, thal6, thal7]\n","Index: []\n","\n","[0 rows x 25 columns]"],"text/html":["\n","  <div id=\"df-5a5b4420-bf96-47da-af8e-490e2432726b\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>sex</th>\n","      <th>trestbps</th>\n","      <th>chol</th>\n","      <th>fbs</th>\n","      <th>thalach</th>\n","      <th>exang</th>\n","      <th>oldpeak</th>\n","      <th>cp1</th>\n","      <th>cp2</th>\n","      <th>...</th>\n","      <th>slope1</th>\n","      <th>slope2</th>\n","      <th>slope3</th>\n","      <th>ca0</th>\n","      <th>ca1</th>\n","      <th>ca2</th>\n","      <th>ca3</th>\n","      <th>thal3</th>\n","      <th>thal6</th>\n","      <th>thal7</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","<p>0 rows × 25 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5a5b4420-bf96-47da-af8e-490e2432726b')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-5a5b4420-bf96-47da-af8e-490e2432726b button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-5a5b4420-bf96-47da-af8e-490e2432726b');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"rBB6yEfdPW8h"},"source":["Looks like we didn't do ca and thal correctly."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HQLcfixkPqah","executionInfo":{"status":"ok","timestamp":1657080021314,"user_tz":420,"elapsed":53,"user":{"displayName":"漢訥澀","userId":"11127674219014497621"}},"outputId":"45aa35cb-7365-4cdd-9a54-23a9ddabbe6d"},"source":["X_old['ca']==0.0"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0       True\n","1      False\n","2      False\n","3       True\n","4       True\n","       ...  \n","298     True\n","299    False\n","300    False\n","301    False\n","302     True\n","Name: ca, Length: 299, dtype: bool"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"QPGibz4VPrB4"},"source":["The problem is that ca is still an object."]},{"cell_type":"code","metadata":{"id":"B4BEIOmDQq0C"},"source":["X = X_old.astype({\"ca\": \"float64\", \"thal\": \"float64\"}).copy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aCQIoLHZQxNu","executionInfo":{"status":"ok","timestamp":1657080021340,"user_tz":420,"elapsed":76,"user":{"displayName":"漢訥澀","userId":"11127674219014497621"}},"outputId":"23969b0c-99f3-48f8-ebf9-4979d69b0df7"},"source":["X.info()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 299 entries, 0 to 302\n","Data columns (total 13 columns):\n"," #   Column    Non-Null Count  Dtype  \n","---  ------    --------------  -----  \n"," 0   age       299 non-null    float64\n"," 1   sex       299 non-null    float64\n"," 2   cp        299 non-null    float64\n"," 3   trestbps  299 non-null    float64\n"," 4   chol      299 non-null    float64\n"," 5   fbs       299 non-null    float64\n"," 6   restecg   299 non-null    float64\n"," 7   thalach   299 non-null    float64\n"," 8   exang     299 non-null    float64\n"," 9   oldpeak   299 non-null    float64\n"," 10  slope     299 non-null    float64\n"," 11  ca        299 non-null    float64\n"," 12  thal      299 non-null    float64\n","dtypes: float64(13)\n","memory usage: 40.8 KB\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vYPJg2gsQznb","executionInfo":{"status":"ok","timestamp":1657080021346,"user_tz":420,"elapsed":81,"user":{"displayName":"漢訥澀","userId":"11127674219014497621"}},"outputId":"4a4d4210-4773-417f-b99b-08add4b28265"},"source":["X['ca']==0.0"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0       True\n","1      False\n","2      False\n","3       True\n","4       True\n","       ...  \n","298     True\n","299    False\n","300    False\n","301    False\n","302     True\n","Name: ca, Length: 299, dtype: bool"]},"metadata":{},"execution_count":33}]},{"cell_type":"markdown","metadata":{"id":"IauZy4jIR3WU"},"source":["Now we do it all again:\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0flmlvzASBSr","executionInfo":{"status":"ok","timestamp":1657080022937,"user_tz":420,"elapsed":1666,"user":{"displayName":"漢訥澀","userId":"11127674219014497621"}},"outputId":"8ed2aeef-dd60-418c-ccb7-7170f3fcbbdb"},"source":["for row in X.index:\n","  X.loc[row,\"cp1\"] = X.loc[row,\"cp\"] == 1.0\n","  X.loc[row,\"cp2\"] = X.loc[row,\"cp\"] == 2.0\n","  X.loc[row,\"cp3\"] = X.loc[row,\"cp\"] == 3.0\n","  X.loc[row,\"cp4\"] = X.loc[row,\"cp\"] == 4.0\n","X.drop(\"cp\",axis=1,inplace=True)\n","\n","for row in X.index:\n","  X.loc[row,\"restecg0\"] = X.loc[row,\"restecg\"] == 0.0\n","  X.loc[row,\"restecg1\"] = X.loc[row,\"restecg\"] == 1.0\n","  X.loc[row,\"restecg2\"] = X.loc[row,\"restecg\"] == 2.0\n","X.drop(\"restecg\",axis=1,inplace=True)\n","\n","for row in X.index:\n","  X.loc[row,\"slope1\"] = X.loc[row,\"slope\"] == 1.0\n","  X.loc[row,\"slope2\"] = X.loc[row,\"slope\"] == 2.0\n","  X.loc[row,\"slope3\"] = X.loc[row,\"slope\"] == 3.0\n","X.drop(\"slope\",axis=1,inplace=True)\n","\n","for row in X.index:\n","  X.loc[row,\"ca0\"] = X.loc[row,\"ca\"] == 0.0\n","  X.loc[row,\"ca1\"] = X.loc[row,\"ca\"] == 1.0\n","  X.loc[row,\"ca2\"] = X.loc[row,\"ca\"] == 2.0\n","  X.loc[row,\"ca3\"] = X.loc[row,\"ca\"] == 3.0\n","X.drop(\"ca\",axis=1,inplace=True)\n","\n","for row in X.index:\n","  X.loc[row,\"thal3\"] = X.loc[row,\"thal\"] == 3.0\n","  X.loc[row,\"thal6\"] = X.loc[row,\"thal\"] == 6.0\n","  X.loc[row,\"thal7\"] = X.loc[row,\"thal\"] == 7.0\n","X.drop(\"thal\",axis=1,inplace=True)\n","\n","maskcp = ((X[\"cp1\"] == False) & (X[\"cp2\"] == False) & (X[\"cp3\"] == False) & (X[\"cp4\"] == False))\n","X.loc[maskcp]\n","maskrestecg = ((X[\"restecg0\"] == False) & (X[\"restecg1\"] == False) & (X[\"restecg2\"] == False))\n","display(X.loc[maskrestecg])\n","maskslope = ((X[\"slope1\"] == False) & (X[\"slope2\"] == False) & (X[\"slope3\"] == False))\n","display(X.loc[maskslope])\n","maskca = ((X[\"ca0\"] == False) & (X[\"ca1\"] == False) & (X[\"ca2\"] == False) & (X[\"ca3\"] == False))\n","display(X.loc[maskca])\n","maskthal = ((X[\"thal3\"] == False) & (X[\"thal6\"] == False) & (X[\"thal7\"] == False))\n","display(X.loc[maskthal])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Empty DataFrame\n","Columns: [age, sex, trestbps, chol, fbs, thalach, exang, oldpeak, cp1, cp2, cp3, cp4, restecg0, restecg1, restecg2, slope1, slope2, slope3, ca0, ca1, ca2, ca3, thal3, thal6, thal7]\n","Index: []\n","\n","[0 rows x 25 columns]"],"text/html":["\n","  <div id=\"df-2cb75582-6ed9-44b1-9966-8a5ed11ab9bf\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>sex</th>\n","      <th>trestbps</th>\n","      <th>chol</th>\n","      <th>fbs</th>\n","      <th>thalach</th>\n","      <th>exang</th>\n","      <th>oldpeak</th>\n","      <th>cp1</th>\n","      <th>cp2</th>\n","      <th>...</th>\n","      <th>slope1</th>\n","      <th>slope2</th>\n","      <th>slope3</th>\n","      <th>ca0</th>\n","      <th>ca1</th>\n","      <th>ca2</th>\n","      <th>ca3</th>\n","      <th>thal3</th>\n","      <th>thal6</th>\n","      <th>thal7</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","<p>0 rows × 25 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2cb75582-6ed9-44b1-9966-8a5ed11ab9bf')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-2cb75582-6ed9-44b1-9966-8a5ed11ab9bf button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-2cb75582-6ed9-44b1-9966-8a5ed11ab9bf');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Empty DataFrame\n","Columns: [age, sex, trestbps, chol, fbs, thalach, exang, oldpeak, cp1, cp2, cp3, cp4, restecg0, restecg1, restecg2, slope1, slope2, slope3, ca0, ca1, ca2, ca3, thal3, thal6, thal7]\n","Index: []\n","\n","[0 rows x 25 columns]"],"text/html":["\n","  <div id=\"df-cae578b2-a446-4173-baec-825e7db336e8\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>sex</th>\n","      <th>trestbps</th>\n","      <th>chol</th>\n","      <th>fbs</th>\n","      <th>thalach</th>\n","      <th>exang</th>\n","      <th>oldpeak</th>\n","      <th>cp1</th>\n","      <th>cp2</th>\n","      <th>...</th>\n","      <th>slope1</th>\n","      <th>slope2</th>\n","      <th>slope3</th>\n","      <th>ca0</th>\n","      <th>ca1</th>\n","      <th>ca2</th>\n","      <th>ca3</th>\n","      <th>thal3</th>\n","      <th>thal6</th>\n","      <th>thal7</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","<p>0 rows × 25 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cae578b2-a446-4173-baec-825e7db336e8')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-cae578b2-a446-4173-baec-825e7db336e8 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-cae578b2-a446-4173-baec-825e7db336e8');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Empty DataFrame\n","Columns: [age, sex, trestbps, chol, fbs, thalach, exang, oldpeak, cp1, cp2, cp3, cp4, restecg0, restecg1, restecg2, slope1, slope2, slope3, ca0, ca1, ca2, ca3, thal3, thal6, thal7]\n","Index: []\n","\n","[0 rows x 25 columns]"],"text/html":["\n","  <div id=\"df-a248d512-c998-4a5b-b2d5-0ad7c061db84\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>sex</th>\n","      <th>trestbps</th>\n","      <th>chol</th>\n","      <th>fbs</th>\n","      <th>thalach</th>\n","      <th>exang</th>\n","      <th>oldpeak</th>\n","      <th>cp1</th>\n","      <th>cp2</th>\n","      <th>...</th>\n","      <th>slope1</th>\n","      <th>slope2</th>\n","      <th>slope3</th>\n","      <th>ca0</th>\n","      <th>ca1</th>\n","      <th>ca2</th>\n","      <th>ca3</th>\n","      <th>thal3</th>\n","      <th>thal6</th>\n","      <th>thal7</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","<p>0 rows × 25 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a248d512-c998-4a5b-b2d5-0ad7c061db84')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a248d512-c998-4a5b-b2d5-0ad7c061db84 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a248d512-c998-4a5b-b2d5-0ad7c061db84');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Empty DataFrame\n","Columns: [age, sex, trestbps, chol, fbs, thalach, exang, oldpeak, cp1, cp2, cp3, cp4, restecg0, restecg1, restecg2, slope1, slope2, slope3, ca0, ca1, ca2, ca3, thal3, thal6, thal7]\n","Index: []\n","\n","[0 rows x 25 columns]"],"text/html":["\n","  <div id=\"df-e3481705-fd0c-40a2-b94f-488ac1d12e27\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>sex</th>\n","      <th>trestbps</th>\n","      <th>chol</th>\n","      <th>fbs</th>\n","      <th>thalach</th>\n","      <th>exang</th>\n","      <th>oldpeak</th>\n","      <th>cp1</th>\n","      <th>cp2</th>\n","      <th>...</th>\n","      <th>slope1</th>\n","      <th>slope2</th>\n","      <th>slope3</th>\n","      <th>ca0</th>\n","      <th>ca1</th>\n","      <th>ca2</th>\n","      <th>ca3</th>\n","      <th>thal3</th>\n","      <th>thal6</th>\n","      <th>thal7</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","<p>0 rows × 25 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e3481705-fd0c-40a2-b94f-488ac1d12e27')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e3481705-fd0c-40a2-b94f-488ac1d12e27 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e3481705-fd0c-40a2-b94f-488ac1d12e27');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"oStDLCBrSRuJ"},"source":["Great, now for checking the second part:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CmoVF84oSlW8","executionInfo":{"status":"ok","timestamp":1657080023518,"user_tz":420,"elapsed":643,"user":{"displayName":"漢訥澀","userId":"11127674219014497621"}},"outputId":"cee87f7a-c416-4b76-e7e4-941285f666af"},"source":["display(X[\"cp1\"].value_counts())\n","display(X[\"cp2\"].value_counts())\n","display(X[\"cp3\"].value_counts())\n","display(X[\"cp4\"].value_counts())\n","display(X[\"restecg0\"].value_counts())\n","display(X[\"restecg1\"].value_counts())\n","display(X[\"restecg2\"].value_counts())\n","display(X[\"slope1\"].value_counts())\n","display(X[\"slope2\"].value_counts())\n","display(X[\"slope3\"].value_counts())\n","display(X[\"ca0\"].value_counts())\n","display(X[\"ca1\"].value_counts())\n","display(X[\"ca2\"].value_counts())\n","display(X[\"ca3\"].value_counts())\n","display(X[\"thal3\"].value_counts())\n","display(X[\"thal6\"].value_counts())\n","display(X[\"thal7\"].value_counts())"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["False    276\n","True      23\n","Name: cp1, dtype: int64"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["False    250\n","True      49\n","Name: cp2, dtype: int64"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["False    214\n","True      85\n","Name: cp3, dtype: int64"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["False    157\n","True     142\n","Name: cp4, dtype: int64"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["False    151\n","True     148\n","Name: restecg0, dtype: int64"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["False    295\n","True       4\n","Name: restecg1, dtype: int64"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["False    152\n","True     147\n","Name: restecg2, dtype: int64"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["False    158\n","True     141\n","Name: slope1, dtype: int64"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["False    162\n","True     137\n","Name: slope2, dtype: int64"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["False    278\n","True      21\n","Name: slope3, dtype: int64"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["True     176\n","False    123\n","Name: ca0, dtype: int64"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["False    234\n","True      65\n","Name: ca1, dtype: int64"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["False    261\n","True      38\n","Name: ca2, dtype: int64"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["False    279\n","True      20\n","Name: ca3, dtype: int64"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["True     166\n","False    133\n","Name: thal3, dtype: int64"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["False    281\n","True      18\n","Name: thal6, dtype: int64"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["False    184\n","True     115\n","Name: thal7, dtype: int64"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"R4a1RU01S6XT"},"source":["Noticable is that restecg0 only has four True values."]},{"cell_type":"markdown","metadata":{"id":"9N4fQ0a9WPPb"},"source":["### Or we could just use get_dummies:\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jQ172cfMWU1x","executionInfo":{"status":"ok","timestamp":1657080023520,"user_tz":420,"elapsed":60,"user":{"displayName":"漢訥澀","userId":"11127674219014497621"}},"outputId":"afec3ca6-4242-4203-b6a0-9bba2385a73a"},"source":["X_encoded = pd.get_dummies(X_old,columns = [\"cp\",\"restecg\",\"slope\",\"ca\",\"thal\"])\n","X_encoded[\"exang\"] = X_encoded[\"exang\"].astype(int)\n","X_encoded[\"fbs\"] = X_encoded[\"fbs\"].astype(int)\n","X_encoded[\"sex\"] = X_encoded[\"sex\"].astype(int)\n","X_encoded"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      age  sex  trestbps   chol  fbs  thalach  exang  oldpeak  cp_1.0  cp_2.0  \\\n","0    63.0    1     145.0  233.0    1    150.0      0      2.3       1       0   \n","1    67.0    1     160.0  286.0    0    108.0      1      1.5       0       0   \n","2    67.0    1     120.0  229.0    0    129.0      1      2.6       0       0   \n","3    37.0    1     130.0  250.0    0    187.0      0      3.5       0       0   \n","4    41.0    0     130.0  204.0    0    172.0      0      1.4       0       1   \n","..    ...  ...       ...    ...  ...      ...    ...      ...     ...     ...   \n","298  45.0    1     110.0  264.0    0    132.0      0      1.2       1       0   \n","299  68.0    1     144.0  193.0    1    141.0      0      3.4       0       0   \n","300  57.0    1     130.0  131.0    0    115.0      1      1.2       0       0   \n","301  57.0    0     130.0  236.0    0    174.0      0      0.0       0       1   \n","302  38.0    1     138.0  175.0    0    173.0      0      0.0       0       0   \n","\n","     ...  slope_1.0  slope_2.0  slope_3.0  ca_0.0  ca_1.0  ca_2.0  ca_3.0  \\\n","0    ...          0          0          1       1       0       0       0   \n","1    ...          0          1          0       0       0       0       1   \n","2    ...          0          1          0       0       0       1       0   \n","3    ...          0          0          1       1       0       0       0   \n","4    ...          1          0          0       1       0       0       0   \n","..   ...        ...        ...        ...     ...     ...     ...     ...   \n","298  ...          0          1          0       1       0       0       0   \n","299  ...          0          1          0       0       0       1       0   \n","300  ...          0          1          0       0       1       0       0   \n","301  ...          0          1          0       0       1       0       0   \n","302  ...          1          0          0       1       0       0       0   \n","\n","     thal_3.0  thal_6.0  thal_7.0  \n","0           0         1         0  \n","1           1         0         0  \n","2           0         0         1  \n","3           1         0         0  \n","4           1         0         0  \n","..        ...       ...       ...  \n","298         0         0         1  \n","299         0         0         1  \n","300         0         0         1  \n","301         1         0         0  \n","302         1         0         0  \n","\n","[299 rows x 25 columns]"],"text/html":["\n","  <div id=\"df-98563b7f-ab29-4f34-aa7f-3a4fa6e7e5ae\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>sex</th>\n","      <th>trestbps</th>\n","      <th>chol</th>\n","      <th>fbs</th>\n","      <th>thalach</th>\n","      <th>exang</th>\n","      <th>oldpeak</th>\n","      <th>cp_1.0</th>\n","      <th>cp_2.0</th>\n","      <th>...</th>\n","      <th>slope_1.0</th>\n","      <th>slope_2.0</th>\n","      <th>slope_3.0</th>\n","      <th>ca_0.0</th>\n","      <th>ca_1.0</th>\n","      <th>ca_2.0</th>\n","      <th>ca_3.0</th>\n","      <th>thal_3.0</th>\n","      <th>thal_6.0</th>\n","      <th>thal_7.0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>63.0</td>\n","      <td>1</td>\n","      <td>145.0</td>\n","      <td>233.0</td>\n","      <td>1</td>\n","      <td>150.0</td>\n","      <td>0</td>\n","      <td>2.3</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>67.0</td>\n","      <td>1</td>\n","      <td>160.0</td>\n","      <td>286.0</td>\n","      <td>0</td>\n","      <td>108.0</td>\n","      <td>1</td>\n","      <td>1.5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>67.0</td>\n","      <td>1</td>\n","      <td>120.0</td>\n","      <td>229.0</td>\n","      <td>0</td>\n","      <td>129.0</td>\n","      <td>1</td>\n","      <td>2.6</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>37.0</td>\n","      <td>1</td>\n","      <td>130.0</td>\n","      <td>250.0</td>\n","      <td>0</td>\n","      <td>187.0</td>\n","      <td>0</td>\n","      <td>3.5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>41.0</td>\n","      <td>0</td>\n","      <td>130.0</td>\n","      <td>204.0</td>\n","      <td>0</td>\n","      <td>172.0</td>\n","      <td>0</td>\n","      <td>1.4</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>298</th>\n","      <td>45.0</td>\n","      <td>1</td>\n","      <td>110.0</td>\n","      <td>264.0</td>\n","      <td>0</td>\n","      <td>132.0</td>\n","      <td>0</td>\n","      <td>1.2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>299</th>\n","      <td>68.0</td>\n","      <td>1</td>\n","      <td>144.0</td>\n","      <td>193.0</td>\n","      <td>1</td>\n","      <td>141.0</td>\n","      <td>0</td>\n","      <td>3.4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>300</th>\n","      <td>57.0</td>\n","      <td>1</td>\n","      <td>130.0</td>\n","      <td>131.0</td>\n","      <td>0</td>\n","      <td>115.0</td>\n","      <td>1</td>\n","      <td>1.2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>301</th>\n","      <td>57.0</td>\n","      <td>0</td>\n","      <td>130.0</td>\n","      <td>236.0</td>\n","      <td>0</td>\n","      <td>174.0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>302</th>\n","      <td>38.0</td>\n","      <td>1</td>\n","      <td>138.0</td>\n","      <td>175.0</td>\n","      <td>0</td>\n","      <td>173.0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>299 rows × 25 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-98563b7f-ab29-4f34-aa7f-3a4fa6e7e5ae')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-98563b7f-ab29-4f34-aa7f-3a4fa6e7e5ae button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-98563b7f-ab29-4f34-aa7f-3a4fa6e7e5ae');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":36}]},{"cell_type":"markdown","metadata":{"id":"uhldH_4AWk73"},"source":["### Let's now deal with y:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qB-izdy4YVSn","executionInfo":{"status":"ok","timestamp":1657080023520,"user_tz":420,"elapsed":32,"user":{"displayName":"漢訥澀","userId":"11127674219014497621"}},"outputId":"330c4939-741f-4530-adf4-0c8a884ae91f"},"source":["y_mask = y > 0\n","y[y_mask] = 1\n","y"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0      0\n","1      1\n","2      1\n","3      0\n","4      0\n","      ..\n","298    1\n","299    1\n","300    1\n","301    1\n","302    0\n","Name: num, Length: 299, dtype: int64"]},"metadata":{},"execution_count":37}]},{"cell_type":"markdown","metadata":{"id":"eZTjwIQLYvKS"},"source":["That way, we only have 1 and 0, not 2,3,4."]}]}